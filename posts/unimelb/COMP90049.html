<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.59" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://crc011220.github.io/posts/unimelb/COMP90049.html"><meta property="og:site_name" content="Richard Chen"><meta property="og:title" content="Introduction to Machine Learning (COMP90049)"><meta property="og:description" content="Introduction to Machine Learning (COMP90049) Week 1 Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitl..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-06-14T09:30:50.000Z"><meta property="article:tag" content="Unimelb"><meta property="article:published_time" content="2025-03-02T00:00:00.000Z"><meta property="article:modified_time" content="2025-06-14T09:30:50.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Introduction to Machine Learning (COMP90049)","image":[""],"datePublished":"2025-03-02T00:00:00.000Z","dateModified":"2025-06-14T09:30:50.000Z","author":[{"@type":"Person","name":"Richard Chen"}]}</script><link rel="icon" href="/mac.ico"><title>Introduction to Machine Learning (COMP90049) | Richard Chen</title><meta name="description" content="Introduction to Machine Learning (COMP90049) Week 1 Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitl...">
    <link rel="stylesheet" href="/assets/css/styles.563fed46.css">
    <link rel="preload" href="/assets/js/runtime~app.ba280723.js" as="script"><link rel="preload" href="/assets/css/styles.563fed46.css" as="style"><link rel="preload" href="/assets/js/6312.2d95f1ad.js" as="script"><link rel="preload" href="/assets/js/app.bf24beb5.js" as="script">
    <link rel="prefetch" href="/assets/js/zh_posts_declarative_haskell.html.2de58c68.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_java8_函数式编程.html.824175b7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_netty_Netty01-nio.html.e18e32a4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_netty_Netty04-优化与源码.html.f9b9d47e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_netty_Netty03-进阶.html.08340e71.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_netty_Netty02-intro.html.ffd72070.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_nginx_1.html.3e3ca755.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_declarative_prolog.html.a05216b1.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_COMP90024.html.419e3f7a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hive_Hive-SQL语法大全.html.4ca54797.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_14.html.9788afd4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_declarative_haskell速记.html.ab0e81f0.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_COMP90049.html.8af5cfe6.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_COMP90050.html.d5705289.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_图.html.ba12d1ad.js" as="script"><link rel="prefetch" href="/assets/js/posts_note_Coin-exchange-project-note.html.00a5fbe1.js" as="script"><link rel="prefetch" href="/assets/js/posts_tailwind_summary.html.65a1bdb1.js" as="script"><link rel="prefetch" href="/assets/js/2664.0c0e265f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_数组.html.d77508fe.js" as="script"><link rel="prefetch" href="/assets/js/posts_typescript_1.html.303fabd4.js" as="script"><link rel="prefetch" href="/assets/js/posts_typescript_2.html.6b0b8abc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_二叉树.html.36d3ec66.js" as="script"><link rel="prefetch" href="/assets/js/posts_aws-saa_1.html.dd0f5c35.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_动态规划.html.393c14e9.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_ml.html.61e6fbb6.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_COMP90048.html.225cfabb.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_1.html.94e933ec.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_declarative_prolog速记.html.3d518755.js" as="script"><link rel="prefetch" href="/assets/js/8300.ef6ef338.js" as="script"><link rel="prefetch" href="/assets/js/posts_mybatis_2.html.9a05b659.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_7.html.be068800.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_4.html.5dd4d88a.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_5.html.c78a65d9.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_8.html.dcb82242.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_2.html.3c496de7.js" as="script"><link rel="prefetch" href="/assets/js/posts_aws-saa_3.html.1edee090.js" as="script"><link rel="prefetch" href="/assets/js/posts_nginx_1.html.8ff423c8.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_SWEN90016.html.0fbe2d76.js" as="script"><link rel="prefetch" href="/assets/js/posts_spark_Spark-Core.html.cd8636a7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_栈和队列.html.3d660ab7.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_11.html.f0a8d06e.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_14.html.8b214548.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_8.html.9cda8d6b.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_4.html.5d36a4ea.js" as="script"><link rel="prefetch" href="/assets/js/posts_spark_Spark-Sql.html.76cca896.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_21.html.904e2810.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_11.html.8d4af2d8.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_5.html.f3508083.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_6.html.6d663852.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_10.html.f04a8c72.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_1.html.713f4462.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_6.html.f0749890.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_4.html.6837b70a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_单调栈.html.0e0444b9.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_15.html.2d6f62c9.js" as="script"><link rel="prefetch" href="/assets/js/posts_aws-saa_2.html.3e18d573.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_6.html.d96cbcf7.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_3.html.b67ee897.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_12.html.5465a818.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_14.html.2f0072e3.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_13.html.de689093.js" as="script"><link rel="prefetch" href="/assets/js/posts_ai_javaai.html.1763e5de.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_5.html.3dd0ba99.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_6.html.64a3195d.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_2.html.cebc237a.js" as="script"><link rel="prefetch" href="/assets/js/posts_typescript_3.html.96c00b28.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_2.html.bcca6767.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_16.html.b4dd60f6.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_2.html.625a5d8c.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_7.html.9ddc3cf1.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_3.html.ab27c026.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_12.html.d3c10274.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_17.html.e632515c.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_13.html.a996409a.js" as="script"><link rel="prefetch" href="/assets/js/intro.html.346b713b.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_9.html.f7cf1a3c.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_15.html.873ab02a.js" as="script"><link rel="prefetch" href="/assets/js/posts_ai_create_mcp.html.cd96f05a.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_18.html.3244fd5f.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_1.html.c948d11f.js" as="script"><link rel="prefetch" href="/assets/js/posts_ai_mcp.html.12a53f00.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_双指针.html.223139d9.js" as="script"><link rel="prefetch" href="/assets/js/posts_mybatis_1.html.46f844ef.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_1.html.176fa26c.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_7.html.fe2c2946.js" as="script"><link rel="prefetch" href="/assets/js/posts_istio_1.html.b29c1e6e.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_4.html.a5b484d4.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_15.html.c985094b.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_10.html.36b85468.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_1.html.af846411.js" as="script"><link rel="prefetch" href="/assets/js/zh_intro.html.e5e6702c.js" as="script"><link rel="prefetch" href="/assets/js/posts_spark_Spark-Intro.html.bbb267af.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_5.html.cb6a2328.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_19.html.e098bed2.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_7.html.d86fd6a9.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_3.html.bdbee8ab.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_7.html.94b00692.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_8.html.339465c5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_字符串.html.fc1a0e0b.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_index.html.239e1c93.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_3.html.391d081e.js" as="script"><link rel="prefetch" href="/assets/js/posts_hadoop_Hive.html.262cc7c9.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_7.html.379ab7f8.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_9.html.95c72561.js" as="script"><link rel="prefetch" href="/assets/js/posts_hadoop_HDFS.html.283fd982.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_index.html.26d96d19.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_2.html.9f746bd8.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_8.html.8177671e.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_4.html.7f4ae050.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_5.html.14dbc6db.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_3.html.11c40f4f.js" as="script"><link rel="prefetch" href="/assets/js/posts_docker_1.html.1880db3c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_链表.html.fa4fc6d2.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_11.html.60ffe1d3.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_microsvc_1.html.1b33e34f.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_index.html.42eacdc0.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_summary.html.f7e4e9a6.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_6.html.23812d5e.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_6.html.a87da5b6.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_3.html.8dd3942e.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_6.html.8a445e18.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_7.html.f5fc0da9.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_8.html.10fbd61b.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_4.html.573becd0.js" as="script"><link rel="prefetch" href="/assets/js/posts_elasticsearch_query-dsl.html.b1f6d5ea.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_cloudflare.html.557796c4.js" as="script"><link rel="prefetch" href="/assets/js/posts_algorithm_20.html.0c40c0b3.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_1.html.5165a4ea.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_5.html.5be2b613.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_2.html.50b07800.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_10.html.f704fd8f.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_16.html.18d78ab9.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_practices_3.html.31996ba5.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_tools.html.d10ee53e.js" as="script"><link rel="prefetch" href="/assets/js/posts_elasticsearch_1.html.ebb5f1f0.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_2.html.7aac274d.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_index.html.9b1d8150.js" as="script"><link rel="prefetch" href="/assets/js/posts_dubbo_1.html.445e9a92.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_concepts_4.html.426a80e8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_回溯.html.be4b57c7.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_9.html.d9a9f223.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_index.html.08982f6e.js" as="script"><link rel="prefetch" href="/assets/js/posts_nextjs_index.html.fe13e6fc.js" as="script"><link rel="prefetch" href="/assets/js/posts_interview_9.html.ca0495f6.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_index.html.fa15d98a.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_1.html.ef8b85c0.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_8.html.003abe76.js" as="script"><link rel="prefetch" href="/assets/js/posts_elasticsearch_2.html.2310493b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_genesis.html.1d78bd11.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_5.html.4925b8f0.js" as="script"><link rel="prefetch" href="/assets/js/posts_unimelb_index.html.9a47540b.js" as="script"><link rel="prefetch" href="/assets/js/posts_hadoop_MapReduce.html.da681e9e.js" as="script"><link rel="prefetch" href="/assets/js/posts_genesis.html.4e8cb9a0.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_12.html.48356527.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_sealos.html.f883124c.js" as="script"><link rel="prefetch" href="/assets/js/posts_hadoop_index.html.08c56d0f.js" as="script"><link rel="prefetch" href="/assets/js/posts_dubbo_2.html.fe8ef07d.js" as="script"><link rel="prefetch" href="/assets/js/posts_cliché_13.html.9e92bb7e.js" as="script"><link rel="prefetch" href="/assets/js/posts_aws-saa_index.html.37d8e220.js" as="script"><link rel="prefetch" href="/assets/js/posts_typescript_index.html.5959c918.js" as="script"><link rel="prefetch" href="/assets/js/posts_elasticsearch_index.html.cac7d5a3.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_index.html.93255781.js" as="script"><link rel="prefetch" href="/assets/js/index.html.f1baac64.js" as="script"><link rel="prefetch" href="/assets/js/posts_dubbo_index.html.31855f74.js" as="script"><link rel="prefetch" href="/assets/js/posts_hadoop_Yarn.html.9cc0a345.js" as="script"><link rel="prefetch" href="/assets/js/posts_nginx_index.html.6a85b57c.js" as="script"><link rel="prefetch" href="/assets/js/posts_ai_index.html.077b8907.js" as="script"><link rel="prefetch" href="/assets/js/posts_mybatis_index.html.f46a6e67.js" as="script"><link rel="prefetch" href="/assets/js/posts_spark_index.html.7c088f18.js" as="script"><link rel="prefetch" href="/assets/js/posts_kubernetes_microsvc_index.html.bdd0aa95.js" as="script"><link rel="prefetch" href="/assets/js/posts_cmb_9.html.b2f274e3.js" as="script"><link rel="prefetch" href="/assets/js/zh_index.html.b716cca7.js" as="script"><link rel="prefetch" href="/assets/js/posts_nginx_2.html.ace27108.js" as="script"><link rel="prefetch" href="/assets/js/posts_tailwind_index.html.fc04aaae.js" as="script"><link rel="prefetch" href="/assets/js/posts_docker_index.html.c85d264a.js" as="script"><link rel="prefetch" href="/assets/js/posts_istio_index.html.82460225.js" as="script"><link rel="prefetch" href="/assets/js/posts_note_index.html.0e3c67ef.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_哈希.html.342e8089.js" as="script"><link rel="prefetch" href="/assets/js/category_learning-records_index.html.576de52f.js" as="script"><link rel="prefetch" href="/assets/js/tag_nginx_index.html.429ee8e4.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_declarative-programming_index.html.833dc4ec.js" as="script"><link rel="prefetch" href="/assets/js/category_index.html.25d997fb.js" as="script"><link rel="prefetch" href="/assets/js/timeline_index.html.c8382c3f.js" as="script"><link rel="prefetch" href="/assets/js/article_index.html.754ed5f9.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_学习笔记_index.html.6c21dbc9.js" as="script"><link rel="prefetch" href="/assets/js/category_internship-journal_index.html.ff04715e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_learning-records_index.html.89e713e0.js" as="script"><link rel="prefetch" href="/assets/js/tag_index.html.d7d9fa91.js" as="script"><link rel="prefetch" href="/assets/js/star_index.html.db14b2c2.js" as="script"><link rel="prefetch" href="/assets/js/tag_programmer-cliché_index.html.b8a9b2f4.js" as="script"><link rel="prefetch" href="/assets/js/tag_algorithm-practices_index.html.3e225319.js" as="script"><link rel="prefetch" href="/assets/js/tag_china-merchant-bank_index.html.9797c8de.js" as="script"><link rel="prefetch" href="/assets/js/tag_technical-interview_index.html.ba0e5197.js" as="script"><link rel="prefetch" href="/assets/js/posts_index.html.ebef2662.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_开始_index.html.a9db6c96.js" as="script"><link rel="prefetch" href="/assets/js/tag_tailwind-css_index.html.d46e2f33.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_旅程_index.html.6d96a29f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_简历_index.html.0c2e51e9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_algorithm_index.html.86e7903e.js" as="script"><link rel="prefetch" href="/assets/js/category_genesis_index.html.d083a48c.js" as="script"><link rel="prefetch" href="/assets/js/tag_javascript_index.html.ede35886.js" as="script"><link rel="prefetch" href="/assets/js/tag_kubernetes_index.html.59f335e6.js" as="script"><link rel="prefetch" href="/assets/js/tag_typescript_index.html.5503bbb7.js" as="script"><link rel="prefetch" href="/assets/js/tag_leetcode_index.html.e67a7df0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_java8_index.html.0ef9ad6c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_netty_index.html.4edaa8ea.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_nginx_index.html.6c523351.js" as="script"><link rel="prefetch" href="/assets/js/tag_aws-saa_index.html.7fcee46c.js" as="script"><link rel="prefetch" href="/assets/js/tag_mybatis_index.html.8a8a1949.js" as="script"><link rel="prefetch" href="/assets/js/tag_unimelb_index.html.ecb8b42a.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_hive_index.html.a64532b5.js" as="script"><link rel="prefetch" href="/assets/js/tag_docker_index.html.cef315fc.js" as="script"><link rel="prefetch" href="/assets/js/tag_hadoop_index.html.2e99281a.js" as="script"><link rel="prefetch" href="/assets/js/tag_nextjs_index.html.e5bc4642.js" as="script"><link rel="prefetch" href="/assets/js/tag_resume_index.html.efa76b7b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_nio_index.html.761667cf.js" as="script"><link rel="prefetch" href="/assets/js/tag_dubbo_index.html.df2e18bb.js" as="script"><link rel="prefetch" href="/assets/js/tag_index_index.html.935a8cb9.js" as="script"><link rel="prefetch" href="/assets/js/tag_istio_index.html.c36e5323.js" as="script"><link rel="prefetch" href="/assets/js/tag_notes_index.html.39c1108a.js" as="script"><link rel="prefetch" href="/assets/js/tag_react_index.html.9197e564.js" as="script"><link rel="prefetch" href="/assets/js/tag_spark_index.html.965860c8.js" as="script"><link rel="prefetch" href="/assets/js/404.html.94cc8c0f.js" as="script"><link rel="prefetch" href="/assets/js/zh_timeline_index.html.bad17f7e.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai_index.html.a08d3373.js" as="script"><link rel="prefetch" href="/assets/js/tag_es_index.html.ad7869c1.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_index.html.05abcedf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_declarative_index.html.33525ec4.js" as="script"><link rel="prefetch" href="/assets/js/zh_article_index.html.b4ce75ee.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_index.html.7cd5e86b.js" as="script"><link rel="prefetch" href="/assets/js/zh_star_index.html.73563921.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_java8_index.html.86af96f4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_netty_index.html.c04cfafd.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_nginx_index.html.1a7661a1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_algo_index.html.31c7b411.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hive_index.html.ea34aa1a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_index.html.d6f4b666.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="Take me home"><img class="vp-nav-logo" src="https://img.taotu.cn/ssd/ssd4/1/2024-07-08/1_e36ceb8c09656a291a6f0a33178736f9.webp" alt><!----><span class="vp-site-name hide-in-pad">Richard Chen</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="Blog Home"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Posts"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Posts<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Algorithm</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/algorithm/" aria-label="Algorithm Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Algorithm Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">AWS-SAA</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/aws-saa/" aria-label="AWS-SAA Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->AWS-SAA Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Cliché</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/clich%C3%A9/" aria-label="Cliché Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Cliché Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">China Merchant Bank</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/cmb/" aria-label="CMB Internship Record"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->CMB Internship Record<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Docker</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/docker/" aria-label="Docker Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Docker Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Dubbo</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/dubbo/" aria-label="Dubbo Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Dubbo Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">ElasticSearch</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/elasticsearch/" aria-label="ElasticSearch Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->ElasticSearch Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Hadoop and Hive</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/hadoop/" aria-label="Hadoop and Hive Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Hadoop and Hive Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Interview</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/interview/" aria-label="Interview Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Interview Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Kubernetes</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/kubernetes/concepts/" aria-label="Concepts"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Concepts<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/kubernetes/practices/" aria-label="Practices"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Practices<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/kubernetes/microsvc/" aria-label="MicroSvc"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->MicroSvc<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">MyBatis</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/mybatis/" aria-label="MyBatis Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->MyBatis Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">NextJS</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/nextjs/" aria-label="NextJS Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->NextJS Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">NGINX</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/nginx/" aria-label="NGINX Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->NGINX Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Note</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/note/" aria-label="Notes"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Notes<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Tailwind CSS</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/tailwind/" aria-label="Tailwind CSS Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Tailwind CSS Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Typescript</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/typescript/" aria-label="Typescript Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Typescript Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Unimelb Notes</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link route-link-active auto-link" href="/posts/unimelb/" aria-label="Unimelb Notes Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Unimelb Notes Contents<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/posts/unimelb/COMP90049.html" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/zh/" aria-label="简体中文"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Crc011220/Crc011220.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search Blogs"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search Blogs</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="Blog Home"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->Blog Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Posts</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Ai</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Algorithm</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Aws Saa</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Cliché</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Cmb</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Docker</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Dubbo</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Elasticsearch</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/genesis.html" aria-label="Genesis"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Genesis<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Hadoop</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Interview</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Istio</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Kubernetes</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Mybatis</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Nextjs</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Nginx</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Note</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Spark</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Tailwind</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Typescript</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Unimelb</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/COMP90050.html" aria-label="Advanced Database Systems (COMP90050)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Advanced Database Systems (COMP90050)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/COMP90024.html" aria-label="Cluster and Cloud Computing (COMP90024)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Cluster and Cloud Computing (COMP90024)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/COMP90048.html" aria-label="Declarative Programming (COMP90048)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Declarative Programming (COMP90048)<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/posts/unimelb/COMP90049.html" aria-label="Introduction to Machine Learning (COMP90049)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Introduction to Machine Learning (COMP90049)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/ml.html" aria-label="Machine Learning"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Machine Learning<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/SWEN90016.html" aria-label="Software Process and Management (SWEN90016)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Software Process and Management (SWEN90016)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/unimelb/" aria-label="UNIMELB Course Notes"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->UNIMELB Course Notes<!----></a></li></ul></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="About Me"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-circle-info" style=""></span><!--]-->About Me<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Introduction to Machine Learning (COMP90049)</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Richard Chen</span></span><span property="author" content="Richard Chen"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">March 2, 2025</span><meta property="datePublished" content="2025-03-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 49 min</span><meta property="timeRequired" content="PT49M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color5 clickable" role="navigation">Learning Records</span><!--]--><meta property="articleSection" content="Learning Records"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color7 clickable" role="navigation">Unimelb</span><!--]--><meta property="keywords" content="Unimelb"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-1">Week 1</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#three-ingredients-for-machine-learning">Three ingredients for machine learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#linear-algebra-review">Linear Algebra Review</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning">Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#featured-data-types">Featured Data Types</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#equal-width-vs-equal-frequency-vs-clustering">Equal Width vs. Equal Frequency vs. Clustering</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#standardization-vs-normalization">Standardization vs. Normalization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-2">Week 2</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#probility">Probility</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-3">Week 3</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#zero-r">Zero-R</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#one-r">One-R</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#desicion-trees">Desicion Trees</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#id3-iterative-dichotomiser-3">ID3 (Iterative Dichotomiser 3)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#naive-bayes-theory">Naive Bayes Theory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#different-naive-bayes">Different Naive Bayes</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#conclusion-of-naive-bayes">Conclusion of Naive Bayes</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-4">Week 4</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#linear-regression">Linear Regression</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#optimization">Optimization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#iterative-optimization">Iterative Optimization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gradient-descent">Gradient Descent</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#logistic-regression">Logistic Regression</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution">Softmax function: The function that converts a vector of real numbers to a probability distribution.</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#logistic-regression-summary">Logistic Regression Summary</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-5">Week 5</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#support-vector-machines-svm">Support Vector Machines (SVM)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#hard-margin">Hard Margin</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#applications-of-svm">Applications of SVM</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#ranking">Ranking</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#structured-prediction">Structured prediction</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-6">Week 6</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation">Evaluation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#classification-evaluation-metrics">Classification Evaluation Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#averaging-methods">Averaging methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-macro-averaging">1. Macro Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-micro-averaging">2. Micro Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-weighted-averaging">3. Weighted Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#regression-performance-metrics">Regression Performance Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#learning-curves">Learning Curves</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bias-and-variance">Bias and Variance</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-7-feature-selection">Week 7: Feature Selection</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#univariate-methods">Univariate Methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#multivariate-methods">Multivariate Methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#filter-methods">Filter Methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#wrapper-methods">Wrapper Methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#embedded-methods">Embedded Methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#forward-selection-wrapper-method">Forward Selection (Wrapper Method)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#forward-selection-embedded-method">Forward Selection (Embedded Method)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#backward-elimination-wrapper-method">Backward Elimination (Wrapper Method)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#backward-elimination-embedded-method">Backward Elimination (Embedded Method)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#l0-norm-regularization">L0-Norm Regularization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#l1-norm-regularization">L1-Norm Regularization</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-8">Week 8</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#ann">ANN</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#perceptron">Perceptron</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#perceptron-algorithm">Perceptron Algorithm</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#online-learning-vs-batch-learning">Online learning vs. Batch learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#multi-layer-perceptron">Multi-layer perceptron</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#multi-layer-perceptron-vs-other-neural-networks">Multi-layer Perceptron vs. other Neural Networks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#linear-vs-non-linear-classification">Linear vs. Non-linear classification</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#feature-engineering-vs-feature-learning">Feature engineering vs feature learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#activation-functions">Activation Functions</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#when-is-linear-classification-enough">When is Linear Classification Enough?</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#pros-and-cons-of-neural-networks">Pros and Cons of Neural Networks</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-9">Week 9</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#why-cannot-we-use-the-step-functon">Why cannot we use the step functon?</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#a-problem-encountered-when-training-a-multilayer-perceptron-mlp-and-the-method-to-solve-it">A problem encountered when training a multilayer perceptron (MLP) and the method to solve it.</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#comparison-of-perceptron-and-backpropagation">Comparison of Perceptron and Backpropagation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#backpropagation">Backpropagation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#generalized-delta-rule">Generalized Delta Rule</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#discriminative-and-generative">Discriminative and Generative</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#🔍-one-sentence-difference">🔍 One-sentence Difference:</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#📚-more-specifically">📚 More Specifically:</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#🎓-classic-examples">🎓 Classic Examples:</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#✅-when-to-use-which">✅ When to Use Which?</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-10">Week 10</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#unsupervised-learning">Unsupervised Learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#types-of-unsupervised-learning">Types of unsupervised learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#clustering">Clustering</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#k-means">K-means</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#hierarchical-clustering">Hierarchical Clustering</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#clustering-metrics">Clustering Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#homogeneity">Homogeneity</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#completeness">Completeness</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#示例输入">示例输入：</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#issues-of-unsupervised-learning">Issues of unsupervised learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#semi-supervised-learning">Semi-supervised learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#self-tranining">Self-tranining</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#active-learning">Active Learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#data-augmentation">Data Augmentation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#unsupervised-pre-training">Unsupervised pre-training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-11">Week 11</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#parametric-models-and-non-parametric-models">Parametric Models and Non-parametric Models:</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#ensembles">Ensembles</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#different-classifiers-different-models-or-the-same-model-with-feature-manipulation-stacking">Different classifiers (different models or the same model with feature manipulation): Stacking</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-stacking">1. Stacking</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#same-classifier-instance-manipulation-bagging-primarily-targets-variance-reduction">Same classifier, instance manipulation: Bagging (primarily targets variance reduction)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-bagging">2. Bagging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#same-classifier-algorithm-manipulation-boosting-primarily-targets-bias-reduction">Same classifier, algorithm manipulation: Boosting (primarily targets bias reduction)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-boosting">3. Boosting</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#summary">Summary</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#anomaly-detection">Anomaly Detection</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#types-of-anomaly">Types of Anomaly</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#anomaly-detection-algorithms">Anomaly Detection Algorithms</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-12">Week 12</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bias">Bias</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#measurement-bias">Measurement Bias</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#model-bias">Model Bias</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation-deployment-bias">Evaluation / Deployment Bias</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="introduction-to-machine-learning-comp90049" tabindex="-1"><a class="header-anchor" href="#introduction-to-machine-learning-comp90049"><span>Introduction to Machine Learning (COMP90049)</span></a></h1><h2 id="week-1" tabindex="-1"><a class="header-anchor" href="#week-1"><span>Week 1</span></a></h2><ul><li>Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitly programmed.</li></ul><h3 id="three-ingredients-for-machine-learning" tabindex="-1"><a class="header-anchor" href="#three-ingredients-for-machine-learning"><span>Three ingredients for machine learning</span></a></h3><ol><li>Data</li></ol><ul><li>Discrete vs continuous vs ...</li><li>Big data vs small data</li><li>Labeled data vs unlabeled data</li><li>Public vs sensitive data</li></ul><ol start="2"><li>Models</li></ol><ul><li>function mapping from inputs to outputs</li><li>probabilistic machine learning models</li><li>geometric machine learning models</li><li>parameters of the function are unknown</li></ul><ol start="3"><li>Learning</li></ol><ul><li>Improving (on a task) after data is taken into account</li><li>Finding the best model parameters (for a given task)</li><li>Supervised vs. unsupervised learning</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><p>Supervised Learning:监督学习使用带有标签的数据进行训练，模型学习输入（features）到输出（labels）之间的映射关系。 Unsupervised Learning:无监督学习使用没有标签的数据，模型通过分析数据的模式和结构来进行学习。</p></div><h3 id="linear-algebra-review" tabindex="-1"><a class="header-anchor" href="#linear-algebra-review"><span>Linear Algebra Review</span></a></h3><h4 id="matrices" tabindex="-1"><a class="header-anchor" href="#matrices"><span>Matrices</span></a></h4><ul><li>Matrices addition/subtraction: Add(Subtract) correspond ingentries in A and B</li><li>Matrix multiplication: Multiply corresponding entries in A and B and sum the products <img src="/assets/img/matrix-multiplication.014b9a1e.png" alt="Matrix Multiplication" loading="lazy"></li><li>Matrix transpose: Transpose of a matrix is obtained by interchanging its rows and columns. Matrix is <strong>symmetric</strong> if it is equal to its transpose.</li><li>Matrix inverse: The inverse of a matrix A is denoted by A^-1 and is obtained by multiplying A by its inverse.</li><li>A matrix cannot be inverted if: More rows than columns, More columns than rows,Redundant rows/columns (linear independence)</li></ul><h4 id="vectors" tabindex="-1"><a class="header-anchor" href="#vectors"><span>Vectors</span></a></h4><ul><li>A vector is a matrix with several rows and <strong>one</strong> column</li><li>Vector addition/subtraction: Add(Subtract) corresponding entries in A and B</li><li>Vector inner product: Multiply corresponding entries in A and B and sum the products</li><li>Vector Euclidean norm: The square root of the sum of the squares of the entries in the vector. <img src="/assets/img/vector-norm.36eacd69.png" alt="Verctor Euclidean Norm" loading="lazy"></li><li>Vector inner product: The dot product of two vectors A and B is the sum of the products of their corresponding entries.</li><li>The cosine of the angle between two vectors can be found by using norms and the inner product</li></ul><h3 id="instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning" tabindex="-1"><a class="header-anchor" href="#instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning"><span>Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)</span></a></h3><ul><li>In ML terminology examples are called Instances</li><li>Each instance can have some Features or Attributes</li><li>Concepts are things that we aim to learn. Generally, in the form of labels or classes</li></ul><h4 id="unsupervised-do-not-have-access-to-an-inventory-of-classes-and-instead-discover-groups-of-similar-examples-in-a-given-dataset" tabindex="-1"><a class="header-anchor" href="#unsupervised-do-not-have-access-to-an-inventory-of-classes-and-instead-discover-groups-of-similar-examples-in-a-given-dataset"><span>Unsupervised do not have access to an inventory of classes and instead discover groups of ‘similar’ examples in a given dataset.</span></a></h4><ul><li>Clustering is unsupervised — the learner operates without a set of labelled training data</li><li><strong>Success is often measured subjectively; evaluation is problematic</strong></li></ul><h4 id="supervised-methods-have-prior-knowledge-of-classes-and-set-out-to-discover-and-categorise-new-instances-according-to-those-classes" tabindex="-1"><a class="header-anchor" href="#supervised-methods-have-prior-knowledge-of-classes-and-set-out-to-discover-and-categorise-new-instances-according-to-those-classes"><span>Supervised methods have prior knowledge of classes and set out to discover and categorise new instances according to those classes</span></a></h4><ul><li>Classification learning is supervised</li><li>In Classification, we can exhaustively list/enumerate all possible labels for a given instance; a correct prediction entails mapping an instance to the label which is truly correct</li><li>Regression learning is supervised</li><li>In Regression,&quot;infinitely&quot; many labels are possible, we cannot conceivably enumerate them; a “correct” prediction is when the numeric value is acceptably close to the true value</li></ul><h3 id="featured-data-types" tabindex="-1"><a class="header-anchor" href="#featured-data-types"><span>Featured Data Types</span></a></h3><ol><li>Discrete: Nominal (Categorical)</li></ol><ul><li>Values are distinct symbols, values themselves serve only as labels or names</li><li>No relation is implied among nominal values (no ordering or distance measure)</li><li>Only equality tests can be performed</li><li>e.g. Student Number</li></ul><ol start="2"><li>Ordinal</li></ol><ul><li>An explicit order is imposed on the values</li><li>Addition and subtraction does not make sense</li><li>e.g. Educational Level</li></ul><ol start="3"><li>Continuous: Numeric</li></ol><ul><li>Numeric quantities are real-valued attributes</li><li>All mathematical operations are allowed</li></ul><h2 id="equal-width-vs-equal-frequency-vs-clustering" tabindex="-1"><a class="header-anchor" href="#equal-width-vs-equal-frequency-vs-clustering"><span>Equal Width vs. Equal Frequency vs. Clustering</span></a></h2><table><thead><tr><th>Method</th><th>Equal Width Binning</th><th>Equal Frequency Binning</th><th>Clustering</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Each bin has the same width</td><td>Each bin contains the same number of data points</td><td>Groups data points based on similarity</td></tr><tr><td><strong>Type</strong></td><td>Data discretization</td><td>Data discretization</td><td>Unsupervised learning</td></tr><tr><td><strong>Advantages</strong></td><td>Easy to compute, simple</td><td>Suitable for skewed distributions</td><td>Can detect natural groupings in data</td></tr><tr><td><strong>Disadvantages</strong></td><td>Sparse or dense bins if data density varies</td><td>Uneven bin width, harder to interpret</td><td>May require tuning (e.g., number of clusters)</td></tr><tr><td><strong>Common Algorithms</strong></td><td>Fixed width intervals</td><td>Quantiles-based binning</td><td>K-Means, DBSCAN, Hierarchical Clustering</td></tr><tr><td><strong>Use Cases</strong></td><td>Histogram creation, feature engineering</td><td>Handling skewed data in ML models</td><td>Customer segmentation, anomaly detection</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><p>Equal Width Binning：用于简单离散化，每个 bin 宽度相同，但可能会导致数据密度不均衡。 Equal Frequency Binning：每个 bin 的数据量相等，适合处理偏态数据，但 bin 的宽度不一致，可能难以解释。 Clustering（聚类）：用于无监督学习，根据数据点的相似性自动分组，适合发现隐藏模式，但通常需要调整参数（如 k 值）。</p></div><h2 id="standardization-vs-normalization" tabindex="-1"><a class="header-anchor" href="#standardization-vs-normalization"><span>Standardization vs. Normalization</span></a></h2><table><thead><tr><th><strong>Method</strong></th><th><strong>Standardization (Z-score)</strong></th><th><strong>Min-Max Normalization</strong></th></tr></thead><tbody><tr><td><strong>Formula</strong></td><td>( X&#39; = \frac{X - \mu}{\sigma} )</td><td>( X&#39; = \frac{X - X_{\min}}{X_{\max} - X_{\min}} )</td></tr><tr><td><strong>Range</strong></td><td>Mean = 0, Std = 1</td><td>[0,1] or [-1,1]</td></tr><tr><td><strong>Best for</strong></td><td>Normally distributed data</td><td>Data with fixed bounds</td></tr><tr><td><strong>Sensitive to outliers?</strong></td><td>Less sensitive</td><td>More sensitive</td></tr><tr><td><strong>Formula</strong></td><td>(X - mean) / std</td><td>(X - min) / (max - min)</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><p>Standardization：将数据标准化到均值为 0，标准差为 1 的分布，适用于正态分布的 数据。 Normalization：将数据缩放到 [0,1] 或 [-1,1] 范围内，适用于数据范围固定的数据。</p></div><h2 id="week-2" tabindex="-1"><a class="header-anchor" href="#week-2"><span>Week 2</span></a></h2><h3 id="k-nearest-neighbors-knn" tabindex="-1"><a class="header-anchor" href="#k-nearest-neighbors-knn"><span>K-Nearest Neighbors (KNN)</span></a></h3><ul><li>supervied learning algorithm</li></ul><h4 id="knn-classification" tabindex="-1"><a class="header-anchor" href="#knn-classification"><span>KNN Classification</span></a></h4><ul><li>Return the most common class label among neighbors</li><li>Example: cat vs dog images; text classification; ...</li></ul><h4 id="knn-regression" tabindex="-1"><a class="header-anchor" href="#knn-regression"><span>KNN Regression</span></a></h4><ul><li>Return the average value of among K nearest neighbors</li><li>Example: housing price prediction;</li></ul><h4 id="to-measure-categorical-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-categorical-distance-we-can-use"><span>To measure categorical distance, we can use:</span></a></h4><ul><li>Hamming distance: number of positions where the two strings differ</li><li>Jaccard Similarity: intersection over union of two sets</li></ul><h4 id="to-measure-numerical-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-numerical-distance-we-can-use"><span>To measure numerical distance, we can use:</span></a></h4><ul><li>Manhattan distance: sum of absolute differences between corresponding components</li><li>Euclidean distance: square root of the sum of the squares of the differences between corresponding components</li><li>Cosine distance: 1 minus the cosine of the angle between two vectors</li></ul><h4 id="to-measure-oridinal-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-oridinal-distance-we-can-use"><span>To measure oridinal distance, we can use:</span></a></h4><ul><li>Normalized Ranks: rank each value and normalize them to [0, 1]</li></ul><h4 id="majority-vote" tabindex="-1"><a class="header-anchor" href="#majority-vote"><span>Majority Vote</span></a></h4><h4 id="inverse-distance" tabindex="-1"><a class="header-anchor" href="#inverse-distance"><span>Inverse Distance</span></a></h4><ul><li>Give more weight to the nearer neighbors rather than quantity.</li><li>The bigger the weight, the more important the neighbor is. <img src="/assets/img/Inverse-Distance.857bdd2b.png" alt="Inverse Distance" loading="lazy"></li></ul><h4 id="inverse-linear-distance" tabindex="-1"><a class="header-anchor" href="#inverse-linear-distance"><span>Inverse Linear Distance</span></a></h4><ul><li>Give more weight to the nearer neighbors, but with a decreasing slope.</li><li>The bigger the weight, the more important the neighbor is.</li></ul><table><thead><tr><th>方法名称</th><th>是否考虑距离</th><th>权重形式</th><th>特点</th></tr></thead><tbody><tr><td>Majority Vote</td><td>❌ 不考虑</td><td>所有邻居权重相等</td><td>简单、容易实现，容易受异常值影响</td></tr><tr><td>Inverse Distance</td><td>✅ 强调距离</td><td>权重 = 1 / 距离</td><td>近邻影响大，远邻影响小，权重下降快</td></tr><tr><td>Inverse Linear Distance</td><td>✅ 强调距离</td><td>权重 = 1 - d/D（D是最大距离，d是当前距离）</td><td>更平滑下降，抗异常能力更强</td></tr></tbody></table><h4 id="value-of-k" tabindex="-1"><a class="header-anchor" href="#value-of-k"><span>Value of K</span></a></h4><table><thead><tr><th><strong>K Value</strong></th><th><strong>Bias</strong></th><th><strong>Variance</strong></th><th><strong>Overfitting</strong></th><th><strong>Underfitting</strong></th><th><strong>Best For</strong></th></tr></thead><tbody><tr><td><strong>Small K</strong> (e.g., K=1, K=3)</td><td><strong>Low Bias</strong>: The model can closely follow the data.</td><td><strong>High Variance</strong>: Sensitive to noise and outliers.</td><td>Likely to overfit due to high sensitivity to small fluctuations in the training data.</td><td>Unlikely to underfit unless the data is too noisy or simple.</td><td>- Complex data with clear patterns<br> - When the dataset is relatively small.</td></tr><tr><td><strong>Large K</strong> (e.g., K=10, K=20)</td><td><strong>High Bias</strong>: The model becomes less sensitive to variations in the data.</td><td><strong>Low Variance</strong>: Smoothing out the noise by considering more neighbors.</td><td>Less likely to overfit as it smooths out fluctuations.</td><td>Might underfit if the data has complex relationships or non-linear patterns.</td><td>- Noisy data<br> - When a generalization is more important than capturing every detail.</td></tr><tr><td><strong>Medium K</strong> (e.g., K=5, K=7)</td><td>A balanced approach with moderate bias.</td><td>Balanced variance, aiming for generalization.</td><td>Minimizes both overfitting and underfitting.</td><td>Good compromise between bias and variance.</td><td>- Standard choice for most datasets, balancing generalization and accuracy.</td></tr></tbody></table><h4 id="why-knn" tabindex="-1"><a class="header-anchor" href="#why-knn"><span>Why KNN</span></a></h4><ul><li>Pros</li><li>Intuitive and simple</li><li>No assumptions</li><li>Supports classification and regression</li><li>No training: new data →evolve and adapt immediately</li><li>Cons</li><li>How to decide on best distance functions?</li><li>How to combine multiple neighbors?</li><li>How to select K ?</li><li>Expensive with large (or growing) data sets</li></ul><h4 id="lazy-learning-vs-eager-learning" tabindex="-1"><a class="header-anchor" href="#lazy-learning-vs-eager-learning"><span>Lazy Learning vs. Eager Learning</span></a></h4><table><thead><tr><th>Criteria</th><th>Lazy Learning (e.g., KNN)</th><th>Eager Learning</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Delays learning until a query is made</td><td>Learns from the training data immediately</td></tr><tr><td><strong>Training Phase</strong></td><td>Fast (no model building)</td><td>Slow (model is built during training)</td></tr><tr><td><strong>Prediction Phase</strong></td><td>Slow (requires processing the entire dataset)</td><td>Fast (uses the pre-built model)</td></tr><tr><td><strong>Memory Requirement</strong></td><td>High (stores the entire training dataset)</td><td>Lower (only stores the model)</td></tr><tr><td><strong>Flexibility</strong></td><td>High (can adapt to new data easily)</td><td>Low (requires retraining for new data)</td></tr><tr><td><strong>Example</strong></td><td>K-Nearest Neighbors (KNN)</td><td>Decision Trees, Neural Networks</td></tr></tbody></table><h3 id="probility" tabindex="-1"><a class="header-anchor" href="#probility"><span>Probility</span></a></h3><ul><li>P(A=a): the probability that random variable A takes value a</li><li>0 &lt;= P(A=a) &lt;= 1</li><li>P(True) = 1</li><li>P(False) = 0</li></ul><h4 id="joint-probability" tabindex="-1"><a class="header-anchor" href="#joint-probability"><span>Joint Probability</span></a></h4><ul><li>P(A, B): joint probability of two events A and B</li><li>the probability of both A and B occurring = P(A ∩ B)</li></ul><h4 id="conditional-probability" tabindex="-1"><a class="header-anchor" href="#conditional-probability"><span>Conditional Probability</span></a></h4><ul><li>P(A|B): the probability of A occurring given that B has occurred</li><li>P(A|B) = P(A ∩ B) / P(B)</li></ul><h4 id="independent-probability" tabindex="-1"><a class="header-anchor" href="#independent-probability"><span>Independent Probability</span></a></h4><ul><li>Two events A and B are independent if P(A|B) = P(A)</li><li>P(A, B) = P(A) * P(B)</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h4 id="disjoint" tabindex="-1"><a class="header-anchor" href="#disjoint"><span>Disjoint</span></a></h4><ul><li>P(A∩B)=0</li></ul><h4 id="product-rule" tabindex="-1"><a class="header-anchor" href="#product-rule"><span>Product Rule</span></a></h4><ul><li>P(A, B) = P(A|B) * P(B) = P(B|A) * P(A)</li></ul><h4 id="chain-rule" tabindex="-1"><a class="header-anchor" href="#chain-rule"><span>Chain Rule</span></a></h4><ul><li>P(A,B,C)=P(A)⋅P(B∣A)⋅P(C∣A,B)</li></ul></div><h4 id="bayes-rule" tabindex="-1"><a class="header-anchor" href="#bayes-rule"><span>Bayes&#39; Rule</span></a></h4><ul><li>P(A|B) = ( P(B|A) * P(A) ) / P(B)</li><li>Bayes’ Rule allows us to compute P(A|B) given knowledge of the ‘inverse’ probability P(B|A).</li></ul><h4 id="marginalization" tabindex="-1"><a class="header-anchor" href="#marginalization"><span>Marginalization</span></a></h4><figure><img src="/assets/img/Marginalization.4e6857d9.png" alt="Marginalization" tabindex="0" loading="lazy"><figcaption>Marginalization</figcaption></figure><h4 id="probability-distributions" tabindex="-1"><a class="header-anchor" href="#probability-distributions"><span>Probability Distributions</span></a></h4><ul><li>Probability distributions can be discrete or continuous.</li><li>Discrete Random Variable: Takes on a countable number of distinct values (e.g., number of heads in coin flips).</li><li>Continuous Random Variable: Takes on an infinite number of possible values (e.g., height of students).</li></ul><table><thead><tr><th><strong>Distribution</strong></th><th><strong>Type</strong></th><th><strong>Range</strong></th><th><strong>Parameters</strong></th><th><strong>Formula</strong></th><th><strong>Example</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td><strong>Normal</strong></td><td>Continuous</td><td>−∞ to +∞</td><td>Mean μ, Variance σ²</td><td><code>P(x) = (1 / √(2πσ²)) * exp(-((x - μ)² / (2σ²)))</code></td><td>Human height, exam scores</td><td>Linear regression, Gaussian models</td></tr><tr><td><strong>Bernoulli</strong></td><td>Discrete</td><td>0, 1</td><td>Probability p</td><td><code>P(X = k) = p^k (1 - p)^(1 - k)</code></td><td>Coin flip</td><td>Binary classification</td></tr><tr><td><strong>Binomial</strong></td><td>Discrete</td><td>0 to n</td><td>Number of trials n, Success probability p</td><td><code>P(k) = C(n, k) * p^k * (1 - p)^(n - k)</code></td><td>Number of heads in 10 coin flips</td><td>Binary classification, hypothesis testing</td></tr><tr><td><strong>Multinomial</strong></td><td>Discrete</td><td>0 to n for each category</td><td>Number of trials n, Probabilities p₁, ..., pₖ</td><td><code>P(x₁, ..., xₖ) = (n! / (x₁!x₂!...xₖ!)) * ∏(pᵢ^xᵢ)</code></td><td>Rolling a dice multiple times</td><td>Text classification, NLP</td></tr><tr><td><strong>Categorical</strong></td><td>Discrete</td><td>1 to k</td><td>Probabilities p₁, ..., pₖ</td><td><code>P(X = i) = pᵢ</code></td><td>Choosing a color from a set of options</td><td>Classification, clustering</td></tr></tbody></table><h2 id="week-3" tabindex="-1"><a class="header-anchor" href="#week-3"><span>Week 3</span></a></h2><h3 id="zero-r" tabindex="-1"><a class="header-anchor" href="#zero-r"><span>Zero-R</span></a></h3><ul><li>A simple baseline model that predicts the most frequent class in the training data.</li></ul><h3 id="one-r" tabindex="-1"><a class="header-anchor" href="#one-r"><span>One-R</span></a></h3><ul><li>Also known as Decision stom</li><li>Uses only one feature (“best” feature) to build a model</li></ul><h3 id="desicion-trees" tabindex="-1"><a class="header-anchor" href="#desicion-trees"><span>Desicion Trees</span></a></h3><figure><img src="/assets/img/Decision-Tree-Example.6ee2db6e.png" alt="Decision Tree Example" tabindex="0" loading="lazy"><figcaption>Decision Tree Example</figcaption></figure><ul><li>Choose an attribute to partition the data at the node such that each partition is as pure (homogeneous) as possible.</li><li>In each partition, most of the instances should belong to as few classes as possible</li><li>Each partition should be as large as possible.</li></ul><h3 id="id3-iterative-dichotomiser-3" tabindex="-1"><a class="header-anchor" href="#id3-iterative-dichotomiser-3"><span>ID3 (Iterative Dichotomiser 3)</span></a></h3><ul><li>A top-down approach that splits the data into smaller subsets based on the value of a chosen feature.</li></ul><h4 id="entropy-measure-of-uncertainty-the-expected-average-level-of-uncertainty-surprise" tabindex="-1"><a class="header-anchor" href="#entropy-measure-of-uncertainty-the-expected-average-level-of-uncertainty-surprise"><span>Entropy (measure of uncertainty. The expected (average) level of uncertainty (surprise))</span></a></h4><ul><li>For a Low probability event: if it happens, it’s big news! Big surprise! <strong>High information!</strong></li><li>For a High probability event: it was likely to happen anyway. Not very surprising. <strong>Low information!</strong></li><li>Higher H means more uncertain.</li><li>Low entropy means high certainty</li><li>High entropy means high uncertainty <img src="/assets/img/Entropy-Example.6d4874bc.png" alt="Entropy Example" loading="lazy"></li></ul><h4 id="conditional-entropy-measures-the-amount-of-uncertainty-in-x-given-y" tabindex="-1"><a class="header-anchor" href="#conditional-entropy-measures-the-amount-of-uncertainty-in-x-given-y"><span>Conditional Entropy measures the amount of uncertainty in X given Y.</span></a></h4><h4 id="information-gain-measure-of-the-reduction-in-entropy-after-splitting" tabindex="-1"><a class="header-anchor" href="#information-gain-measure-of-the-reduction-in-entropy-after-splitting"><span>Information Gain (measure of the reduction in entropy after splitting)</span></a></h4><ul><li>Information gain measures the reduction in entropy about the target variable achieved by partitioning the data based on a given feature.</li><li>Choose the largest as information gain to split the tree.</li></ul><h4 id="shortcomings-of-ig" tabindex="-1"><a class="header-anchor" href="#shortcomings-of-ig"><span>Shortcomings of IG</span></a></h4><ul><li>Overfitting: Greedy algorithm may choose a feature that is too specific and does not generalize well to unseen data.</li><li>C4.5: use gain ratio to choose the best feature to split the tree. <ul><li>Gain ratio (GR) reduces the bias for information gain towards highly branching attributes by normalising relative to the split information</li><li>Split info (SI) is the entropy of a given split (evenness of the distribution of instances to attribute values)</li><li>GR = IG / SI</li><li>Choose the largest as GR to split the tree.</li><li>如果下面的attribute的选项是pure的，就不选择这个选项作为继续的分支，然后在剩余的attribute里选择GR最大的attribute作为继续的分支。如果下面的attribute全是pure的，就到达了叶子节点。</li></ul></li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><p>计算 <strong>信息增益（Information Gain, IG）</strong>、<strong>增益率（Gain Ratio, GR）</strong> 和 <strong>分裂信息（Split Information, SI）</strong>，我们需要以训练集中的“PLAY”列为目标变量（label）来进行分类评估。</p><p>下面是逐步讲解和计算方法：</p><hr><h2 id="✅-1-基础准备" tabindex="-1"><a class="header-anchor" href="#✅-1-基础准备"><span>✅ 1. 基础准备</span></a></h2><p>训练数据（6个实例）：</p><table><thead><tr><th>ID</th><th>Outlook</th><th>Temp</th><th>Humidity</th><th>Wind</th><th>PLAY</th></tr></thead><tbody><tr><td>A</td><td>s</td><td>h</td><td>h</td><td>F</td><td>N</td></tr><tr><td>B</td><td>s</td><td>h</td><td>h</td><td>T</td><td>N</td></tr><tr><td>C</td><td>o</td><td>h</td><td>h</td><td>F</td><td>Y</td></tr><tr><td>D</td><td>r</td><td>m</td><td>h</td><td>F</td><td>Y</td></tr><tr><td>E</td><td>r</td><td>c</td><td>n</td><td>F</td><td>Y</td></tr><tr><td>F</td><td>r</td><td>c</td><td>n</td><td>T</td><td>N</td></tr></tbody></table><ul><li>正类（PLAY=Y）：C、D、E（3个）</li><li>负类（PLAY=N）：A、B、F（3个）</li></ul><hr><h2 id="✅-2-计算总熵-entropy-s" tabindex="-1"><a class="header-anchor" href="#✅-2-计算总熵-entropy-s"><span>✅ 2. 计算总熵 Entropy(S)</span></a></h2><p>$$ Entropy(S) = -p_+\log_2(p_+) - p_-\log_2(p_-) $$</p><p>$$ p_+ = 3/6 = 0.5,\quad p_- = 0.5 $$</p><p>$$ Entropy(S) = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 1 $$</p><hr><h2 id="✅-3-对每个属性计算信息增益-以-outlook-为例" tabindex="-1"><a class="header-anchor" href="#✅-3-对每个属性计算信息增益-以-outlook-为例"><span>✅ 3. 对每个属性计算信息增益（以 Outlook 为例）</span></a></h2><h3 id="outlook-的取值有-s-o-r" tabindex="-1"><a class="header-anchor" href="#outlook-的取值有-s-o-r"><span>Outlook 的取值有：<code>s</code>, <code>o</code>, <code>r</code></span></a></h3><ul><li><p><strong>s（A, B）</strong>：PLAY = N, N → Entropy = 0</p></li><li><p><strong>o（C）</strong>：PLAY = Y → Entropy = 0</p></li><li><p><strong>r（D, E, F）</strong>：PLAY = Y, Y, N →</p><p>$$ p_+ = 2/3, p_- = 1/3<br> \Rightarrow Entropy = -\frac{2}{3}\log_2\frac{2}{3} - \frac{1}{3}\log_2\frac{1}{3} ≈ 0.918 $$</p></li></ul><p>加权总熵：</p><p>$$ Entropy_{Outlook} = \frac{2}{6} \cdot 0 + \frac{1}{6} \cdot 0 + \frac{3}{6} \cdot 0.918 ≈ 0.459 $$</p><p><strong>信息增益</strong>：</p><p>$$ IG(Outlook) = Entropy(S) - Entropy_{Outlook} = 1 - 0.459 = 0.541 $$</p><hr><h2 id="✅-4-split-information-si" tabindex="-1"><a class="header-anchor" href="#✅-4-split-information-si"><span>✅ 4. Split Information（SI）</span></a></h2><p>$$ SI(Outlook) = -\sum_i \frac{|S_i|}{|S|} \log_2 \frac{|S_i|}{|S|} $$</p><ul><li>s: 2/6 → log₂(2/6)</li><li>o: 1/6 → log₂(1/6)</li><li>r: 3/6 → log₂(3/6)</li></ul><p>$$ SI = -(\frac{2}{6} \log_2 \frac{2}{6} + \frac{1}{6} \log_2 \frac{1}{6} + \frac{3}{6} \log_2 \frac{3}{6})<br> \approx -(0.389 + 0.431 + 0.5) = 1.32 $$</p><hr><h2 id="✅-5-gain-ratio-gr" tabindex="-1"><a class="header-anchor" href="#✅-5-gain-ratio-gr"><span>✅ 5. Gain Ratio（GR）</span></a></h2><p>$$ GR = \frac{IG}{SI} = \frac{0.541}{1.32} ≈ 0.41 $$</p><hr><h2 id="✅-类似方法可以应用于其他特征" tabindex="-1"><a class="header-anchor" href="#✅-类似方法可以应用于其他特征"><span>✅ 类似方法可以应用于其他特征：</span></a></h2><ul><li>按照属性值分组，计算每组的熵和加权熵</li><li>再算信息增益（IG）</li><li>再算分裂信息（SI）</li><li>最后算增益率（GR）</li></ul></div><h3 id="naive-bayes-theory" tabindex="-1"><a class="header-anchor" href="#naive-bayes-theory"><span>Naive Bayes Theory</span></a></h3><div class="hint-container info"><p class="hint-container-title">Info</p><p>arg max: argument of maximum value</p></div><ul><li>Supervied ML method</li></ul><h4 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>Example:</span></a></h4><p><img src="/assets/img/Naive-Bayes-Example-1-1.2ea50406.png" alt="Naive Bayes Example1-1" loading="lazy"><img src="/assets/img/Naive-Bayes-Example-1-2.b15986db.png" alt="Naive Bayes Example1-2" loading="lazy"></p><ul><li>If any term P(xm|y ) = 0 then the class probability P(y|x ) = 0</li><li>To solve this: use Laplace smoothing.</li></ul><ol><li>First Solution: We can assign a (small) positive probability 𝜀 to every unseen class-feature combination</li><li>Second Solution: We can add a “pseudocount” α to each feature count observed during training, often is 1.</li></ol><ul><li>Probabilities are changed drastically when there are few instances; with a large number of instances, the changes are small</li><li>Laplace smoothing (and smoothing in general) <strong>reduces variance</strong> of the NB classifier because it reduces sensitivity to individual (non-)observations in the training data</li></ul><h3 id="different-naive-bayes" tabindex="-1"><a class="header-anchor" href="#different-naive-bayes"><span>Different Naive Bayes</span></a></h3><p>Naïve Bayes classifiers have several key variants that differ based on how they model the distribution of features. Below is a comparison of the most common types:</p><table><thead><tr><th>Variant</th><th>Assumption on Feature Distribution</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>Gaussian Naïve Bayes (GNB)</strong></td><td>Assumes features follow a Gaussian (normal) distribution.</td><td>Suitable for continuous data, often used in text classification and real-world datasets with normally distributed features.</td></tr><tr><td><strong>Multinomial Naïve Bayes (MNB)</strong></td><td>Assumes feature counts follow a multinomial distribution.</td><td>Best for text classification (e.g., spam detection, document classification) where features are word counts or term frequencies.</td></tr><tr><td><strong>Bernoulli Naïve Bayes (BNB)</strong></td><td>Assumes binary feature presence (1 = present, 0 = absent).</td><td>Used in binary text classification (e.g., sentiment analysis, spam filtering), where features represent whether a word appears in a document.</td></tr><tr><td><strong>Complement Naïve Bayes (CNB)</strong></td><td>A modification of Multinomial Naïve Bayes, designed to handle class imbalances.</td><td>Works better for imbalanced datasets and improves accuracy by adjusting feature probabilities.</td></tr><tr><td><strong>Categorical Naïve Bayes</strong></td><td>Assumes features are discrete categorical variables.</td><td>Used for classification tasks with categorical inputs that are not necessarily text-based.</td></tr></tbody></table><p>Each variant modifies the way probabilities are calculated based on the data&#39;s nature, making Naïve Bayes a flexible and effective algorithm for different types of classification tasks.</p><h3 id="conclusion-of-naive-bayes" tabindex="-1"><a class="header-anchor" href="#conclusion-of-naive-bayes"><span>Conclusion of Naive Bayes</span></a></h3><ol><li>Why does it work given that it’s a blatantly wrong model of the data?</li></ol><ul><li>we don’t need the true distribution over P(y|x ), we just need to be able to identify the most likely outcome</li></ul><ol start="2"><li>Advantages of Naive Bayes</li></ol><ul><li>easy to build and estimate</li><li>easy to scale to many feature dimensions (e.g., words in the vocabulary) and data sizes</li><li>reasonably easy to explain why a specific class was predicted</li><li>good starting point for a classification project</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h3 id="how-nb-works" tabindex="-1"><a class="header-anchor" href="#how-nb-works"><span>How NB Works</span></a></h3><ul><li>Calculate Prior Probabilities: Compute the probability of each class based on the training data.</li><li>Compute Likelihood: Estimate the probability of features given each class using the conditional probability formula.</li><li>Apply Bayes’ Theorem: Use Bayes&#39; rule to compute the posterior probability for each class.</li><li>Classify: Assign the class with the highest posterior probability to the new instance.</li></ul></div><h2 id="week-4" tabindex="-1"><a class="header-anchor" href="#week-4"><span>Week 4</span></a></h2><h3 id="linear-regression" tabindex="-1"><a class="header-anchor" href="#linear-regression"><span>Linear Regression</span></a></h3><ul><li>A supervised learning algorithm that models the relationship between a scalar dependent variable (y) and one or more explanatory variables (X1, X2,..., Xn).</li><li>The model assumes that the relationship between the dependent and independent variables is linear.</li><li>The goal is to find the best line that fits the data.</li></ul><h4 id="loss-function" tabindex="-1"><a class="header-anchor" href="#loss-function"><span>Loss Function</span></a></h4><ul><li>The loss function measures the error between the predicted values and the actual values.</li><li>The loss function is used to optimize the model parameters (i.e., the weights and biases) to minimize the loss.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><ul><li>When using a regression model for prediction, it is important to only predict within the relevant range of data</li><li>We should not try to extrapolate beyond the range of observed X’s</li><li>Make sure independent variables are NOT highly correlated with each other, otherwise the model becomes unstable</li></ul></div><h3 id="optimization" tabindex="-1"><a class="header-anchor" href="#optimization"><span>Optimization</span></a></h3><ul><li>Find parameter values 𝜽 that maximize (or minimize) the value of a function f(𝜽)</li></ul><h3 id="iterative-optimization" tabindex="-1"><a class="header-anchor" href="#iterative-optimization"><span>Iterative Optimization</span></a></h3><h4 id="closed-form-solutions" tabindex="-1"><a class="header-anchor" href="#closed-form-solutions"><span>Closed-form solutions</span></a></h4><ul><li>Previously, we computed the closed form solution for the MLE of the binomial distribution</li><li>We follow our recipe, and arrive at a single solution</li></ul><h4 id="unfortunately-life-is-not-always-as-easy" tabindex="-1"><a class="header-anchor" href="#unfortunately-life-is-not-always-as-easy"><span>Unfortunately, life is not always as easy</span></a></h4><ul><li>Often, no closed-form solution exists</li><li>Instead, we have to iteratively improve our estimate of θˆ until we arrive at a satisfactory solution</li><li>Gradient descent is one popular iterative optimization method</li></ul><h3 id="gradient-descent" tabindex="-1"><a class="header-anchor" href="#gradient-descent"><span>Gradient Descent</span></a></h3><ul><li>Descending a mountain (aka. our function) as fast as possible: atevery position take the next step that takes you most directly into the valley</li></ul><p><img src="/assets/img/gradient-descent.587a0aa0.png" alt="Gradient Descent" loading="lazy"><img src="/assets/img/gd-algo.e7a8b6b4.png" alt="Gradient Descent Algorithm" loading="lazy"></p><div class="hint-container info"><p class="hint-container-title">Info</p><ol><li>with an appropriate learning rate, GD will find the global minimum for differentiable convex functions (bowl shape)</li><li>with an appropriate learning rate, GD will find a local minimum for differentiable non-convex functions</li></ol></div><h3 id="logistic-regression" tabindex="-1"><a class="header-anchor" href="#logistic-regression"><span>Logistic Regression</span></a></h3><table><thead><tr><th><strong>Comparison</strong></th><th><strong>Naïve Bayes</strong></th><th><strong>Logistic Regression</strong></th></tr></thead><tbody><tr><td><strong>Model Type</strong></td><td>Generative Model</td><td>Discriminative Model</td></tr><tr><td><strong>Probability Learned</strong></td><td>( P(x, y) ) (Joint Probability)</td><td>( P(y</td></tr><tr><td><strong>Assumptions</strong></td><td>Assumes feature independence</td><td>No specific feature independence assumption</td></tr><tr><td><strong>Computational Complexity</strong></td><td>Low, fast computation</td><td>Higher, requires gradient descent</td></tr><tr><td><strong>Use Cases</strong></td><td>Text classification (e.g., spam detection)</td><td>Tasks requiring feature relationship modeling</td></tr><tr><td><strong>Suitable for Large Datasets?</strong></td><td>Yes, simple computation</td><td>Yes, but computationally more intensive</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><h2 id="understanding-odds-and-log-odds-in-logistic-regression" tabindex="-1"><a class="header-anchor" href="#understanding-odds-and-log-odds-in-logistic-regression"><span>Understanding Odds and Log Odds in Logistic Regression</span></a></h2><h2 id="_1-what-are-odds" tabindex="-1"><a class="header-anchor" href="#_1-what-are-odds"><span>1. What are Odds?</span></a></h2><p>Odds represent the ratio of the probability of an event occurring to the probability of it not occurring. Mathematically, odds are defined as:</p><p>[ \text{odds} = \frac{P}{1 - P} ]</p><p>where:</p><ul><li>( P ) is the probability of the event occurring.</li><li>( 1 - P ) is the probability of the event not occurring.</li></ul><h3 id="types-of-odds" tabindex="-1"><a class="header-anchor" href="#types-of-odds"><span><strong>Types of Odds</strong></span></a></h3><ul><li><strong>Odds against an event</strong>: When ( 0 &lt; \text{odds} &lt; 1 ), meaning the event is less likely to happen than not.</li><li><strong>Odds in favor of an event</strong>: When ( \text{odds} &gt; 1 ), meaning the event is more likely to happen than not.</li></ul><p><strong>Example:</strong></p><ul><li>If an event has a <strong>60% chance</strong> of occurring (( P = 0.6 )), the odds are: [ \text{odds} = \frac{0.6}{1 - 0.6} = \frac{0.6}{0.4} = 1.5 ] This means the event is <strong>1.5 times more likely</strong> to occur than not.</li></ul><h2 id="_2-why-use-log-odds-logit-function" tabindex="-1"><a class="header-anchor" href="#_2-why-use-log-odds-logit-function"><span>2. Why Use Log Odds (Logit Function)?</span></a></h2><p>Since odds can range from <strong>0 to infinity</strong>, they are not ideal for direct modeling in a regression setting. Instead, we take the <strong>logarithm of odds</strong>, known as the <strong>logit function</strong>:</p><p>[ \text{log odds} = \log \left(\frac{P}{1 - P}\right) ]</p><h3 id="advantages-of-log-odds" tabindex="-1"><a class="header-anchor" href="#advantages-of-log-odds"><span><strong>Advantages of Log Odds:</strong></span></a></h3><ol><li><p><strong>Transforms probability into an unrestricted range</strong></p><ul><li>( P ) is always between ( 0 ) and ( 1 ), but log odds can take any value from <strong>(-\infty) to (+\infty)</strong>.</li><li>This makes it easier to model using linear regression techniques.</li></ul></li><li><p><strong>Handles Non-Linearity in Probability</strong></p><ul><li>The relationship between probability and log odds is <strong>non-linear</strong>, but when transformed to log odds, it becomes <strong>linear</strong>.</li></ul></li></ol><p><strong>Example Calculation:</strong></p><ul><li>If the event has a probability of ( P = 0.8 ): [ \text{odds} = \frac{0.8}{1 - 0.8} = \frac{0.8}{0.2} = 4 ] <ul><li>Taking the natural logarithm: [ \log(4) \approx 1.386 ]</li><li>Now, instead of dealing with probabilities, we can work with a linear scale.</li></ul></li></ul><h2 id="_3-connection-to-logistic-regression" tabindex="-1"><a class="header-anchor" href="#_3-connection-to-logistic-regression"><span>3. Connection to Logistic Regression</span></a></h2><p>In <strong>logistic regression</strong>, we model the probability of an event occurring using the equation:</p><p>[ \log \left(\frac{P}{1 - P}\right) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n ]</p><p>This means:</p><ul><li>Instead of predicting probabilities directly, logistic regression predicts <strong>log odds</strong>, which follow a linear relationship with input features.</li><li>We then use the <strong>sigmoid function</strong> to convert log odds back into probabilities.</li></ul><h2 id="_4-conclusion" tabindex="-1"><a class="header-anchor" href="#_4-conclusion"><span>4. Conclusion</span></a></h2><ul><li><strong>Odds</strong> measure how likely an event is compared to it not happening.</li><li><strong>Log odds (logit function)</strong> transform probabilities into a linear form, making them easier to model.</li><li>Logistic regression leverages log odds to predict probabilities effectively.</li></ul><p>Understanding this transformation is key to interpreting logistic regression results and making informed predictions.</p></div><ul><li>Logit function: The logarithm of the odds.</li></ul><h3 id="softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution" tabindex="-1"><a class="header-anchor" href="#softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution"><span>Softmax function: The function that converts a vector of real numbers to a probability distribution.</span></a></h3><h3 id="logistic-regression-summary" tabindex="-1"><a class="header-anchor" href="#logistic-regression-summary"><span>Logistic Regression Summary</span></a></h3><ol><li>Pros</li></ol><ul><li>Probabilistic interpretation</li><li>No restrictive assumptions on features</li><li>Often outperforms Naive Bayes</li><li>Particularly suited to frequency-based features (so, popular in NLP)</li></ul><ol start="2"><li>Cons</li></ol><ul><li>Can only learn linear feature-data relationships</li><li>Some feature scaling issues</li><li>Often needs a lot of data to work well</li><li>Regularisation a nuisance, but important since overfitting can be a big problem</li></ul><h2 id="week-5" tabindex="-1"><a class="header-anchor" href="#week-5"><span>Week 5</span></a></h2><h3 id="support-vector-machines-svm" tabindex="-1"><a class="header-anchor" href="#support-vector-machines-svm"><span>Support Vector Machines (SVM)</span></a></h3><h4 id="classifier" tabindex="-1"><a class="header-anchor" href="#classifier"><span>Classifier</span></a></h4><ul><li>A linear classifier (through origin) with parameters divides the space into positive and negative halves</li></ul><h3 id="hard-margin" tabindex="-1"><a class="header-anchor" href="#hard-margin"><span>Hard Margin</span></a></h3><figure><img src="/assets/img/margin.188baf72.png" alt="Hard Margin" tabindex="0" loading="lazy"><figcaption>Hard Margin</figcaption></figure><ul><li>The margin is the distance between the decision boundary and the closest data point.</li><li>The goal is to maximize the margin while keeping the data points as far away from the decision boundary as possible.</li><li>The decision boundary is the line that separates the positive and negative data points.</li><li>It is called hard margin because the data points must be correctly classified on both sides of the decision boundary.</li></ul><h4 id="soft-margin" tabindex="-1"><a class="header-anchor" href="#soft-margin"><span>Soft Margin</span></a></h4><ul><li>The margin is softened by introducing a penalty term that depends on the distance of the data points from the decision boundary.</li><li>The goal is to minimize the margin while keeping the data points as far away from the decision boundary as possible.</li><li>The decision boundary is the line that separates the positive and negative data points.</li><li>It is called soft margin because the data points can be misclassified on both sides of the decision boundary.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h3 id="kernel-trick" tabindex="-1"><a class="header-anchor" href="#kernel-trick"><span>Kernel Trick</span></a></h3><ul><li>The kernel trick is a way to transform non-linear data into a higher-dimensional space where it becomes linearly separable.</li><li>The kernel function is a similarity function that measures the similarity between two data points in the original space.</li><li>The kernel trick allows us to use non-linear models in a linear model.</li></ul></div><h3 id="applications-of-svm" tabindex="-1"><a class="header-anchor" href="#applications-of-svm"><span>Applications of SVM</span></a></h3><h4 id="multiclass-problems" tabindex="-1"><a class="header-anchor" href="#multiclass-problems"><span>Multiclass problems</span></a></h4><ol><li><strong>Introduce parameter vectors</strong>: If there are ( k ) categories, we introduce a parameter vector ( \theta_1, \theta_2, \ldots, \theta_k ) for each category.</li><li><strong>Jointly learn parameters</strong>: Jointly learn these parameters by ensuring that the discriminant function associated with the correct category has the highest value. The goal is to minimize the following expression: [ \frac{1}{2} \sum_{y=1}^{k} | \theta_y |^2 ] The constraint is that for all ( y&#39; \neq y_i ) and ( i = 1, \ldots, n ), it satisfies: [ (\theta_{y_i} \cdot x_i) \geq (\theta_{y&#39;} \cdot x_i) + 1 ]</li><li><strong>Predict new examples</strong>: Predict the label of a new example based on the following formula: [ \hat{y} = \arg\max_{y=1,\ldots,k} (\theta^*_y \cdot x) ]</li></ol><h4 id="rating-problems" tabindex="-1"><a class="header-anchor" href="#rating-problems"><span>Rating problems</span></a></h4><h4 id="ordinal-regression-problems" tabindex="-1"><a class="header-anchor" href="#ordinal-regression-problems"><span>Ordinal regression problems</span></a></h4><ul><li>Target variable: The target variable in ordinal regression is an ordinal variable, where the categories have a certain order, such as education level (elementary, middle school, high school, university, graduate) or movie ratings (1 star to 5 stars).</li><li>Model purpose: Unlike multiclass classification problems, ordinal regression models not only predict the correct category but also the order relationship between categories.</li></ul><h4 id="translate-each-rating-into-a-set-of-binary-labels" tabindex="-1"><a class="header-anchor" href="#translate-each-rating-into-a-set-of-binary-labels"><span>Translate each rating into a set of binary labels</span></a></h4><ul><li><p>We can convert these labels into binary labels. For example, we can divide the ratings into two categories: &quot;like&quot; and &quot;dislike&quot;. Here is a simple conversion example:</p></li><li><p>1 star and 2 stars: Dislike (represented by 0)</p></li><li><p>3 stars: May require additional processing as it can be considered neutral or context-dependent</p></li><li><p>4 stars and 5 stars: Like (represented by 1)</p></li><li><p>Alternatively, we can create a binary label for each rating level as follows:</p></li><li><p>1 star: [1, 0, 0, 0, 0]</p></li><li><p>2 stars: [0, 1, 0, 0, 0]</p></li><li><p>3 stars: [0, 0, 1, 0, 0]</p></li><li><p>4 stars: [0, 0, 0, 1, 0]</p></li><li><p>5 stars: [0, 0, 0, 0, 1]</p></li></ul><h3 id="ranking" tabindex="-1"><a class="header-anchor" href="#ranking"><span>Ranking</span></a></h3><ul><li>In machine learning, ranking tasks typically involve ordering a set of items such that certain items have higher priority over others. SVM can address ranking problems by modifying the standard binary classification SVM, a method commonly known as RankSVM. RankSVM learns a ranking function through pairwise comparisons, ensuring that positive examples rank higher than negative ones.</li></ul><h3 id="structured-prediction" tabindex="-1"><a class="header-anchor" href="#structured-prediction"><span>Structured prediction</span></a></h3><ul><li>Structured prediction is a branch of machine learning that involves predicting structured outputs, such as sequences, trees, or graphs. SVM can be extended to handle these problems through Structured SVM. Structured SVM learns a prediction function by maximizing the margin, which can output entire structures rather than simple class labels. This approach is particularly useful in fields like natural language processing, bioinformatics, and computer vision.</li></ul><p><strong>The core idea of Support Vector Machines (SVM) is to find an optimal hyperplane that best separates data points of different classes while maximizing the margin between the classes. This margin is the distance from the hyperplane to the nearest training data point. The goal of SVM is to find a hyperplane with the largest margin, which can improve the model&#39;s generalization ability and reduce the risk of overfitting.</strong></p><h2 id="week-6" tabindex="-1"><a class="header-anchor" href="#week-6"><span>Week 6</span></a></h2><div class="hint-container info"><p class="hint-container-title">Info</p><table><thead><tr><th>方法</th><th>描述</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>Holdout</strong></td><td>简单划分为训练+测试</td><td>快速、实现简单</td><td>结果可能不稳定</td></tr><tr><td><strong>Stratification</strong></td><td>分层抽样，保持类别分布</td><td>保证各类别比例均衡</td><td>通常与其他方法结合使用</td></tr><tr><td><strong>Cross-validation</strong></td><td>多次划分训练+验证，轮流训练</td><td>结果稳健、泛化更好</td><td>计算成本较高（训练多次模型）</td></tr></tbody></table><table><thead><tr><th>Scenario</th><th>Holdout</th><th>Cross-Validation</th></tr></thead><tbody><tr><td>Large dataset (millions of records)</td><td>✅ Works well</td><td>❌ Too slow</td></tr><tr><td>Small dataset (few thousand samples)</td><td>❌ Might not be reliable</td><td>✅ More accurate</td></tr><tr><td>Need fast evaluation</td><td>✅ Quick &amp; simple</td><td>❌ Computationally heavy</td></tr><tr><td>Hyperparameter tuning</td><td>❌ Risky (prone to overfitting)</td><td>✅ More stable</td></tr><tr><td>Deep learning models (large compute cost)</td><td>✅ Saves time</td><td>❌ Too expensive</td></tr></tbody></table></div><h3 id="evaluation" tabindex="-1"><a class="header-anchor" href="#evaluation"><span>Evaluation</span></a></h3><p>if our model will perform effectively on unseen data</p><h4 id="holdout" tabindex="-1"><a class="header-anchor" href="#holdout"><span>Holdout</span></a></h4><ul><li>Split the data randomly into two parts: a training set and a test set.</li><li>Train the model on the training set and evaluate it on the test set.</li></ul><h4 id="holdout-weakness" tabindex="-1"><a class="header-anchor" href="#holdout-weakness"><span>Holdout Weakness</span></a></h4><ul><li>The size of the split affects the estimate of the model’s behaviour: Lots of test instances, and few training instances → , the learner doesn’t have enough information to build an accurate model. Lots of training instances, and few test instances → learner builds an accurate model, but test data might not be representative (so estimates of performance can be too high/too low)</li><li>Bias in Sampling: Random sampling of data can lead to different distribution in train and test datasets</li></ul><h4 id="stratification" tabindex="-1"><a class="header-anchor" href="#stratification"><span>Stratification</span></a></h4><ul><li>Stratification ensures that each fold or partition of the data maintains the same class distribution as the original dataset</li></ul><h4 id="stratification-weakness" tabindex="-1"><a class="header-anchor" href="#stratification-weakness"><span>Stratification Weakness</span></a></h4><ul><li>Complexity: Can be more complex to implement compared to simple random sampling.</li><li>Inefficient Resource Utilization: some data is only used for training and some only for testing</li></ul><h4 id="k-fold-cross-validation" tabindex="-1"><a class="header-anchor" href="#k-fold-cross-validation"><span>k-fold Cross-validation</span></a></h4><ul><li>Divide the data into k equal parts, and use k-1 parts for training and 1 part for testing.</li><li>Repeat k times, each time using a different part for testing and the remaining parts for training.</li><li>The average performance of the k models is the estimate of the performance of the model on the entire dataset.</li></ul><h4 id="k-fold-cross-validation-weakness" tabindex="-1"><a class="header-anchor" href="#k-fold-cross-validation-weakness"><span>k-fold Cross-validation Weakness</span></a></h4><ul><li>Fewer folds: more instances per partition, more variance in performance estimates</li><li>More folds: fewer instances per partition, less variance but slower</li></ul><h4 id="hyperparameter-tuning" tabindex="-1"><a class="header-anchor" href="#hyperparameter-tuning"><span>Hyperparameter Tuning</span></a></h4><ul><li>If the Decision Tree is too short (Depth is too small), The DT would be too simple (Something like 1-R)</li><li>If the Decision Tree is too long (Depth is too big), The DT would be too complex and cannot generalize well</li><li>To decide about the best depth for a tree (and other hyperparameters), we need a way to measure how well our tree can - edict labels for new, unseen data.</li><li>But… If we use our Test data for hyperparameter tuning, then we don’t have any other (unseen) data to test the performance of the final model (after the hyperparameter tuning) → It will cause data leakage</li><li>We need a third set → Validation data, or use cross-validation to split the data into training, validation, and test sets.</li></ul><h3 id="classification-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#classification-evaluation-metrics"><span>Classification Evaluation Metrics</span></a></h3><ul><li>We can present all possible classification results a two-class problem in the following <strong>confusion matrix</strong>.</li><li><strong>Confusion matrix is for classification problems.</strong></li><li>True Positive (TP): The model correctly predicts the positive class.</li><li>False Positive (FP): The model incorrectly predicts the positive class.</li><li>True Negative (TN): The model correctly predicts the negative class.</li><li>False Negative (FN): The model incorrectly predicts the negative class.</li></ul><h4 id="recall" tabindex="-1"><a class="header-anchor" href="#recall"><span>Recall</span></a></h4><ul><li>Recall: From the cases that are actually positive, what percentage has been correctly identified (predicted positive) by our classifier. <strong>Recall = TP / (TP + FN)</strong></li></ul><h4 id="precision" tabindex="-1"><a class="header-anchor" href="#precision"><span>Precision</span></a></h4><ul><li>From the cases that are predicted positive, what percentage are actually positive. <strong>Precision = TP / (TP + FP</strong>)</li></ul><h4 id="f1-score" tabindex="-1"><a class="header-anchor" href="#f1-score"><span>F1-score</span></a></h4><ul><li>A popular metric that can help with finding a balance between Precision &amp; Recall is F1-Score. F1-Score is the harmonic mean of Precision and Recall. <strong>F1-Score = 2 * (Precision * Recall) / (Precision + Recall)</strong></li></ul><h3 id="averaging-methods" tabindex="-1"><a class="header-anchor" href="#averaging-methods"><span>Averaging methods</span></a></h3><p>In multi-class problems, Macro Averaging, Micro Averaging, and Weighted Averaging are three methods for calculating the average precision or recall. These methods are particularly useful in evaluating the performance of classification models, especially in datasets with imbalanced classes. Here is a brief description of these three methods:</p><h3 id="_1-macro-averaging" tabindex="-1"><a class="header-anchor" href="#_1-macro-averaging"><span>1. Macro Averaging</span></a></h3><p>Macro Averaging first calculates the metrics (such as precision, recall, or F1 score) for each class, and then calculates the simple arithmetic average of these metrics. It treats each class as equally important, without considering the actual frequency or size of the classes. The calculation formula is as follows: $$ \text{Macro Average} = \frac{1}{N} \sum_{i=1}^{N} \text{metric}_i $$ Where, $N$ is the total number of classes, and $\text{metric}_i$ is the value of the metric for the $i$-th class. <strong>Advantages</strong>: Each class is treated equally, so this may be a better metric for imbalanced datasets. <strong>Disadvantages</strong>: If some classes are very few, they may disproportionately affect the overall performance evaluation.</p><h3 id="_2-micro-averaging" tabindex="-1"><a class="header-anchor" href="#_2-micro-averaging"><span>2. Micro Averaging</span></a></h3><p>Micro Averaging first calculates the total precision, total recall, and total F1 score for all classes, and then calculates the arithmetic average of these totals. It calculates the metrics by considering individual predictions in each class, so it takes into account the actual frequency of the classes. The calculation formula is as follows: $$ \text{Micro Average Precision} = \frac{\sum_{i=1}^{N} TP_i}{\sum_{i=1}^{N} TP_i + \sum_{i=1}^{N} FP_i} $$ $$ \text{Micro Average Recall} = \frac{\sum_{i=1}^{N} TP_i}{\sum_{i=1}^{N} TP_i + \sum_{i=1}^{N} FN_i} $$ Where, $TP_i$, $FP_i$, and $FN_i$ are the numbers of true positives, false positives, and false negatives, respectively, for the $i$-th class. <strong>Advantages</strong>: It takes into account the true distribution of each class, so it is usually a more reliable metric for imbalanced datasets. <strong>Disadvantages</strong>: It may overlook the imbalance between classes, as the contributions of all classes are averaged.</p><h3 id="_3-weighted-averaging" tabindex="-1"><a class="header-anchor" href="#_3-weighted-averaging"><span>3. Weighted Averaging</span></a></h3><p>Weighted Averaging is a variant of Macro Averaging, which takes into account the support of each class (i.e., the number of instances in each class). The value of the metric for each class is multiplied by the support of that class, and then the average of these weighted values is calculated. The calculation formula is as follows: $$ \text{Weighted Average} = \frac{\sum_{i=1}^{N} (\text{metric}_i \times \text{support}<em i="1">i)}{\sum</em>^{N} \text{support}_i} $$ Where, $\text{metric}_i$ is the value of the metric for the $i$-th class, and $\text{support}_i$ is the support of the $i$-th class. <strong>Advantages</strong>: It takes into account the relative size of each class, so it is a fairer metric for imbalanced datasets. <strong>Disadvantages</strong>: Like Macro Averaging, it may give disproportionate weight to minority classes, which may distort the overall performance evaluation. The choice of which averaging method to use depends on the specific application scenario and the characteristics of the dataset. In the case of class imbalance, Weighted Averaging is often considered the best choice, as it simultaneously considers the performance and relative importance of each class.</p><div class="hint-container info"><p class="hint-container-title">Info</p><ul><li>If we want to prioritize small classes and ensure each class is given equal importance, macro averaging is the better choice.</li><li>If we care more about overall system performance, where larger classes influence the results more (common in real-world applications like fraud detection), micro averaging is preferable.</li></ul></div><h3 id="regression-performance-metrics" tabindex="-1"><a class="header-anchor" href="#regression-performance-metrics"><span><strong>Regression</strong> Performance Metrics</span></a></h3><h4 id="mse-rmse-and-mae-mean-squared-error-root-mean-squared-error-and-mean-absolute-error" tabindex="-1"><a class="header-anchor" href="#mse-rmse-and-mae-mean-squared-error-root-mean-squared-error-and-mean-absolute-error"><span>MSE, RMSE, and MAE (Mean Squared Error, Root Mean Squared Error, and Mean Absolute Error)</span></a></h4><ul><li>Sum of Squared Errors (SSE) is the sum of the squared differences between the predicted and actual values.</li><li>MSE emphasizes larger errors due to squaring and is sensitive to outliers.</li><li>RMSE is the square root of MSE, providing a more interpretable metric in the same units as the target variable.</li><li>MAE treats all errors equally, is less sensitive to outliers, and provides a straightforward average error measure.</li></ul><figure><img src="/assets/img/generalization.81b87722.png" alt="Generalization Problem" tabindex="0" loading="lazy"><figcaption>Generalization Problem</figcaption></figure><ol><li>Evidence of overfitting: large gap between training and test performance</li><li>Evidence of underfitting: High error rate for both test and training set</li></ol><ul><li>训练误差高，测试误差高 → 欠拟合</li><li>训练误差低，测试误差高 → 过拟合</li><li>训练误差高，测试误差低 → 非常罕见，通常意味着数据泄露或其他问题</li></ul><h3 id="learning-curves" tabindex="-1"><a class="header-anchor" href="#learning-curves"><span>Learning Curves</span></a></h3><ul><li>A learning curve is a plot of learning performance over experience or time</li><li>y-axis: Performance measured by accuracy, error rate or other metrics</li><li>x-axis: conditions, e.g., sizes of training sets, model complexity, number of iterations…</li><li>The learning curve can be used to identify overfitting, underfitting, or a suitable trade-off between the two.</li><li>Training learning curve: calculated from the training set that shows how well the model is learning.</li><li>Validation learning curve: calculated from a holdout set that shows how well the model is generalising.</li></ul><h4 id="data-trade-off" tabindex="-1"><a class="header-anchor" href="#data-trade-off"><span>data trade-off</span></a></h4><ul><li>More training instances? → (usually) better model</li><li>More evaluation instances? → more reliable estimate of effectivenes</li></ul><h3 id="bias-and-variance" tabindex="-1"><a class="header-anchor" href="#bias-and-variance"><span>Bias and Variance</span></a></h3><ul><li><p>Model bias: the tendency of our model to make systematically wrong predictions</p></li><li><p>Evaluation bias: the tendency of our evaluation strategy to over- or under-estimate the effectiveness of our model</p></li><li><p>Sampling bias: if our training or evaluation dataset isn’t representative of the population.</p></li><li><p>Model variance: Sensitivity of a machine learning model&#39;s predictions to small changes in the training data, leading to different outcomes when the model is trained on different subsets of the data.</p></li><li><p>Evaluation variance: Variability in the performance metrics of a model (such as accuracy, precision, or recall) when evaluated across different test datasets or under different evaluation conditions.</p></li></ul><h2 id="week-7-feature-selection" tabindex="-1"><a class="header-anchor" href="#week-7-feature-selection"><span>Week 7: Feature Selection</span></a></h2><h3 id="univariate-methods" tabindex="-1"><a class="header-anchor" href="#univariate-methods"><span>Univariate Methods</span></a></h3><ul><li><strong>Concept</strong>: Evaluate each feature individually in relation to the target variable.</li><li><strong>Advantages</strong>: <ul><li>Intuitive approach to assess the &quot;goodness&quot; of each attribute.</li><li>Model-independent.</li><li>Linear time complexity with respect to the number of attributes.</li></ul></li><li><strong>Good Features</strong>: Features that are &quot;correlated&quot; with the label and can predict it effectively.</li><li><strong>Methods</strong>: <ul><li><strong>Signal-to-Noise Ratio (SNR)</strong>: Select features with high SNR.</li><li><strong>Mutual Information (MI)</strong>: Measures the dependence between the target variable and each feature. Select features with high MI.</li><li><strong>Chi-Squared Test</strong>: Determines if there is a significant association between categorical variables (for classification tasks).</li><li><strong>ANOVA F-value</strong>: Evaluates the significance of individual features in a dataset (for classification tasks with continuous features).</li><li><strong>Correlation-based Feature Selection (CFS)</strong>: Measures the correlation between each feature and the target variable. Selects features with high correlation.</li></ul></li></ul><h3 id="multivariate-methods" tabindex="-1"><a class="header-anchor" href="#multivariate-methods"><span>Multivariate Methods</span></a></h3><ul><li><strong>Concept</strong>: Consider subsets of features together to capture their combined predictive power.</li><li><strong>Limitations of Univariate Methods</strong>: Sometimes, single features may not provide good classification, but a combination of features can lead to better decision boundaries.</li><li><strong>Complexity</strong>: Choosing the optimal subset of attributes that gives the best performance on the validation data.</li><li><strong>Feature Subset Assessment</strong>: <ul><li>Split data into training, validation, and test sets.</li><li>Train a classifier on each feature subset using the training data.</li><li>Select the feature subset that performs best on the validation data.</li><li>Test the selected subset on the test data.</li></ul></li><li><strong>Algorithms</strong>: <img src="/assets/img/feature_selection.7f8c163f.png" alt="feature selection algorithms" loading="lazy"></li></ul><h3 id="filter-methods" tabindex="-1"><a class="header-anchor" href="#filter-methods"><span>Filter Methods</span></a></h3><ul><li><strong>Concept</strong>: Rank features (or feature subsets) independently of the classifier based on a predefined criterion.</li><li><strong>Criterion</strong>: Measure the &quot;relevance&quot; of each feature (or feature subset) with respect to the target variable.</li><li><strong>Search Strategy</strong>: Typically involves sorting features based on individual rankings or nested subsets.</li><li><strong>Assessment</strong>: No direct assessment, but cross-validation can be used to determine the optimal number of features to select.</li></ul><h3 id="wrapper-methods" tabindex="-1"><a class="header-anchor" href="#wrapper-methods"><span>Wrapper Methods</span></a></h3><ul><li><strong>Concept</strong>: Use a classifier to evaluate the &quot;usefulness&quot; of different feature subsets.</li><li><strong>Criterion</strong>: Measure the predictive power of each feature subset.</li><li><strong>Search Strategy</strong>: Search through the space of all possible feature subsets. Train a new classifier for each candidate subset.</li><li><strong>Assessment</strong>: Use cross-validation to evaluate the performance of each subset.</li></ul><h3 id="embedded-methods" tabindex="-1"><a class="header-anchor" href="#embedded-methods"><span>Embedded Methods</span></a></h3><ul><li><strong>Concept</strong>: Similar to wrapper methods, but utilize the classifier&#39;s internal knowledge to guide the search for the best feature subset, avoiding the need to train a new classifier for every candidate subset.</li><li><strong>Criterion</strong>: Measure the &quot;usefulness&quot; of each feature subset.</li><li><strong>Search Strategy</strong>: Guided search based on the classifier&#39;s knowledge.</li><li><strong>Assessment</strong>: Use cross-validation.</li></ul><h3 id="forward-selection-wrapper-method" tabindex="-1"><a class="header-anchor" href="#forward-selection-wrapper-method"><span>Forward Selection (Wrapper Method)</span></a></h3><ul><li><strong>Running Time</strong>: Increases with the number of attributes.</li><li><strong>Advantages</strong>: Performs well when the optimal subset is small.</li><li><strong>Disadvantages</strong>: May converge to a suboptimal solution and is not feasible for large datasets.</li></ul><h3 id="forward-selection-embedded-method" tabindex="-1"><a class="header-anchor" href="#forward-selection-embedded-method"><span>Forward Selection (Embedded Method)</span></a></h3><ul><li><strong>Running Time</strong>: Faster than the wrapper method.</li><li><strong>Advantages</strong>: Uses the classifier&#39;s knowledge to evaluate candidate features.</li></ul><h3 id="backward-elimination-wrapper-method" tabindex="-1"><a class="header-anchor" href="#backward-elimination-wrapper-method"><span>Backward Elimination (Wrapper Method)</span></a></h3><ul><li><strong>Running Time</strong>: Increases with the number of attributes.</li><li><strong>Advantages</strong>: Removes the most irrelevant attributes at the beginning.</li><li><strong>Disadvantages</strong>: May converge to a suboptimal solution and is not feasible for large datasets.</li></ul><h3 id="backward-elimination-embedded-method" tabindex="-1"><a class="header-anchor" href="#backward-elimination-embedded-method"><span>Backward Elimination (Embedded Method)</span></a></h3><ul><li><strong>Running Time</strong>: Faster than the wrapper method.</li><li><strong>Advantages</strong>: Uses the classifier&#39;s knowledge to evaluate candidate features.</li></ul><h3 id="l0-norm-regularization" tabindex="-1"><a class="header-anchor" href="#l0-norm-regularization"><span>L0-Norm Regularization</span></a></h3><ul><li><strong>Concept</strong>: Directly minimize the number of features used by the algorithm by promoting zero entries in the weight vector.</li><li><strong>Objective Function</strong>: Minimize the number of non-zero entries in the weight vector.</li><li><strong>Challenges</strong>: The objective function is non-continuous and non-convex, making the optimization problem combinatorially hard.</li></ul><h3 id="l1-norm-regularization" tabindex="-1"><a class="header-anchor" href="#l1-norm-regularization"><span>L1-Norm Regularization</span></a></h3><ul><li><strong>Concept</strong>: Use a convex function to promote sparsity in the weight vector, resulting in fewer non-zero entries.</li><li><strong>Objective Function</strong>: Minimize the L1 norm of the weight vector (taxicab norm or Manhattan norm).</li><li><strong>Advantages</strong>: The optimization problem is convex, making it easier to solve.</li><li><strong>Result</strong>: The solution typically has fewer non-zero entries, effectively performing feature selection.</li></ul><p><strong>主成分分析 (PCA</strong>)</p><ul><li><strong>目标</strong>：找到一组正交的基向量，能够最大限度地解释数据方差。</li><li><strong>步骤</strong>： <ol><li><strong>数据中心化</strong>：将数据集中的每个特征减去其均值，消除均值的影响。</li><li><strong>计算协方差矩阵</strong>：计算中心化数据矩阵的协方差矩阵。</li><li><strong>特征值分解/奇异值分解</strong>：计算协方差矩阵的特征值和特征向量，或者对中心化数据矩阵进行 SVD。</li><li><strong>选择主成分</strong>：选择前 m 个特征值最大的特征向量作为主成分。</li><li><strong>数据降维</strong>：将原始数据投影到主成分构成的子空间中，实现降维。</li></ol></li></ul><ul><li><strong>PCA 的优势</strong>：</li></ul><ul><li><strong>简化数据</strong>：减少变量数量，降低模型复杂度。</li><li><strong>揭示潜在结构</strong>：找到数据背后的潜在维度，帮助理解数据本质。</li><li><strong>提高效率</strong>：降低计算成本，提高模型训练和预测速度。</li></ul><ul><li><strong>PCA 的局限性</strong>：</li></ul><ul><li><strong>无法利用类别标签信息</strong>：PCA 无法区分不同类别。</li><li><strong>最大化方差</strong>：PCA 最大化方差，但不一定能够区分不同类别。</li></ul><ul><li><strong>PCA 的应用</strong>：</li></ul><ul><li><strong>图像压缩</strong>：将图像分割成小块，并将每个小块投影到低维子空间中，实现压缩。</li><li><strong>面部表情识别</strong>：将面部图像投影到低维子空间中，提取关键特征，用于表情识别。 <strong>PCA 的评估</strong>：</li><li><strong>避免破坏交叉验证</strong>：不要在整个数据集上进行降维，否则会破坏交叉验证。</li><li><strong>在交叉验证的训练集上进行降维</strong>：在交叉验证的每个训练集上进行降维，并在验证集上进行评估。</li></ul><h2 id="week-8" tabindex="-1"><a class="header-anchor" href="#week-8"><span>Week 8</span></a></h2><ul><li>朴素贝叶斯和逻辑回归都是基于概率模型的分类算法，但朴素贝叶斯假设特征之间相互独立，而逻辑回归则没有这个限制。</li><li>感知器不使用概率模型，而是直接优化分类错误率，更加简单直观。</li></ul><h3 id="ann" tabindex="-1"><a class="header-anchor" href="#ann"><span>ANN</span></a></h3><ul><li>Artificial neural network is a network of processing elements</li><li>Each element converts inputs to output</li><li>The output is a function (called activation function) of a weighted sum of inputs</li><li>Training an ANN means adjusting weights for training data given a pre-defined network topology</li></ul><h3 id="perceptron" tabindex="-1"><a class="header-anchor" href="#perceptron"><span>Perceptron</span></a></h3><ul><li>The Perceptron is a minimal neural network</li><li>neural networks are composed of neurons</li><li>A neuron is defined as y = f(θ0 + θ1 x1 +...+ θd xd)</li></ul><h3 id="perceptron-algorithm" tabindex="-1"><a class="header-anchor" href="#perceptron-algorithm"><span>Perceptron Algorithm</span></a></h3><ul><li>supervised classification algorithm</li><li>感知器通过调整权重来优化性能，并通过比较预测输出与真实输出之间的差异来更新权重。如果预测正确，则不做任何改变；如果预测错误，则根据实际情况增加或减少权重。</li><li>一个epoch就是完成一次完整的训练过程</li><li>if mistakes: 𝑦(𝜃 ⋅ 𝒙) ≤ 0, we need to update</li><li>感知器算法通过迭代地更新权重来最小化误分类的数量。每次迭代中，如果发现某个样本被误分类，就根据学习率和样本的特征更新权重。这个过程重复进行直到达到预定的epoch数。</li><li>can set learning rate 𝜂 as 1 for simplicity</li></ul><h3 id="online-learning-vs-batch-learning" tabindex="-1"><a class="header-anchor" href="#online-learning-vs-batch-learning"><span>Online learning vs. Batch learning</span></a></h3><ul><li>The perceptron algorithm is an online algorithm: update weights after each training example</li><li>In contrast, Naive Bayes and logistic regression (with Gradient Descent) are batch algorithms</li></ul><h3 id="multi-layer-perceptron" tabindex="-1"><a class="header-anchor" href="#multi-layer-perceptron"><span>Multi-layer perceptron</span></a></h3><ul><li>Input layer with input units x: the first layer, takes features x as inputs</li><li>Output layer with output units y: the last layer, has one unit per possible output (e.g., 1 unit for binary classification)</li><li>Hidden layers with hidden units h: all layers in between.</li></ul><h3 id="multi-layer-perceptron-vs-other-neural-networks" tabindex="-1"><a class="header-anchor" href="#multi-layer-perceptron-vs-other-neural-networks"><span>Multi-layer Perceptron vs. other Neural Networks</span></a></h3><h4 id="multi-layer-perceptron-1" tabindex="-1"><a class="header-anchor" href="#multi-layer-perceptron-1"><span>Multi-layer Perceptron</span></a></h4><ul><li>One specific type of neural network</li><li>Feed-forward</li><li>Fully connected</li><li>Supervised learner</li></ul><h4 id="other-types-of-neural-networks" tabindex="-1"><a class="header-anchor" href="#other-types-of-neural-networks"><span>Other types of neural networks</span></a></h4><ul><li>Convolutional neural networks</li><li>Recurrent neural networks</li><li>Autoencoder (unsupervised)</li></ul><h3 id="linear-vs-non-linear-classification" tabindex="-1"><a class="header-anchor" href="#linear-vs-non-linear-classification"><span>Linear vs. Non-linear classification</span></a></h3><h4 id="linear-classification" tabindex="-1"><a class="header-anchor" href="#linear-classification"><span>Linear classification</span></a></h4><ul><li>The perceptron, naive bayes, logistic regression are linear classifiers</li><li>Decision boundary is a linear combination of features σ𝑖 𝜃𝑖𝑋𝑖</li><li>Cannot learn‘feature interactions’ naturally</li></ul><h4 id="non-linear-classification" tabindex="-1"><a class="header-anchor" href="#non-linear-classification"><span>Non-linear classification</span></a></h4><ul><li>Neural networks with at least 1 hidden layer and non-linear activations</li><li>Decision boundary is a non-linear function of the inputs</li></ul><h3 id="feature-engineering-vs-feature-learning" tabindex="-1"><a class="header-anchor" href="#feature-engineering-vs-feature-learning"><span>Feature engineering vs feature learning</span></a></h3><table><thead><tr><th>项目</th><th>Feature Engineering（特征工程）</th><th>Feature Learning（特征学习）</th></tr></thead><tbody><tr><td><strong>使用方式</strong></td><td>人工设计并提取特征作为模型输入</td><td>模型从原始数据中自动学习特征</td></tr><tr><td><strong>典型模型</strong></td><td>感知机、朴素贝叶斯、逻辑回归、决策树等</td><td>神经网络（MLP、CNN、RNN、Transformer等）</td></tr><tr><td><strong>数据输入</strong></td><td>需要是人类理解的、高质量的“特征”（如 sunny, overcast 等）</td><td>原始的数值数据，如图像像素、文本词向量、音频信号</td></tr><tr><td><strong>是否需要领域知识</strong></td><td>✅ 需要，特征提取要依赖专家经验</td><td>❌ 不需要，模型自动学出与任务相关的特征</td></tr><tr><td><strong>控制权/可解释性</strong></td><td>高（你知道模型在用什么特征）</td><td>低（中间特征难解释，但通常更有效）</td></tr><tr><td><strong>对调参的依赖</strong></td><td>相对较少（因为特征是固定的）</td><td>高（要调网络结构、激活函数、学习率等超参数）</td></tr></tbody></table><h3 id="activation-functions" tabindex="-1"><a class="header-anchor" href="#activation-functions"><span>Activation Functions</span></a></h3><figure><img src="/assets/img/activationfunctino.da7dc6c3.png" alt="activation function" tabindex="0" loading="lazy"><figcaption>activation function</figcaption></figure><ol><li><strong>Logistic or Sigmoid Function</strong>: The output range of this function is between 0 and 1, commonly used in the output layer for binary classification problems. But will cause gradient vanishing problem, not commonly used.</li><li><strong>Hyperbolic Tangent Function (tanh)</strong>: The output range of this function is between -1 and 1, more commonly used than the sigmoid function because its output mean is 0, which helps alleviate the gradient vanishing problem.</li><li><strong>Rectified Linear Unit (ReLU)</strong>: Most commonly used activation function This function directly returns the input value when the input is greater than 0, otherwise returns 0. It is one of the most popular activation functions currently, due to its simplicity, efficiency, and ability to accelerate the training process of neural networks.</li></ol><ul><li>Note that the activation functions must be non-linear, as without this, the model is simply a (complex) linear model</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h3 id="感知机学习规则" tabindex="-1"><a class="header-anchor" href="#感知机学习规则"><span>感知机学习规则</span></a></h3><ul><li>二分类问题：sigmoid</li><li>多分类问题：softmax</li><li>多标签问题：sigmoid</li><li>线性回归: 线性函数</li></ul><h3 id="formula" tabindex="-1"><a class="header-anchor" href="#formula"><span>Formula</span></a></h3><h4 id="🧮-公式-1-线性组合-perceptron-中的加权求和" tabindex="-1"><a class="header-anchor" href="#🧮-公式-1-线性组合-perceptron-中的加权求和"><span>🧮 <strong>公式 1：线性组合（Perceptron 中的加权求和）</strong></span></a></h4><p>$$ \Sigma = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 $$</p><h4 id="✅-含义" tabindex="-1"><a class="header-anchor" href="#✅-含义"><span>✅ 含义：</span></a></h4><ul><li>$\Sigma$：感知机的中间结果（还没经过激活函数）</li><li>$\theta_0$：偏置项（bias）</li><li>$\theta_1, \theta_2$：输入特征的权重</li><li>$x_1, x_2$：输入特征值</li></ul><p>这是感知机对一个输入样本做出的“线性打分”。</p><hr><h4 id="🔁-公式-2-感知机权重更新规则-perceptron-learning-rule" tabindex="-1"><a class="header-anchor" href="#🔁-公式-2-感知机权重更新规则-perceptron-learning-rule"><span>🔁 <strong>公式 2：感知机权重更新规则（Perceptron Learning Rule）</strong></span></a></h4><p>$$ \theta_j^{(t)} \leftarrow \theta_j^{(t-1)} + \eta \cdot (y^{(i)} - \hat{y}^{(i,t)}) \cdot x_j^{(i)} $$</p><h4 id="✅-含义-1" tabindex="-1"><a class="header-anchor" href="#✅-含义-1"><span>✅ 含义：</span></a></h4><ul><li>$\theta_j^{(t)}$：第 $t$ 次更新后第 $j$ 个权重</li><li>$\theta_j^{(t-1)}$：上一次的权重值</li><li>$\eta$：学习率（learning rate）</li><li>$y^{(i)}$：第 $i$ 个训练样本的真实标签</li><li>$\hat{y}^{(i,t)}$：当前模型对样本 $i$ 的预测（经过激活函数）</li><li>$x_j^{(i)}$：样本 $i$ 的第 $j$ 个特征</li></ul><h2 id="只在预测错误-即-y-ne-hat-y-时才更新。" tabindex="-1"><a class="header-anchor" href="#只在预测错误-即-y-ne-hat-y-时才更新。"><span>只在预测错误（即 $y \ne \hat{y}$）时才更新。</span></a></h2><p>这两个公式一起构成了感知机的核心工作机制：</p><blockquote><p><strong>先用第一个公式计算预测，再根据第二个公式更新权重。</strong></p></blockquote></div><h3 id="when-is-linear-classification-enough" tabindex="-1"><a class="header-anchor" href="#when-is-linear-classification-enough"><span>When is Linear Classification Enough?</span></a></h3><ul><li>If we know our classes are linearly (approximately) separable</li><li>If the feature space is (very) high-dimensional ...i.e., the number of features exceeds the number of training instances (kernel trick)</li><li>If the traning set is small</li><li>If interpretability is important, i.e., understanding how (combinations of) features explain different predictions</li><li>…but with increasing availability of data, and more powerful computers, non-linear models are gaining popularity, with currently Neural Networks being their most popular variant.</li></ul><h3 id="pros-and-cons-of-neural-networks" tabindex="-1"><a class="header-anchor" href="#pros-and-cons-of-neural-networks"><span>Pros and Cons of Neural Networks</span></a></h3><h4 id="pros" tabindex="-1"><a class="header-anchor" href="#pros"><span>Pros</span></a></h4><ul><li>Powerful tool!</li><li>Neural networks with at least 1 hidden layer can approximate any (continuous) function. They are universal approximators</li><li>Automatic feature learning</li><li>Empirically, very good performance for many diverse tasks</li></ul><h4 id="cons" tabindex="-1"><a class="header-anchor" href="#cons"><span>Cons</span></a></h4><ul><li>Powerful model increases the danger of ‘overfitting’</li><li>Requires large training data sets</li><li>Often requires powerful compute resources (GPUs)</li><li>Lack of interpretability</li></ul><h2 id="week-9" tabindex="-1"><a class="header-anchor" href="#week-9"><span>Week 9</span></a></h2><table><thead><tr><th>Characteristics</th><th>Activation Function</th><th>Step Function</th></tr></thead><tbody><tr><td>Definition</td><td>Introduces non-linear factors, determines whether a node is activated</td><td>Takes different constant values in different intervals</td></tr><tr><td>Mathematical Properties</td><td>Usually continuous and differentiable</td><td>Discontinuous, with jumps only at specific points</td></tr><tr><td>Applications</td><td>Introduces non-linearity in neural networks, increasing model complexity</td><td>Signal processing, control theory, early perceptron models</td></tr><tr><td>Common Types</td><td>Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax</td><td>Heaviside step function</td></tr><tr><td>Advantages</td><td>Improves model expressiveness, suitable for complex pattern recognition</td><td>Simple, easy to understand and implement</td></tr><tr><td>Disadvantages</td><td>May cause gradient disappearance or explosion, requires selection of suitable functions</td><td>Discontinuous, not suitable for gradient-based optimization algorithms</td></tr><tr><td>Use in Neural Networks</td><td>Widely used, a core component of modern neural networks</td><td>Rarely used, mainly in early models</td></tr><tr><td>Summary: Activation functions and step functions have different applications and properties in mathematics and engineering. Activation functions are mainly used in neural networks to introduce non-linearity and improve model expressiveness, while step functions are used in signal processing, control theory, and other fields. In neural networks, step functions are not suitable as activation functions due to their discontinuity.</td><td></td><td></td></tr></tbody></table><h3 id="why-cannot-we-use-the-step-functon" tabindex="-1"><a class="header-anchor" href="#why-cannot-we-use-the-step-functon"><span>Why cannot we use the step functon?</span></a></h3><p>Derivative is not defined at 0. When it’s not at 0, the derivative is 0. = No way to update weights and learn anything!</p><h3 id="a-problem-encountered-when-training-a-multilayer-perceptron-mlp-and-the-method-to-solve-it" tabindex="-1"><a class="header-anchor" href="#a-problem-encountered-when-training-a-multilayer-perceptron-mlp-and-the-method-to-solve-it"><span>A problem encountered when training a multilayer perceptron (MLP) and the method to solve it.</span></a></h3><ol><li>The update rule depends on the actual target output y: When training a neural network, we typically use an update rule to adjust the network&#39;s weights, which depends on the actual target output. However, in a multilayer network, we can only directly obtain the actual output of the final layer, but not the actual output of the hidden layers.</li><li>We can only access the actual output of the final layer: In a multilayer network, we can only directly observe the output of the final layer, while the output of the hidden layers is not directly observable.</li><li>We do not know the actual activation of the hidden layers: Since we cannot directly observe the actual output of the hidden layers, we do not know the actual activation state of the hidden layers. To solve these problems, backpropagation (Backpropagation) provides an effective method. Backpropagation is an algorithm for training multilayer neural networks, which updates the weights in the network by calculating the partial derivatives of the error. This method allows us to effectively calculate the error partial derivatives for each individual weight, thereby updating the weights of the hidden layers, not just the outermost layer. This way, we can still effectively train the entire network without knowing the actual activation of the hidden layers.</li></ol><h3 id="comparison-of-perceptron-and-backpropagation" tabindex="-1"><a class="header-anchor" href="#comparison-of-perceptron-and-backpropagation"><span>Comparison of Perceptron and Backpropagation</span></a></h3><table><thead><tr><th>感知机更新</th><th>Backprop</th></tr></thead><tbody><tr><td>只有一层（输出层）</td><td>多层（含隐藏层）</td></tr><tr><td>权重更新基于误差 $y - \hat{y}$</td><td>基于损失函数对每个权重的导数</td></tr><tr><td>没有链式法则</td><td>使用链式法则（链式求导）</td></tr><tr><td>不需要激活函数的导数</td><td>必须使用激活函数的导数（如 sigmoid&#39;）</td></tr><tr><td>简单快速，但只能处理线性可分</td><td>更复杂，但可以拟合非线性函数</td></tr></tbody></table><h3 id="backpropagation" tabindex="-1"><a class="header-anchor" href="#backpropagation"><span>Backpropagation</span></a></h3><p><strong>训练多层感知机（MLP）</strong> 的标准流程，用于理解 <strong>反向传播（Backpropagation）+ 梯度下降（Gradient Descent）</strong> 的全过程。</p><h4 id="step-1-forward-propagate-an-input-x" tabindex="-1"><a class="header-anchor" href="#step-1-forward-propagate-an-input-x"><span><strong>Step 1: Forward propagate an input x</strong></span></a></h4><p>输入 $\mathbf{x}$（训练样本）进入神经网络：</p><ul><li><p>一层一层地执行线性变换和激活函数</p></li><li><p>对每一层 $l$：</p><p>$$ \mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)} \ \mathbf{a}^{(l)} = f(\mathbf{z}^{(l)}) $$</p><p>其中 $f$ 是激活函数（如 ReLU、Sigmoid）</p></li></ul><p>✅ 最终得到输出 $\hat{y}$</p><h4 id="step-2-compute-the-predicted-output-hat-y" tabindex="-1"><a class="header-anchor" href="#step-2-compute-the-predicted-output-hat-y"><span><strong>Step 2: Compute the predicted output $\hat{y}$</strong></span></a></h4><p>这是前向传播的最后结果：</p><p>$$ \hat{y} = \text{MLP}(\mathbf{x}) $$</p><p>可能是：</p><ul><li>标量（用于二分类）</li><li>向量（用于多分类，比如经过 softmax）</li></ul><h4 id="step-3-compare-hat-y-with-true-output-y-compute-error" tabindex="-1"><a class="header-anchor" href="#step-3-compare-hat-y-with-true-output-y-compute-error"><span><strong>Step 3: Compare $\hat{y}$ with true output $y$, compute error</strong></span></a></h4><p>定义一个 <strong>损失函数（Loss Function）</strong>：</p><p>常用的损失函数：</p><ul><li>均方误差（MSE）：$\frac{1}{2} (y - \hat{y})^2$</li><li>交叉熵（Cross-Entropy）用于分类</li></ul><p>记为：</p><p>$$ \mathcal{L}(\hat{y}, y) $$</p><p>我们现在知道当前预测有多“错”。</p><h4 id="step-4-modify-each-weight-to-reduce-the-error-gradient-descent" tabindex="-1"><a class="header-anchor" href="#step-4-modify-each-weight-to-reduce-the-error-gradient-descent"><span><strong>Step 4: Modify each weight to reduce the error (Gradient Descent)</strong></span></a></h4><p>这一步就是 <strong>反向传播的核心</strong>：</p><ol><li>通过 <strong>链式法则</strong> 计算损失函数对每一层权重的梯度：</li></ol><p>$$ \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} $$</p><ol start="2"><li>使用 <strong>梯度下降</strong> 更新每一层权重：</li></ol><p>$$ \mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} $$</p><p>$$ \mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}} $$</p><p>其中 $\eta$ 是学习率。</p><p>🔁 每一层的梯度依赖于后一层的梯度：这就是<strong>反向传播</strong>（误差从输出层传回隐藏层）</p><h4 id="step-5-repeat-for-all-data-and-multiple-epochs" tabindex="-1"><a class="header-anchor" href="#step-5-repeat-for-all-data-and-multiple-epochs"><span><strong>Step 5: Repeat for all data and multiple epochs</strong></span></a></h4><ul><li>对训练集中的每个样本重复上面过程（可能使用 mini-batch）</li><li>进行多个轮（epochs），直到损失不再明显下降</li></ul><h4 id="✨-总结流程图" tabindex="-1"><a class="header-anchor" href="#✨-总结流程图"><span>✨ 总结流程图</span></a></h4><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Input x</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>[Forward Propagation]</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>Predicted Output ŷ</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>Compare with True Output y</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>[Compute Loss]</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>[Backpropagation: compute gradients]</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>Update weights with Gradient Descent</span></span>
<span class="line"><span>  ↓</span></span>
<span class="line"><span>Repeat</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h3 id="generalized-delta-rule" tabindex="-1"><a class="header-anchor" href="#generalized-delta-rule"><span>Generalized Delta Rule</span></a></h3><p>The <strong>Generalized Delta Rule</strong> 是 <strong>反向传播算法（Backpropagation）</strong> 的数学核心，用于训练多层感知机（MLP）。它是一种将感知机的简单“误差更新规则”推广到多层神经网络中的方法。</p><hr><h4 id="🧠-一句话解释" tabindex="-1"><a class="header-anchor" href="#🧠-一句话解释"><span>🧠 一句话解释：</span></a></h4><blockquote><p><strong>Generalized Delta Rule</strong> 就是用<strong>链式法则</strong>计算隐藏层中每个神经元的误差信号，并据此更新权重。</p></blockquote><h4 id="🧮-回顾感知机的基本更新" tabindex="-1"><a class="header-anchor" href="#🧮-回顾感知机的基本更新"><span>🧮 回顾感知机的基本更新：</span></a></h4><p>在单层感知机中，如果输出是：</p><p>$$ \hat{y} = f(\theta^T x) $$</p><p>误差是：</p><p>$$ \delta = y - \hat{y} $$</p><p>然后我们更新权重：</p><p>$$ \theta \leftarrow \theta + \eta \cdot \delta \cdot x $$</p><p>但对于多层神经网络（有隐藏层），我们不能直接用 $y - \hat{y}$ 来更新隐藏层的权重。</p><h4 id="🚀-generalized-delta-rule-的核心思想" tabindex="-1"><a class="header-anchor" href="#🚀-generalized-delta-rule-的核心思想"><span>🚀 Generalized Delta Rule 的核心思想：</span></a></h4><p>对于任意神经元 $j$ 的权重 $w_{ji}$，更新公式为：</p><p>$$ w_{ji} \leftarrow w_{ji} + \eta \cdot \delta_j \cdot x_i $$</p><p>其中：</p><ul><li>$\eta$：学习率</li><li>$\delta_j$：第 $j$ 个神经元的“误差信号”</li><li>$x_i$：输入（来自上一层）</li></ul><h4 id="❗关键点在于怎么定义-delta-j-误差信号" tabindex="-1"><a class="header-anchor" href="#❗关键点在于怎么定义-delta-j-误差信号"><span>❗关键点在于怎么定义 $\delta_j$（误差信号）</span></a></h4><h4 id="👉-对于输出层的神经元" tabindex="-1"><a class="header-anchor" href="#👉-对于输出层的神经元"><span>👉 对于输出层的神经元：</span></a></h4><p>$$ \delta_j = (y_j - \hat{y}_j) \cdot f&#39;(z_j) $$</p><ul><li>$f&#39;(z_j)$：激活函数的导数</li><li>$z_j$：未激活前的线性输入</li></ul><h4 id="👉-对于隐藏层的神经元" tabindex="-1"><a class="header-anchor" href="#👉-对于隐藏层的神经元"><span>👉 对于隐藏层的神经元：</span></a></h4><p>$$ \delta_j = f&#39;(z_j) \cdot \sum_k \delta_k \cdot w_{kj} $$</p><ul><li>$\delta_k$：来自下一层（靠近输出层）的误差信号</li><li>$w_{kj}$：从本层神经元 $j$ 到下一层神经元 $k$ 的权重</li></ul><h4 id="🧠-总结" tabindex="-1"><a class="header-anchor" href="#🧠-总结"><span>🧠 总结</span></a></h4><table><thead><tr><th>项目</th><th>感知机</th><th>Generalized Delta Rule (Backprop)</th></tr></thead><tbody><tr><td>适用网络</td><td>单层</td><td>多层（含隐藏层）</td></tr><tr><td>误差传播</td><td>直接用 $y - \hat{y}$</td><td>使用链式法则逐层传播</td></tr><tr><td>激活函数导数</td><td>不需要</td><td>必须计算 $f&#39;(z)$</td></tr><tr><td>支持非线性</td><td>❌</td><td>✅</td></tr><tr><td>支持隐藏层训练</td><td>❌</td><td>✅</td></tr></tbody></table><h3 id="discriminative-and-generative" tabindex="-1"><a class="header-anchor" href="#discriminative-and-generative"><span>Discriminative and Generative</span></a></h3><p>&quot;<strong>Discriminative</strong>&quot; and &quot;<strong>Generative</strong>&quot; are two different model approaches in machine learning used for classification or prediction tasks, but their <strong>modeling ideas and goals differ</strong>. Here are their main differences:</p><hr><h3 id="🔍-one-sentence-difference" tabindex="-1"><a class="header-anchor" href="#🔍-one-sentence-difference"><span>🔍 One-sentence Difference:</span></a></h3><ul><li><strong>Discriminative Models</strong>: Directly learn the <strong>mapping from input to output</strong> (i.e., learn $P(y \mid x)$).</li><li><strong>Generative Models</strong>: Learn the <strong>data generation process</strong> (i.e., learn $P(x, y)$, which can derive $P(x \mid y)$ and $P(y)$).</li></ul><hr><h3 id="📚-more-specifically" tabindex="-1"><a class="header-anchor" href="#📚-more-specifically"><span>📚 More Specifically:</span></a></h3><table><thead><tr><th>Attribute</th><th>Discriminative Models</th><th>Generative Models</th></tr></thead><tbody><tr><td>What is learned?</td><td>Learn $P(y \mid x)$: the probability of class y given input x</td><td>Learn $P(x \mid y)$ and $P(y)$: how input x is generated by class y</td></tr><tr><td>Examples</td><td>Logistic Regression, SVM, Random Forest, BERT</td><td>Naive Bayes, HMM, GMM, GAN, VAE</td></tr><tr><td>Capabilities</td><td>Classification, regression</td><td>Classification, sample generation (images, text), missing value imputation, data simulation</td></tr><tr><td>Model input distribution?</td><td>❌ Does not model the distribution of $x$</td><td>✅ Models the generation mechanism of $x$</td></tr><tr><td>Can generate data?</td><td>❌ No</td><td>✅ Can generate new samples (images, text, etc.)</td></tr><tr><td>Typical performance</td><td>More accurate for classification (because it directly optimizes the classification task)</td><td>More robust with less data, but may have slightly weaker classification performance</td></tr></tbody></table><hr><h3 id="🎓-classic-examples" tabindex="-1"><a class="header-anchor" href="#🎓-classic-examples"><span>🎓 Classic Examples:</span></a></h3><h4 id="naive-bayes-generative" tabindex="-1"><a class="header-anchor" href="#naive-bayes-generative"><span>Naive Bayes (Generative)</span></a></h4><ul><li>Assume you know the word frequency for each class (e.g., &quot;spam&quot; or &quot;normal email&quot;).</li><li>You can first generate a spam email and then see if a new email resembles spam.</li></ul><h4 id="logistic-regression-discriminative" tabindex="-1"><a class="header-anchor" href="#logistic-regression-discriminative"><span>Logistic Regression (Discriminative)</span></a></h4><ul><li>Only learns to directly determine from the features of an email whether it is spam or normal.</li></ul><hr><h3 id="✅-when-to-use-which" tabindex="-1"><a class="header-anchor" href="#✅-when-to-use-which"><span>✅ When to Use Which?</span></a></h3><ul><li><p>Use <strong>Discriminative</strong>:</p><ul><li>Classification is the sole task (e.g., sentiment classification, image recognition)</li><li>Sufficient data is available, aiming for the best prediction performance</li></ul></li><li><p>Use <strong>Generative</strong>:</p><ul><li>Want to <strong>generate data</strong> (images, speech, text, etc.)</li><li>There is <strong>missing data</strong>, <strong>few-shot learning</strong>, or a need for stronger expressive capability</li></ul></li></ul><hr><h2 id="week-10" tabindex="-1"><a class="header-anchor" href="#week-10"><span>Week 10</span></a></h2><h3 id="unsupervised-learning" tabindex="-1"><a class="header-anchor" href="#unsupervised-learning"><span>Unsupervised Learning</span></a></h3><ul><li>Finding the correct label for an instance can be hard and expensive (the labelling usually should be done manually)</li><li>Labelled data is limited</li><li>On the other hand, we have access to many more datasets that are not labelled</li></ul><h3 id="types-of-unsupervised-learning" tabindex="-1"><a class="header-anchor" href="#types-of-unsupervised-learning"><span>Types of unsupervised learning</span></a></h3><ul><li>PCA: simplify complex datasets with many features by extracting the most essential information</li><li>Clustering: Finding groups of items that are similar</li><li>Anomaly detection: detecting rare occurrences that seem suspicious because they&#39;re different from the established pattern</li><li>Associate Rule Mining: Given a set of transactions, find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction</li></ul><h3 id="clustering" tabindex="-1"><a class="header-anchor" href="#clustering"><span>Clustering</span></a></h3><table><thead><tr><th>Dimension</th><th>Type</th><th>Description</th><th>Example Algorithm</th></tr></thead><tbody><tr><td>Membership</td><td>Exclusive</td><td>Each point belongs to only one cluster</td><td>K-Means</td></tr><tr><td></td><td>Overlapping</td><td>A point can belong to multiple clusters</td><td>FCM, GMM</td></tr><tr><td>Assignment Method</td><td>Deterministic</td><td>Clear membership</td><td>K-Means</td></tr><tr><td></td><td>Probabilistic</td><td>Probabilistic membership</td><td>GMM, FCM</td></tr><tr><td>Structure</td><td>Hierarchical</td><td>Tree structure</td><td>Hierarchical Clustering</td></tr><tr><td></td><td>Partitioning</td><td>Flat partitioning</td><td>K-Means</td></tr><tr><td>Coverage</td><td>Partial</td><td>Not all points need to be covered</td><td>Clustering with outlier handling</td></tr><tr><td></td><td>Complete</td><td>All points are classified</td><td>Most traditional clustering</td></tr><tr><td>Intra-cluster Characteristics</td><td>Homogeneous</td><td>Clusters are as similar as possible</td><td>Most clustering algorithms</td></tr><tr><td></td><td>Heterogeneous</td><td>Allows significant differences within clusters</td><td>Used for specific needs</td></tr></tbody></table><hr><h3 id="k-means" tabindex="-1"><a class="header-anchor" href="#k-means"><span>K-means</span></a></h3><h4 id="k-means-clustering-process-exclusive-complete-and-partitioning-clustering-method" tabindex="-1"><a class="header-anchor" href="#k-means-clustering-process-exclusive-complete-and-partitioning-clustering-method"><span>K-means clustering process (exclusive, complete and partitioning clustering method)</span></a></h4><ul><li>Ask the user how many clusters they would like (K)</li><li>Randomly pick K points (as seeds or initial cluster centroids)</li><li>Repeat <ul><li>Assign each instance to the cluster with the nearest centroid</li><li>Recompute the centroid of each cluster</li></ul></li><li>Until the centroids don’t change (or until changes are smaller than a threshold, or until relatively few points change clusters)</li></ul><h4 id="k-means-limitaion" tabindex="-1"><a class="header-anchor" href="#k-means-limitaion"><span>K-means limitaion</span></a></h4><ul><li>“mean” is ill-defined for categorical attributes</li><li>Sensitive to outliers</li><li>Sensitive to Initial Centroids</li><li>Not able to handle non-spherical clusters</li><li>Not able to handle clusters of differing sizes</li><li>Not able to handle clusters with different densities</li><li>Requires Predefined Number of Clusters (K): Determining the optimal number of clusters can be a challenging task</li></ul><h4 id="k-means-strengths" tabindex="-1"><a class="header-anchor" href="#k-means-strengths"><span>K-means strengths</span></a></h4><ul><li>Simplicity: straightforward and easy-to-understand algorithm.</li><li>Scalability: computationally efficient and can converge relatively quickly. Suitable for clustering tasks involving a large number of data points.</li><li>Interpretable Results: Clusters can be easily interpreted and visualised.</li><li>Applicability: K-means is applicable to a wide range of clustering tasks (often good enough).</li></ul><h4 id="elbow-method-to-find-optimal-k" tabindex="-1"><a class="header-anchor" href="#elbow-method-to-find-optimal-k"><span>Elbow Method (to find optimal K)</span></a></h4><ul><li>A “good” cluster should have High cluster cohesion (Intra-cluster distances are minimized) and High Cluster Separation (Inter-cluster distances are maximized)</li><li>To measure Cohesion: Within-cluster Sum of Squares (WCSS): the sum of the squared distance between each point and the centroid in a cluster (Cluster Cohesion)</li><li>Look for a point where the reduction in WCSS significantly slows down: that is the Elbow point</li></ul><h3 id="hierarchical-clustering" tabindex="-1"><a class="header-anchor" href="#hierarchical-clustering"><span>Hierarchical Clustering</span></a></h3><ul><li>Agglomerative (Bottom-up) clustering <ul><li>Start with the points (instances) as individual clusters</li><li>At each step, merge the closest pair of clusters until only one cluster (or k clusters) left</li></ul></li></ul><figure><img src="/assets/img/agglomerative.4d7d8dea.png" alt="Agglomerative (Bottom-up) clustering" tabindex="0" loading="lazy"><figcaption>Agglomerative (Bottom-up) clustering</figcaption></figure><ul><li>Single Linkage: Distance between the closest members of the cluster <ul><li>Can handle non-similar shapes</li><li>Sensitive to noise and outliers</li></ul></li><li>Complete Linkage: Distance between the furthest members of the cluster <ul><li>Less sensitive to noise and outliers</li><li>Tends to create spherical clusters with a consistent diameter.</li><li>Tends to break large clusters</li></ul></li><li>Centroid Linkage: Distance between the two centroids</li></ul><h4 id="strength" tabindex="-1"><a class="header-anchor" href="#strength"><span>Strength</span></a></h4><ul><li>No assumption of any particular number of clusters <ul><li>Any desired number of clusters can be obtained by &#39;cutting&#39; the dendrogram at the appropriate level</li></ul></li><li>No assumption of cluster shape <ul><li>not assume specific shapes or distributions for the clusters.</li><li>can handle clusters of various shapes, sizes, and densities</li></ul></li><li>They may correspond to meaningful taxonomies <ul><li>Examples in biological sciences (e.g., animal kingdom, phylogeny reconstruction, …)</li></ul></li></ul><h4 id="weakness" tabindex="-1"><a class="header-anchor" href="#weakness"><span>Weakness</span></a></h4><ul><li>Once a decision is made to combine two clusters, it cannot be undone, i.e., an object that is in the wrong cluster will always stay there.</li><li>No objective function is directly minimised, i.e., no &#39;relative&#39; quality measure</li></ul><h3 id="clustering-metrics" tabindex="-1"><a class="header-anchor" href="#clustering-metrics"><span>Clustering Metrics</span></a></h3><h4 id="wcss-and-bcss" tabindex="-1"><a class="header-anchor" href="#wcss-and-bcss"><span>WCSS and BCSS</span></a></h4><p><img src="/assets/img/wcss.7a201b89.png" alt="WCSS" loading="lazy"><img src="/assets/img/bcss.0f78f6e6.png" alt="BCSS" loading="lazy"></p><ul><li><p>The two main concepts used to evaluate the goodness of a clustering structure without relying on external information (i.e., unsupervised evaluation) are:</p><ul><li><p><strong>Cohesion (Intra-cluster Similarity)</strong> – Measures how closely related the points within the same cluster are. A good clustering structure should have high cohesion, meaning data points within a cluster should be densely packed and similar to one another. This is typically evaluated using metrics like within-cluster sum of squares (WCSS).</p></li><li><p><strong>Separation (Inter-cluster Dissimilarity)</strong> – Measures how well-separated different clusters are from each other. A strong clustering structure should have high separation, meaning clusters are distinct and far apart from one another. This is often evaluated using metrics like between-cluster sum of squares (BCSS).</p></li></ul></li><li><p>These two principles ensure that clusters are both internally coherent and externally distinct, leading to a well-formed clustering solution.</p></li></ul><h4 id="calinski-harabasz-index" tabindex="-1"><a class="header-anchor" href="#calinski-harabasz-index"><span>Calinski-Harabasz Index</span></a></h4><figure><img src="/assets/img/calinski-harabasz.a23927c9.png" alt="Calinski-Harabasz Index" tabindex="0" loading="lazy"><figcaption>Calinski-Harabasz Index</figcaption></figure><h3 id="homogeneity" tabindex="-1"><a class="header-anchor" href="#homogeneity"><span>Homogeneity</span></a></h3><ul><li>Whether each cluster contains only data points that are all from the same class. Homogeneity checks if all the items in a cluster are the same type.</li></ul><h3 id="completeness" tabindex="-1"><a class="header-anchor" href="#completeness"><span>Completeness</span></a></h3><ul><li>Whether all data points of a particular class are assigned to the same cluster.</li></ul><h4 id="why-do-we-need-both-metrics" tabindex="-1"><a class="header-anchor" href="#why-do-we-need-both-metrics"><span>Why Do We Need Both Metrics?</span></a></h4><ul><li><p>Homogeneity alone can be misleading. It only checks whether all instances within a cluster share the same label but does not ensure that instances of the same class are grouped together.</p></li><li><p>Thus, a good clustering solution should aim for both high homogeneity and high completeness to ensure meaningful and accurate grouping of data points.</p></li></ul><h4 id="数学公式-如何计算" tabindex="-1"><a class="header-anchor" href="#数学公式-如何计算"><span>数学公式 &amp; 如何计算</span></a></h4><p>这两个指标都基于 <strong>条件熵（conditional entropy）</strong> 的概念，并且和 <strong>V-measure</strong> 有关联。</p><p>我们用以下变量表示：</p><ul><li>$ C $：真实的类别（class）</li><li>$ K $：聚类的结果（cluster）</li></ul><h5 id="_1-homogeneity-h" tabindex="-1"><a class="header-anchor" href="#_1-homogeneity-h"><span>1. Homogeneity (h)</span></a></h5><p>$$ h = \begin{cases} 1 - \frac{H(C|K)}{H(C)} &amp; \text{if } H(C) \neq 0 \ 1 &amp; \text{otherwise} \end{cases} $$</p><p>其中：</p><ul><li>$ H(C|K) $：给定聚类结果的类别条件熵（表示每个簇内部的类别分布混乱程度）</li><li>$ H(C) $：类别的总熵（原始类别分布的不确定性）</li></ul><p>💡 含义：如果每个簇里的类别都很单一（$ H(C|K) \approx 0 $），则 <code>h ≈ 1</code>，说明同质性很好。</p><hr><h5 id="_2-completeness-c" tabindex="-1"><a class="header-anchor" href="#_2-completeness-c"><span>2. Completeness (c)</span></a></h5><p>$$ c = \begin{cases} 1 - \frac{H(K|C)}{H(K)} &amp; \text{if } H(K) \neq 0 \ 1 &amp; \text{otherwise} \end{cases} $$</p><p>其中：</p><ul><li>$ H(K|C) $：给定真实类别的聚类条件熵（表示每个类别在聚类结果中被打散的程度）</li><li>$ H(K) $：聚类结果的总熵（聚类分布的不确定性）</li></ul><p>💡 含义：如果每个类别都被集中在某一个簇中（$ H(K|C) \approx 0 $），则 <code>c ≈ 1</code>，说明完整性很好。</p><hr><h4 id="举个例子说明怎么算" tabindex="-1"><a class="header-anchor" href="#举个例子说明怎么算"><span>举个例子说明怎么算</span></a></h4><h3 id="示例输入" tabindex="-1"><a class="header-anchor" href="#示例输入"><span>示例输入：</span></a></h3><table><thead><tr><th>数据点</th><th>真实类别（Class）</th><th>聚类结果（Cluster）</th></tr></thead><tbody><tr><td>A</td><td>Cat</td><td>1</td></tr><tr><td>B</td><td>Cat</td><td>1</td></tr><tr><td>C</td><td>Dog</td><td>1</td></tr><tr><td>D</td><td>Dog</td><td>2</td></tr><tr><td>E</td><td>Bird</td><td>2</td></tr></tbody></table><p>我们可以看到：</p><ul><li>Cluster 1 包含 Cat, Cat, Dog → 不同类 → <strong>同质性差</strong></li><li>Cat 分布在 Cluster 1 → 完整性 OK</li><li>Dog 分布在 Cluster 1 和 Cluster 2 → <strong>完整性差</strong></li></ul><hr><h4 id="手动计算-简化理解" tabindex="-1"><a class="header-anchor" href="#手动计算-简化理解"><span>手动计算（简化理解）：</span></a></h4><ul><li><p><strong>同质性</strong>（Homogeneity）：</p><ul><li>Cluster 1: 两个猫 + 一个狗 → 不纯</li><li>Cluster 2: 一个狗 + 一个鸟 → 更不纯</li><li>所以 <code>h &lt; 1</code></li></ul></li><li><p><strong>完整性</strong>（Completeness）：</p><ul><li>猫都在 cluster 1 → OK</li><li>狗分布在两个 cluster → 不完整</li><li>鸟在 cluster 2 → OK</li><li>所以 <code>c &lt; 1</code></li></ul></li></ul><hr><h3 id="issues-of-unsupervised-learning" tabindex="-1"><a class="header-anchor" href="#issues-of-unsupervised-learning"><span>Issues of unsupervised learning</span></a></h3><ul><li>Ambiguity in Interpretation <ul><li>Clusters or embeddings may not align with human-understandable or task-relevant categories.</li><li>Clusters or latent features are often hard to interpret or evaluate objectively</li></ul></li><li>Wrong Direction <ul><li>The model may find structure, but there&#39;s no guarantee it&#39;s meaningful or useful for our task</li></ul></li></ul><h3 id="semi-supervised-learning" tabindex="-1"><a class="header-anchor" href="#semi-supervised-learning"><span>Semi-supervised learning</span></a></h3><ul><li>learning from both labelled and unlabelled data, often more unlabelled</li><li>Two common strategies within semi-supervised learning are self-training and active learning</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><table><thead><tr><th>类型</th><th>是否需要标签</th><th>应用场景</th></tr></thead><tbody><tr><td>Supervised</td><td>✅ 是</td><td>分类、回归</td></tr><tr><td>Semi-supervised</td><td>✅ 部分有标签</td><td>资源有限但数据多</td></tr><tr><td>Unsupervised</td><td>❌ 否</td><td>聚类、异常检测、降维等</td></tr></tbody></table></div><h3 id="self-tranining" tabindex="-1"><a class="header-anchor" href="#self-tranining"><span>Self-tranining</span></a></h3><ul><li>Repeat</li><li>Train a model f on L using any supervised learning method</li><li>Apply f to predict the labels on each instance in U</li><li>Identify a subset of U with “high confidence” labels</li><li>Remove them from U and add them to L with the classifier predictions as the “ground-truth”</li><li>Until L does not change</li></ul><h4 id="problems-and-solution" tabindex="-1"><a class="header-anchor" href="#problems-and-solution"><span>Problems and Solution</span></a></h4><ul><li>错误传播 (Classification errors can propagate)：这是自训练的一个主要风险。如果模型在预测未标记数据的标签时犯了错误（比如给了一个高置信度的错误标签），并且这个错误的数据被加入了 L，那么在下一轮训练中，这个错误的数据就会被当作“正确”的标签来训练模型。这会导致模型在新数据上表现更差，错误会像滚雪球一样越滚越大。</li><li>高置信度阈值：一个常用的策略是，模型在预测未标记数据标签时，只有当它对预测结果非常“自信”（置信度高于某个预设的阈值）时，才把这个预测的标签和对应的数据加入到 L 中。如果模型对预测结果不太确定（置信度低），就认为这个预测不可靠，不把它加入 L，从而避免引入错误标签。</li></ul><h3 id="active-learning" tabindex="-1"><a class="header-anchor" href="#active-learning"><span>Active Learning</span></a></h3><ul><li>pose queries (unlabelled instances) for labelling by an oracle (e.g. a human annotator)</li></ul><h4 id="uncertain-sampling" tabindex="-1"><a class="header-anchor" href="#uncertain-sampling"><span>Uncertain Sampling</span></a></h4><ol><li>Least Confidence: Choose samples with the smallest predicted probability for the most likely class</li><li>Margin Sampling: Choose samples where the difference between the top two predicted class probabilities is smallest</li><li>Entropy sampling: Select the sample with the highest prediction entropy</li></ol><h4 id="query-strategies" tabindex="-1"><a class="header-anchor" href="#query-strategies"><span>Query Strategies</span></a></h4><ul><li>Query by Committee(QBC) If the committee disagrees on an instance, the model is uncertain and would benefit most from seeing the true label.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><table><thead><tr><th>策略</th><th>关键点</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td><strong>Uncertainty Sampling</strong></td><td>利用单个模型的不确定性</td><td>简单高效，易实现</td><td>可能偏向异常点</td></tr><tr><td><strong>QBC</strong></td><td>利用多个模型的不一致</td><td>更稳健，捕捉“争议点”</td><td>算法复杂度高，需要训练多个模型</td></tr></tbody></table></div><h3 id="data-augmentation" tabindex="-1"><a class="header-anchor" href="#data-augmentation"><span>Data Augmentation</span></a></h3><ul><li>Bootstrap sampling: create “new” datasets by resampling existing data, with or without replacement</li><li>Cross validation / repeated random subsampling are based on the same idea</li><li>Each “batch” has a slightly different distribution of instances, forces model to use different features and not get stuck in local minima</li><li>Advantages <ul><li>More data nearly always improves learning</li><li>Most learning algorithms have some robustness to noise</li></ul></li><li>Disadvantages <ul><li>Biased training data</li><li>May introduce features that don’t exist in the real world</li><li>May propagate errors</li><li>Increases problems with interpretability and transparency</li></ul></li></ul><h3 id="unsupervised-pre-training" tabindex="-1"><a class="header-anchor" href="#unsupervised-pre-training"><span>Unsupervised pre-training</span></a></h3><h4 id="bert" tabindex="-1"><a class="header-anchor" href="#bert"><span>BERT</span></a></h4><ul><li>pretrained language model developed by Google that understands text by looking at both left and right context simultaneously</li><li><strong>Masked Language Modelling (MLM)</strong>: Predict missing words in a sentence. &quot;The cat [MASK] on the mat.&quot;</li><li><strong>Next Sentence Prediction (NSP)</strong>: Predict if one sentence follows another.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><p>-<strong>预训练（Pre-training）</strong>： 模型先在大规模的无标签数据上训练，比如让语言模型预测句子中的下一个词，或者让图像模型识别图像中的特征。这个阶段模型学习的是通用的“知识”或“表示”，但没有特定任务的标签指导。</p><ul><li><strong>迁移（Transfer）</strong>： 把预训练得到的模型参数（权重）拿过来，作为一个“起点”或者“初始模型”。</li><li><strong>微调（Fine-tuning）</strong>： 然后用你手头具体的有标签数据（比如分类任务的标签）对这个模型进行针对性的训练，让模型学会具体任务的要求。这个过程会稍微调整预训练时学到的参数，使模型更适合你的特定任务。</li></ul></div><h2 id="week-11" tabindex="-1"><a class="header-anchor" href="#week-11"><span>Week 11</span></a></h2><h3 id="parametric-models-and-non-parametric-models" tabindex="-1"><a class="header-anchor" href="#parametric-models-and-non-parametric-models"><span>Parametric Models and Non-parametric Models:</span></a></h3><table><thead><tr><th>Feature</th><th>Parametric Models</th><th>Non-parametric Models</th></tr></thead><tbody><tr><td>Assumptions</td><td>Strong assumptions about data distribution (e.g., normal distribution, linear relationship)</td><td>Almost no assumptions about data distribution</td></tr><tr><td>Number of Parameters</td><td>Fixed number of parameters, usually few</td><td>Number of parameters is not fixed, can be many</td></tr><tr><td>Model Complexity</td><td>Relatively low model complexity</td><td>Model complexity can be high</td></tr><tr><td>Training Data Needs</td><td>Usually requires less training data</td><td>May require a large amount of training data</td></tr><tr><td>Generalization Ability</td><td>Depends on the accuracy of assumptions</td><td>Generally strong generalization ability as it does not rely on specific assumptions</td></tr><tr><td>Overfitting Risk</td><td>Risk of overfitting, especially if assumptions do not match the true distribution</td><td>Also has overfitting risk, but can usually be controlled through model selection and regularization</td></tr><tr><td>Computational Efficiency</td><td>Usually high computational efficiency</td><td>Computational efficiency can be low, especially for large datasets</td></tr><tr><td>Flexibility</td><td>Low flexibility, constrained by assumptions</td><td>High flexibility, able to adapt to complex data structures and relationships</td></tr><tr><td>Examples</td><td>Linear regression, logistic regression, Naive Bayes</td><td>K-nearest neighbors, decision trees, random forests, support vector machines</td></tr><tr><td>Application Scenarios</td><td>Suitable for situations where assumptions are reasonable and data volume is small</td><td>Suitable for situations with large data volumes, complex relationships, or unknown assumptions</td></tr><tr><td>Model Interpretability</td><td>Usually has good interpretability</td><td>Interpretability may be poor, especially for complex models</td></tr></tbody></table><h3 id="ensembles" tabindex="-1"><a class="header-anchor" href="#ensembles"><span>Ensembles</span></a></h3><ul><li>Ensemble learning (aka. Classifier combination): constructs a set of base classifiers from a given set of training data and aggregates the outputs into a single meta-classifier</li><li>Intuition 1: the combination of lots of weak classifiers can be at least as good as one strong classifier</li><li>Intuition 2: the combination of a selection of strong classifiers is (usually) at least as good as the best of the base classifiers</li><li>Ensembles are effective when individual classifiers are slightly better than random (error &lt; 0.5).</li></ul><h4 id="when-does-ensemble-learning-work" tabindex="-1"><a class="header-anchor" href="#when-does-ensemble-learning-work"><span>When does ensemble learning work?</span></a></h4><ul><li>The classifiers should not make the same mistakes (not the same bias)</li><li>The base classifiers are reasonably accurate (better than chance), slightly better than random (error &lt; 0.5).</li></ul><h3 id="different-classifiers-different-models-or-the-same-model-with-feature-manipulation-stacking" tabindex="-1"><a class="header-anchor" href="#different-classifiers-different-models-or-the-same-model-with-feature-manipulation-stacking"><span>Different classifiers (different models or the same model with feature manipulation): Stacking</span></a></h3><h3 id="_1-stacking" tabindex="-1"><a class="header-anchor" href="#_1-stacking"><span>1. Stacking</span></a></h3><p><strong>Description:</strong></p><ul><li><strong>Different classifiers (different models or the same model with feature manipulation):</strong> Stacking, also known as stacked generalization, involves training on multiple different models (these models can be different algorithms or the same algorithm with different features or hyperparameters). The predictions of these models are then used as input features to train a higher-level model (meta-learner, usually a simple linear model like logistic regression). <strong>How it works:</strong></li></ul><ol><li><strong>Level 0 models:</strong> Train multiple base models on the complete dataset.</li><li><strong>Level 1 model:</strong> Use the predictions of the base models as input features to train a higher-level model (meta-learner). <strong>Key Points:</strong></li></ol><ul><li><strong>Diversity:</strong> The effectiveness of stacking relies on the diversity of the base models.</li><li><strong>Complexity:</strong> It can be more complex compared to other ensemble methods, and implementing and tuning parameters can be more difficult.</li><li><strong>Application Scenarios:</strong> Stacking is useful when you can use multiple different models or when you want to leverage the strengths of different algorithms. <strong>In short:</strong> Stacking uses multiple different models to make predictions and uses these predictions as features for another model to obtain the final prediction.</li></ul><h3 id="same-classifier-instance-manipulation-bagging-primarily-targets-variance-reduction" tabindex="-1"><a class="header-anchor" href="#same-classifier-instance-manipulation-bagging-primarily-targets-variance-reduction"><span>Same classifier, instance manipulation: Bagging (primarily targets variance reduction)</span></a></h3><h3 id="_2-bagging" tabindex="-1"><a class="header-anchor" href="#_2-bagging"><span>2. Bagging</span></a></h3><p><strong>Description:</strong></p><ul><li><strong>Same classifier, instance manipulation:</strong> Bagging, or Bootstrap Aggregating, primarily reduces variance by manipulating data instances. It is often used to reduce the variance of models, especially those complex models sensitive to small changes in the training data (e.g., decision trees). <strong>How it works:</strong></li></ul><ol><li><strong>Bootstrap Sampling:</strong> Perform sampling with replacement from the original dataset to create multiple different training sets (called bootstrap samples).</li><li><strong>Parallel Training:</strong> Independently train the same model on each bootstrap sample.</li><li><strong>Aggregate Predictions:</strong> Average the predictions (regression) or use majority voting (classification) for all models. <strong>Key Points:</strong></li></ol><ul><li><strong>Variance Reduction:</strong> Bagging primarily improves model stability by reducing variance.</li><li><strong>Parallelism:</strong> Models in bagging are trained in parallel.</li><li><strong>Application Scenarios:</strong> Suitable for models that are very sensitive to small changes in the training data, such as decision trees. <strong>In short:</strong> Bagging improves model stability by repeatedly sampling the original data, training the same model on each sample, and then aggregating the predictions of these models.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><p>Random Forest is considered a &quot;Bagging&quot; (Bootstrap Aggregating) method because it uses bootstrap sampling techniques when constructing each decision tree. Bagging is an ensemble learning method whose core idea is to create multiple subsets from the original dataset through bootstrap sampling, train a base learner (e.g., decision tree) on each subset, and finally improve the overall model&#39;s performance by combining the predictions of these base learners. In Random Forest, the specific application of Bagging is reflected in the following aspects:</p><ol><li><strong>Bootstrap Sampling:</strong> For the original dataset, Random Forest performs multiple bootstrap samplings, each generating a subset of the same size as the original dataset but with possibly repeated elements. These subsets are used to train different decision trees.</li><li><strong>Independent Training:</strong> Each decision tree is independently trained on its respective bootstrap sample, meaning they do not share the same training data.</li><li><strong>Combine Predictions:</strong> In the prediction phase, Random Forest combines the predictions of all decision trees. For classification problems, majority voting is usually used; for regression problems, the average prediction value is used. Through this method, Random Forest uses Bagging techniques to reduce model variance, improve generalization ability, and enhance robustness to noise and outliers. Each tree is trained on a slightly different data subset, which helps introduce diversity, making the overall model more stable and accurate. Therefore, Random Forest is classified as a Bagging method.</li></ol><ul><li>Random Forest adopts both feature manipulation and instance manipulation approaches.</li></ul></div><h3 id="same-classifier-algorithm-manipulation-boosting-primarily-targets-bias-reduction" tabindex="-1"><a class="header-anchor" href="#same-classifier-algorithm-manipulation-boosting-primarily-targets-bias-reduction"><span>Same classifier, algorithm manipulation: Boosting (primarily targets bias reduction)</span></a></h3><h3 id="_3-boosting" tabindex="-1"><a class="header-anchor" href="#_3-boosting"><span>3. Boosting</span></a></h3><p><strong>Description:</strong></p><ul><li><strong>Same classifier, algorithm manipulation:</strong> Boosting primarily reduces bias by gradually adjusting the model during training. It weights training samples so that the model focuses more on previously misclassified samples. <strong>How it works:</strong></li></ul><ol><li><strong>Initialization:</strong> Initialize the weights of all samples equally.</li><li><strong>Iterative Training:</strong> Gradually train the model, adjusting the weights of the samples based on the error rate of the previous model so that the weights of misclassified samples increase.</li><li><strong>Combine Models:</strong> Combine all models, usually weighting them based on each model&#39;s accuracy. <strong>Key Points:</strong></li></ol><ul><li><strong>Bias Reduction:</strong> Boosting primarily improves model accuracy by reducing bias.</li><li><strong>Sequentiality:</strong> Models in boosting are trained sequentially, with each model depending on the output of the previous model.</li><li><strong>Application Scenarios:</strong> Suitable for tasks where performance can be improved by focusing on difficult samples. <strong>In short:</strong> Boosting improves model accuracy by gradually adjusting the training process so that the model focuses more on samples misclassified by previous models. <strong>Summary:</strong></li><li><strong>Stacking</strong> uses multiple different models and inputs their outputs as features into another model.</li><li><strong>Bagging</strong> uses the same model but trains on different data subsets to reduce variance.</li><li><strong>Boosting</strong> uses the same model but gradually adjusts the model during training to reduce bias.</li></ul><table><thead><tr><th>Feature</th><th>Stacking</th><th>Bagging</th><th>Boosting</th></tr></thead><tbody><tr><td><strong>Model Type</strong></td><td>Different classifiers (different models or the same model with different feature processing)</td><td>Same classifier</td><td>Same classifier</td></tr><tr><td><strong>Objective</strong></td><td>Combine predictions of multiple models to improve overall performance</td><td>Primarily reduce variance</td><td>Primarily reduce bias</td></tr><tr><td><strong>Instance Processing</strong></td><td>Use the original dataset</td><td>Perform bootstrap sampling (sampling with replacement) on training data</td><td>Adjust sample weights based on errors from the previous round of models</td></tr><tr><td><strong>Model Processing</strong></td><td>Use different models or the same model with different feature processing</td><td>Use the same model</td><td>Use the same model</td></tr><tr><td><strong>Algorithm Processing</strong></td><td>Use a meta-learner to combine predictions of multiple models</td><td>No specific algorithm processing</td><td>Adjust model weights through boosting algorithms (e.g., AdaBoost, Gradient Boosting)</td></tr><tr><td><strong>Advantages</strong></td><td>Combine advantages of different models to improve generalization ability</td><td>Reduce overfitting, improve model stability</td><td>Improve prediction accuracy, reduce bias</td></tr><tr><td><strong>Disadvantages</strong></td><td>High computational complexity, requires training multiple models and a meta-learner</td><td>May increase model variance, sensitive to outliers</td><td>May lead to overfitting, sensitive to noise</td></tr><tr><td><strong>Typical Applications</strong></td><td>Combine different types of models (e.g., decision trees, neural networks, SVM)</td><td>Random Forest</td><td>AdaBoost, Gradient Boosting Machines (GBM)</td></tr><tr><td><strong>Applicable Scenarios</strong></td><td>Scenarios requiring the combination of multiple models to improve performance</td><td>Scenarios requiring variance reduction and improved model stability</td><td>Scenarios requiring bias reduction and improved prediction accuracy</td></tr></tbody></table><h3 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h3><ul><li><strong>Stacking</strong> improves overall performance by combining predictions of different models, suitable for scenarios requiring the combination of multiple models to improve performance.</li><li><strong>Bagging</strong> reduces model variance and improves stability through bootstrap sampling, suitable for scenarios requiring variance reduction and improved model stability.</li><li><strong>Boosting</strong> reduces model bias and improves prediction accuracy by adjusting sample weights, suitable for scenarios requiring bias reduction and improved prediction accuracy.</li></ul><h3 id="anomaly-detection" tabindex="-1"><a class="header-anchor" href="#anomaly-detection"><span>Anomaly Detection</span></a></h3><ul><li>Anomalies are different from noise <ul><li>Noise is random error or variance in a measured variable</li><li>Noise should be removed before anomaly detection</li></ul></li></ul><h3 id="types-of-anomaly" tabindex="-1"><a class="header-anchor" href="#types-of-anomaly"><span>Types of Anomaly</span></a></h3><h4 id="global-anomaly" tabindex="-1"><a class="header-anchor" href="#global-anomaly"><span>Global Anomaly</span></a></h4><ul><li>significantly deviates from the rest of the data</li></ul><h4 id="contextual-anomaly" tabindex="-1"><a class="header-anchor" href="#contextual-anomaly"><span>Contextual Anomaly</span></a></h4><ul><li>deviates significantly based on a selected context <ul><li>Contextual attributes: define the context, e.g., time &amp; location</li><li>Behavioural attributes: characteristics of the object, used in anomaly evaluation, e.g., temperature</li></ul></li></ul><h4 id="collective-anomaly" tabindex="-1"><a class="header-anchor" href="#collective-anomaly"><span>Collective Anomaly</span></a></h4><ul><li>deviates significantly from the majority of the data</li></ul><h3 id="anomaly-detection-algorithms" tabindex="-1"><a class="header-anchor" href="#anomaly-detection-algorithms"><span>Anomaly Detection Algorithms</span></a></h3><h4 id="supervised-anomaly-detection" tabindex="-1"><a class="header-anchor" href="#supervised-anomaly-detection"><span>Supervised Anomaly Detection</span></a></h4><ul><li>Use labelled data to train a model to detect anomalies</li><li>Challenges <ul><li>Labeling data is expensive and time-consuming</li><li>Anomalies are often rare and difficult to detect</li><li>Anomalies may change over time</li><li>Cannot detect unknown and emerging anomalies</li><li>Anomalies may be specific to a particular context</li><li>Anomalies may be specific to a particular user or group of users</li></ul></li></ul><h4 id="semi-supervised-anomaly-detection" tabindex="-1"><a class="header-anchor" href="#semi-supervised-anomaly-detection"><span>Semi-Supervised Anomaly Detection</span></a></h4><ul><li>Labels are available only for normal data</li><li>apply the trained model to both unlabelled and labelled data</li><li>Any instance that falls outside the learned decision boundary is marked as a potential anomaly</li><li>Challenges <ul><li>Require labels from the normal class</li><li>Possible high false alarm rate - previously unseen (yet legitimate) data records may be recognised as anomalies</li></ul></li></ul><h4 id="unsupervised-anomaly-detection" tabindex="-1"><a class="header-anchor" href="#unsupervised-anomaly-detection"><span>Unsupervised Anomaly Detection</span></a></h4><ul><li>Assume the normal objects are somewhat &quot;clustered&quot; into multiple groups, each having some distinct features</li><li>An outlier is expected to be far away from any groups of normal objects</li><li>Challenges <ul><li>Normal objects may not share any strong patterns, but the collective outliers may share high similarity in a small area</li><li>Ex. In intrusion detection, normal activities are diverse</li><li>Unsupervised methods may have a high false positive rate but still miss many real outliers</li></ul></li></ul><h4 id="unsupervised-anomaly-detection-approaches" tabindex="-1"><a class="header-anchor" href="#unsupervised-anomaly-detection-approaches"><span>Unsupervised Anomaly Detection Approaches</span></a></h4><ul><li>Statistical (model-based) <ul><li>Assume that normal data follow some statistical model, such as Gaussian distribution</li></ul></li><li>Proximity-based <ul><li>An object is an outlier if the nearest neighbours of the object are far away</li></ul></li><li>Density-based <ul><li>Outliers are objects in regions of low density</li></ul></li><li>Clustering-based <ul><li>Normal data belong to large and dense clusters</li></ul></li></ul><h4 id="statistical-model-based" tabindex="-1"><a class="header-anchor" href="#statistical-model-based"><span>Statistical (model-based)</span></a></h4><ul><li>Pros <ul><li>Statistical tests are well-understood and well-validated.</li><li>Quantitative measure of the degree to which an object is an outlier.</li></ul></li><li>Cons <ul><li>Data may be hard to model parametrically.</li><li>Multiple modes</li><li>Variable density</li><li>In high dimensions, data may be insufficient to estimate the true distribution.</li></ul></li></ul><h4 id="proximity-based" tabindex="-1"><a class="header-anchor" href="#proximity-based"><span>Proximity-based</span></a></h4><ul><li>Pros <ul><li>Easier to define a proximity measure for a dataset than to determine its statistical distribution.</li><li>Quantitative measure of the degree to which an object is an outlier.</li><li>Deals naturally with multiple modes.</li></ul></li><li>Cons <ul><li>O(n^2) complexity.</li><li>Score sensitive to choice of k.</li><li>Does not work well if the data has widely variable density.</li></ul></li></ul><h4 id="density-based" tabindex="-1"><a class="header-anchor" href="#density-based"><span>Density-based</span></a></h4><ul><li>Pros <ul><li>Quantitative measure of the degree to which an object is an outlier.</li><li>It can work well even if the data has variable density.</li></ul></li><li>Cons <ul><li>O(n^2) complexity</li><li>Must choose parameters</li><li>k for nearest neighbours</li><li>d for distance threshold</li></ul></li></ul><h4 id="clustering-based" tabindex="-1"><a class="header-anchor" href="#clustering-based"><span>Clustering-based</span></a></h4><ul><li>Pros <ul><li>Some clustering techniques have O(n) complexity.</li><li>Extends the concept of outlier from single objects to groups of objects.</li></ul></li><li>Cons <ul><li>Requires thresholds for large and small clusters.</li><li>Sensitive to the number of clusters chosen.</li><li>Outliers may affect the initial formation of clusters.</li></ul></li></ul><h2 id="week-12" tabindex="-1"><a class="header-anchor" href="#week-12"><span>Week 12</span></a></h2><h3 id="bias" tabindex="-1"><a class="header-anchor" href="#bias"><span>Bias</span></a></h3><h4 id="out-group-homogeneity-bias-stereotypes-prejudice" tabindex="-1"><a class="header-anchor" href="#out-group-homogeneity-bias-stereotypes-prejudice"><span>Out-group homogeneity bias (Stereotypes/Prejudice)</span></a></h4><ul><li>Humans tend to perceive out-group members as less nuanced than in-group members.</li></ul><h4 id="correlation-fallacy" tabindex="-1"><a class="header-anchor" href="#correlation-fallacy"><span>Correlation Fallacy</span></a></h4><ul><li>Humans have a tendency to mistake correlation (two co-incidentally co-occurring events) with causation.</li></ul><h4 id="historical-bias" tabindex="-1"><a class="header-anchor" href="#historical-bias"><span>Historical Bias</span></a></h4><ul><li>A randomly sampled data set, reflects the world as it was including existing biases which should not be carried forward</li></ul><h3 id="measurement-bias" tabindex="-1"><a class="header-anchor" href="#measurement-bias"><span>Measurement Bias</span></a></h3><ul><li>Noisy measurement → errors or missing data points which are not randomly distributed</li><li>Mistaking a (noisy) proxy for a label of interest</li><li>Oversimplification of the quantity of interest</li><li>should 1. Know your domain 2. Know your task 3. Know your data</li></ul><h3 id="model-bias" tabindex="-1"><a class="header-anchor" href="#model-bias"><span>Model Bias</span></a></h3><ul><li>Weak models: high bias – low variance</li><li>Unjustified model assumptions</li><li>Blind loss functions: Blind to certain types of errors</li><li>should 1. Carefully consider model assumptions 2. Carefully choose loss functions 3. Model groups separately 4. Represent groups fairly in the data</li></ul><h3 id="evaluation-deployment-bias" tabindex="-1"><a class="header-anchor" href="#evaluation-deployment-bias"><span>Evaluation / Deployment Bias</span></a></h3><ul><li>Test set not representative</li><li>Overfit to a test set</li><li>Evaluation metrics may not capture all quantities of interest (disregard minority groups or average effects)</li><li>Use of systems in ways they were not intended to use. Lack of education of end-users.</li></ul></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/Crc011220/Crc011220.github.io/edit/main/src/posts/unimelb/COMP90049.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">6/14/2025, 9:30:50 AM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: ruocchen1220@gmail.com">Ruochen Chen</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/posts/unimelb/COMP90048.html" aria-label="Declarative Programming (COMP90048)"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Declarative Programming (COMP90048)</div></a><a class="route-link auto-link next" href="/posts/unimelb/ml.html" aria-label="Machine Learning"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Machine Learning<span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">Learning today, leading tomorrow</div><div class="vp-copyright">Copyright © 2025 Richard Chen </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/assets/js/runtime~app.ba280723.js" defer></script><script src="/assets/js/6312.2d95f1ad.js" defer></script><script src="/assets/js/app.bf24beb5.js" defer></script>
  </body>
</html>
