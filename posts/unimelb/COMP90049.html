<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.59" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://crc011220.github.io/personalweb/personalweb/posts/unimelb/COMP90049.html"><meta property="og:site_name" content="Richard Chen"><meta property="og:title" content="Introduction to Machine Learning (COMP90049)"><meta property="og:description" content="Introduction to Machine Learning (COMP90049) Week 1 Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitl..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-04-15T06:12:32.000Z"><meta property="article:tag" content="Unimelb"><meta property="article:published_time" content="2025-03-02T00:00:00.000Z"><meta property="article:modified_time" content="2025-04-15T06:12:32.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Introduction to Machine Learning (COMP90049)","image":[""],"datePublished":"2025-03-02T00:00:00.000Z","dateModified":"2025-04-15T06:12:32.000Z","author":[{"@type":"Person","name":"Richard Chen"}]}</script><title>Introduction to Machine Learning (COMP90049) | Richard Chen</title><meta name="description" content="Introduction to Machine Learning (COMP90049) Week 1 Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitl...">
    <link rel="stylesheet" href="/personalweb/assets/css/styles.b9549e6d.css">
    <link rel="preload" href="/personalweb/assets/js/runtime~app.ab402ce4.js" as="script"><link rel="preload" href="/personalweb/assets/css/styles.b9549e6d.css" as="style"><link rel="preload" href="/personalweb/assets/js/6312.9fc55c3f.js" as="script"><link rel="preload" href="/personalweb/assets/js/app.6a4995c2.js" as="script">
    <link rel="prefetch" href="/personalweb/assets/js/zh_posts_java8_ÂáΩÊï∞ÂºèÁºñÁ®ã.html.ab797f87.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_netty_Netty01-nio.html.44323f7e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_netty_Netty04-‰ºòÂåñ‰∏éÊ∫êÁ†Å.html.518f3749.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_netty_Netty03-ËøõÈò∂.html.03a0708a.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_netty_Netty02-intro.html.0e1ebfb5.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_nginx_1.html.15a19623.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_hive_Hive-SQLËØ≠Ê≥ïÂ§ßÂÖ®.html.5e94435f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_14.html.4252e885.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/2664.0c0e265f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_typescript_1.html.da058844.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_typescript_2.html.ce39bb23.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_aws-saa_1.html.87f9cd32.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_1.html.999bf012.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_COMP90048.html.483c2ce3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_COMP90049.html.85d4d3ee.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/8300.ef6ef338.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_mybatis_2.html.db6c97e5.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_note_Coin-exchange-project-note.html.5807d144.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_7.html.cd6ef5d7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_4.html.549414ef.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_5.html.3d312580.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_8.html.ac898e31.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_2.html.234c961c.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_aws-saa_3.html.cbbd1154.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nginx_1.html.4928e339.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_SWEN90016.html.7411380f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_spark_Spark-Core.html.27c1eba7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_11.html.d085f3aa.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_14.html.e9db3197.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_8.html.0098487f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_4.html.2c8e61d3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_spark_Spark-Sql.html.55477302.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_21.html.7b2277c2.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_11.html.319beb61.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_COMP90024.html.0b438284.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_5.html.65e20ad1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_6.html.62bff5e1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_10.html.90907f55.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_COMP90050.html.08ae99bc.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_1.html.b2cf9b81.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_6.html.1b9a9b31.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_4.html.3983752e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_15.html.77272a8b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_aws-saa_2.html.5081291e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_6.html.c4d2a863.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_3.html.2aaed287.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_12.html.9e1f8125.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_14.html.1586eaee.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_13.html.c7c46b2e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_5.html.93c604b1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_6.html.2eb480ac.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_2.html.9137fbba.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_typescript_3.html.5c4fb3a9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_2.html.239717c3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_16.html.26d87d7b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_2.html.9b410a57.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_7.html.c5232f91.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_3.html.5ca6a44c.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_12.html.46325a21.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_17.html.a3b45e69.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_13.html.89d24776.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_9.html.6ebc7454.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_15.html.4f49d558.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_18.html.7a7166a7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_1.html.2e8fec73.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_mybatis_1.html.3e2ec56f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_1.html.a7a51b30.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_7.html.aae3e77f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_istio_1.html.0d471aef.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_4.html.1ffa627f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_15.html.281c4a6d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_10.html.0846c47d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_1.html.99f82efe.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_spark_Spark-Intro.html.68753f67.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_5.html.5a8ea599.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_19.html.4d83b8d9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/intro.html.4ce0690f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_7.html.08c318bd.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_3.html.d6aecf3a.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_7.html.1eb568f8.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_8.html.776feadd.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_3.html.a754e28f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_intro.html.d6f106bd.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_index.html.f7a6e2d0.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_hadoop_Hive.html.008a142f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_7.html.83ea4009.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_9.html.f43ac521.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_hadoop_HDFS.html.0df5841b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_2.html.2a526ac0.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_8.html.038d6cb4.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_4.html.1b547249.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_5.html.b547e762.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_3.html.250e0d8a.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_docker_1.html.a5342962.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_11.html.525c03be.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_microsvc_1.html.69c492be.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_index.html.a4842a92.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_index.html.9d1ea122.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_6.html.449ad459.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_6.html.a838fcf3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_3.html.dde6999b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_6.html.37417e87.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_7.html.d3cf26ff.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_8.html.7ecbc2b2.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_4.html.59b8cfbe.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_algorithm_20.html.44c71e9d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_1.html.5840e517.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_5.html.d4ffbbd2.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_2.html.ba238891.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_10.html.77df97e6.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_practices_3.html.9b1771ef.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_elasticsearch_1.html.93040ee3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_2.html.11621aa2.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_index.html.e0c4c366.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_dubbo_1.html.9dfb12f9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_concepts_4.html.320a766a.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_9.html.c979f0b4.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_index.html.23661c45.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nextjs_index.html.593d6788.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_interview_9.html.3fcb54a3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_index.html.6f08e874.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_1.html.7f99bb43.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_8.html.5622c9b9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_elasticsearch_2.html.8d4d93b2.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_genesis.html.8bcc0416.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_5.html.4c7ff04e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_unimelb_index.html.9237e634.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_hadoop_MapReduce.html.d7738323.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_genesis.html.7fcdc79f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_12.html.2d29c7a6.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_hadoop_index.html.196d47cd.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_dubbo_2.html.91825d57.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_aws-saa_index.html.fe6b4aaf.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_clich√©_13.html.ac2e4363.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_typescript_index.html.6650f841.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_index.html.877714a1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nginx_index.html.246e2644.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_dubbo_index.html.d9a190a9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_hadoop_Yarn.html.96b3e2c4.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_elasticsearch_index.html.17b54266.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/index.html.ba1fa638.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_mybatis_index.html.2b7d1f92.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_spark_index.html.043a225e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_kubernetes_microsvc_index.html.ade8e024.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_ai_springai.html.c0551115.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_cmb_9.html.a9b505d0.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_nginx_2.html.00cf01b4.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_docker_index.html.2057d6b7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_istio_index.html.f9935ef5.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_index.html.5ba1f36d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_note_index.html.86f6364d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_ai_index.html.82e599fb.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/category_learning-records_index.html.a7bd8630.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_nginx_index.html.91b68446.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/category_index.html.e7825f9d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/timeline_index.html.4644635f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/article_index.html.16af2e0e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_category_Â≠¶‰π†Á¨îËÆ∞_index.html.5b09dfef.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_index.html.b0975feb.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/star_index.html.ad5ccb09.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/category_internship-journal_index.html.e8288b0d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_category_learning-records_index.html.8f23fb4c.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/posts_index.html.e478d6ae.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_programmer-clich√©_index.html.16aa55e7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_algorithm-practices_index.html.6f378f04.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_china-merchant-bank_index.html.1061cf33.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_technical-interview_index.html.d673bec4.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_category_ÂºÄÂßã_index.html.913d20d7.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_ÊóÖÁ®ã_index.html.78f0d4db.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_ÁÆÄÂéÜ_index.html.b3db317b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/category_genesis_index.html.6afda0a5.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_javascript_index.html.4ffbacd6.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_kubernetes_index.html.2d830752.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_typescript_index.html.afb66ca1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_leetcode_index.html.483e3594.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_java8_index.html.c99c5e7d.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_netty_index.html.5df1d3bb.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_nginx_index.html.5f2ee87f.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_aws-saa_index.html.7846f8fb.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_mybatis_index.html.eb58b177.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_unimelb_index.html.2c6cfa57.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_hive_index.html.57fd6d40.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_docker_index.html.6818152a.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_hadoop_index.html.71c82c31.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_nextjs_index.html.cf0aa922.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_resume_index.html.d366a821.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_nio_index.html.0cf101a5.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_dubbo_index.html.f8d2bfc3.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_index_index.html.abbf3369.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_istio_index.html.0dd7b26b.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_notes_index.html.15e32766.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_react_index.html.25a541ee.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_spark_index.html.0ce88249.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/404.html.88ba5fc9.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_timeline_index.html.8aaedb0e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_ai_index.html.ce5b4a35.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/tag_es_index.html.a5c174bb.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_category_index.html.1a3f8985.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_article_index.html.fe0ecb87.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_tag_index.html.f0efcacf.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_star_index.html.360371e1.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_java8_index.html.64583806.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_netty_index.html.b8530cfe.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_nginx_index.html.1ec2227e.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_hive_index.html.aece8841.js" as="script"><link rel="prefetch" href="/personalweb/assets/js/zh_posts_index.html.57af5edf.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/personalweb/" aria-label="Take me home"><img class="vp-nav-logo" src="https://img.taotu.cn/ssd/ssd4/1/2024-07-08/1_e36ceb8c09656a291a6f0a33178736f9.webp" alt><!----><span class="vp-site-name hide-in-pad">Richard Chen</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/personalweb/" aria-label="Blog Home"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Posts"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Posts<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Algorithm</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/algorithm/" aria-label="Algorithm Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Algorithm Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">AWS-SAA</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/aws-saa/" aria-label="AWS-SAA Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->AWS-SAA Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Clich√©</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/clich%C3%A9/" aria-label="Clich√© Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Clich√© Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">China Merchant Bank</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/cmb/" aria-label="CMB Internship Record"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->CMB Internship Record<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Docker</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/docker/" aria-label="Docker Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Docker Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Dubbo</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/dubbo/" aria-label="Dubbo Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Dubbo Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">ElasticSearch</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/elasticsearch/" aria-label="ElasticSearch Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->ElasticSearch Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Hadoop and Hive</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/hadoop/" aria-label="Hadoop and Hive Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Hadoop and Hive Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Interview</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/interview/" aria-label="Interview Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Interview Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Kubernetes</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/kubernetes/concepts/" aria-label="Concepts"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Concepts<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/kubernetes/practices/" aria-label="Practices"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Practices<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/kubernetes/microsvc/" aria-label="MicroSvc"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->MicroSvc<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">MyBatis</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/mybatis/" aria-label="MyBatis Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->MyBatis Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">NextJS</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/nextjs/" aria-label="NextJS Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->NextJS Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">NGINX</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/nginx/" aria-label="NGINX Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->NGINX Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Note</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/note/" aria-label="Notes"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Notes<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Typescript</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/personalweb/posts/typescript/" aria-label="Typescript Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Typescript Contents<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Unimelb Notes</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link route-link-active auto-link" href="/personalweb/posts/unimelb/" aria-label="Unimelb Notes Contents"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Unimelb Notes Contents<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/personalweb/posts/unimelb/COMP90049.html" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/personalweb/zh/" aria-label="ÁÆÄ‰Ωì‰∏≠Êñá"><!---->ÁÆÄ‰Ωì‰∏≠Êñá<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Crc011220/personalweb" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search Blogs"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search Blogs</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/" aria-label="Blog Home"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span><!--]-->Blog Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Posts</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Ai</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Algorithm</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Aws Saa</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Clich√©</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Cmb</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Docker</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Dubbo</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Elasticsearch</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/genesis.html" aria-label="Genesis"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Genesis<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Hadoop</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Interview</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Istio</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Kubernetes</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Mybatis</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Nextjs</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Nginx</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Note</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Spark</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Typescript</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Unimelb</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/unimelb/COMP90050.html" aria-label="Advanced Database Systems (COMP90050)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Advanced Database Systems (COMP90050)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/unimelb/COMP90024.html" aria-label="Cluster and Cloud Computing (COMP90024)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Cluster and Cloud Computing (COMP90024)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/unimelb/COMP90048.html" aria-label="Declarative Programming (COMP90048)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Declarative Programming (COMP90048)<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/personalweb/posts/unimelb/COMP90049.html" aria-label="Introduction to Machine Learning (COMP90049)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Introduction to Machine Learning (COMP90049)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/unimelb/SWEN90016.html" aria-label="Software Process and Management (SWEN90016)"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->Software Process and Management (SWEN90016)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/posts/unimelb/" aria-label="UNIMELB Course Notes"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span><!--]-->UNIMELB Course Notes<!----></a></li></ul></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/personalweb/intro.html" aria-label="About Me"><!--[--><span class="font-icon icon fa-fw fa-sm fas fa-circle-info" style=""></span><!--]-->About Me<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Introduction to Machine Learning (COMP90049)</h1><div class="page-info"><span class="page-author-info" aria-label="Authorüñä" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Richard Chen</span></span><span property="author" content="Richard Chen"></span></span><!----><span class="page-date-info" aria-label="Writing DateüìÖ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">March 2, 2025</span><meta property="datePublished" content="2025-03-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time‚åõ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 18 min</span><meta property="timeRequired" content="PT18M"></span><span class="page-category-info" aria-label="Categoryüåà" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color5 clickable" role="navigation">Learning Records</span><!--]--><meta property="articleSection" content="Learning Records"></span><span class="page-tag-info" aria-label="Tagüè∑" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color7 clickable" role="navigation">Unimelb</span><!--]--><meta property="keywords" content="Unimelb"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-1">Week 1</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#three-ingredients-for-machine-learning">Three ingredients for machine learning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#linear-algebra-review">Linear Algebra Review</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning">Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#featured-data-types">Featured Data Types</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#equal-width-vs-equal-frequency-vs-clustering">Equal Width vs. Equal Frequency vs. Clustering</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#standardization-vs-normalization">Standardization vs. Normalization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-2">Week 2</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#probility">Probility</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-3">Week 3</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#zero-r">Zero-R</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#one-r">One-R</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#desicion-trees">Desicion Trees</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#id3-iterative-dichotomiser-3">ID3 (Iterative Dichotomiser 3)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#naive-bayes-theory">Naive Bayes Theory</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#different-naive-bayes">Different Naive Bayes</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#conclusion-of-naive-bayes">Conclusion of Naive Bayes</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-4">Week 4</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#linear-regression">Linear Regression</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#optimization">Optimization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#iterative-optimization">Iterative Optimization</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gradient-descent">Gradient Descent</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#logistic-regression">Logistic Regression</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution">Softmax function: The function that converts a vector of real numbers to a probability distribution.</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#logistic-regression-summary">Logistic Regression Summary</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-5">Week 5</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#support-vector-machines-svm">Support Vector Machines (SVM)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#hard-margin">Hard Margin</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#applications-of-svm">Applications of SVM</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#ranking">Ranking</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#structured-prediction">Structured prediction</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#week-6">Week 6</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation">Evaluation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation-metrics">Evaluation Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#averaging-methods">Averaging methods</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-macro-averaging">1. Macro Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-micro-averaging">2. Micro Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-weighted-averaging">3. Weighted Averaging</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#regression-performance-metrics">Regression Performance Metrics</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#learning-curves">Learning Curves</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bias-and-variance">Bias and Variance</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="introduction-to-machine-learning-comp90049" tabindex="-1"><a class="header-anchor" href="#introduction-to-machine-learning-comp90049"><span>Introduction to Machine Learning (COMP90049)</span></a></h1><h2 id="week-1" tabindex="-1"><a class="header-anchor" href="#week-1"><span>Week 1</span></a></h2><ul><li>Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitly programmed.</li></ul><h3 id="three-ingredients-for-machine-learning" tabindex="-1"><a class="header-anchor" href="#three-ingredients-for-machine-learning"><span>Three ingredients for machine learning</span></a></h3><ol><li><p>Data ‚Ä¢ Discrete vs continuous vs ... ‚Ä¢ Big data vs small data ‚Ä¢ Labeled data vs unlabeled data ‚Ä¢ Public vs sensitive data</p></li><li><p>Models ‚Ä¢ function mapping from inputs to outputs ‚Ä¢ probabilistic machine learning models ‚Ä¢ geometric machine learning models ‚Ä¢ parameters of the function are unknown</p></li><li><p>Learning ‚Ä¢ Improving (on a task) after data is taken into account ‚Ä¢ Finding the best model parameters (for a given task) ‚Ä¢ Supervised vs. unsupervised learning</p></li></ol><div class="hint-container info"><p class="hint-container-title">Info</p><p>Supervised Learning:ÁõëÁù£Â≠¶‰π†‰ΩøÁî®Â∏¶ÊúâÊ†áÁ≠æÁöÑÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÊ®°ÂûãÂ≠¶‰π†ËæìÂÖ•ÔºàfeaturesÔºâÂà∞ËæìÂá∫ÔºàlabelsÔºâ‰πãÈó¥ÁöÑÊò†Â∞ÑÂÖ≥Á≥ª„ÄÇ Unsupervised Learning:Êó†ÁõëÁù£Â≠¶‰π†‰ΩøÁî®Ê≤°ÊúâÊ†áÁ≠æÁöÑÊï∞ÊçÆÔºåÊ®°ÂûãÈÄöËøáÂàÜÊûêÊï∞ÊçÆÁöÑÊ®°ÂºèÂíåÁªìÊûÑÊù•ËøõË°åÂ≠¶‰π†„ÄÇ</p></div><h3 id="linear-algebra-review" tabindex="-1"><a class="header-anchor" href="#linear-algebra-review"><span>Linear Algebra Review</span></a></h3><h4 id="matrices" tabindex="-1"><a class="header-anchor" href="#matrices"><span>Matrices</span></a></h4><ul><li>Matrices addition/subtraction: Add(Subtract) correspond ingentries in A and B</li><li>Matrix multiplication: Multiply corresponding entries in A and B and sum the products <img src="/personalweb/assets/img/matrix-multiplication.014b9a1e.png" alt="Matrix Multiplication" loading="lazy"></li><li>Matrix transpose: Transpose of a matrix is obtained by interchanging its rows and columns. Matrix is <strong>symmetric</strong> if it is equal to its transpose.</li><li>Matrix inverse: The inverse of a matrix A is denoted by A^-1 and is obtained by multiplying A by its inverse.</li><li>A matrix cannot be inverted if: More rows than columns, More columns than rows,Redundant rows/columns (linear independence)</li></ul><h4 id="vectors" tabindex="-1"><a class="header-anchor" href="#vectors"><span>Vectors</span></a></h4><ul><li>A vector is a matrix with several rows and <strong>one</strong> column</li><li>Vector addition/subtraction: Add(Subtract) corresponding entries in A and B</li><li>Vector inner product: Multiply corresponding entries in A and B and sum the products</li><li>Vector Euclidean norm: The square root of the sum of the squares of the entries in the vector. <img src="/personalweb/assets/img/vector-norm.36eacd69.png" alt="Verctor Euclidean Norm" loading="lazy"></li><li>Vector inner product: The dot product of two vectors A and B is the sum of the products of their corresponding entries.</li><li>The cosine of the angle between two vectors can be found by using norms and the inner product</li></ul><h3 id="instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning" tabindex="-1"><a class="header-anchor" href="#instances-attributes-and-learning-paradigms-supervised-vs-unsupervised-learning"><span>Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)</span></a></h3><ul><li>In ML terminology examples are called Instances</li><li>Each instance can have some Features or Attributes</li><li>Concepts are things that we aim to learn. Generally, in the form of labels or classes</li></ul><h4 id="unsupervised-do-not-have-access-to-an-inventory-of-classes-and-instead-discover-groups-of-similar-examples-in-a-given-dataset" tabindex="-1"><a class="header-anchor" href="#unsupervised-do-not-have-access-to-an-inventory-of-classes-and-instead-discover-groups-of-similar-examples-in-a-given-dataset"><span>Unsupervised do not have access to an inventory of classes and instead discover groups of ‚Äòsimilar‚Äô examples in a given dataset.</span></a></h4><ul><li>Clustering is unsupervised ‚Äî the learner operates without a set of labelled training data</li><li><strong>Success is often measured subjectively; evaluation is problematic</strong></li></ul><h4 id="supervised-methods-have-prior-knowledge-of-classes-and-set-out-to-discover-and-categorise-new-instances-according-to-those-classes" tabindex="-1"><a class="header-anchor" href="#supervised-methods-have-prior-knowledge-of-classes-and-set-out-to-discover-and-categorise-new-instances-according-to-those-classes"><span>Supervised methods have prior knowledge of classes and set out to discover and categorise new instances according to those classes</span></a></h4><ul><li>Classification learning is supervised ‚Ä¢ In Classification, we can exhaustively list/enumerate all possible labels for a given instance; a correct prediction entails mapping an instance to the label which is truly correct</li><li>Regression learning is supervised ‚Ä¢ In Regression,&quot;infinitely&quot; many labels are possible, we cannot conceivably enumerate them; a ‚Äúcorrect‚Äù prediction is when the numeric value is acceptably close to the true value</li></ul><h3 id="featured-data-types" tabindex="-1"><a class="header-anchor" href="#featured-data-types"><span>Featured Data Types</span></a></h3><ol><li>Discrete: Nominal (Categorical)</li></ol><ul><li>Values are distinct symbols, values themselves serve only as labels or names</li><li>No relation is implied among nominal values (no ordering or distance measure)</li><li>Only equality tests can be performed</li><li>e.g. Student Number</li></ul><ol start="2"><li>Ordinal</li></ol><ul><li>An explicit order is imposed on the values</li><li>Addition and subtraction does not make sense</li><li>e.g. Educational Level</li></ul><ol start="3"><li>Continuous: Numeric</li></ol><ul><li>Numeric quantities are real-valued attributes</li><li>All mathematical operations are allowed</li></ul><h2 id="equal-width-vs-equal-frequency-vs-clustering" tabindex="-1"><a class="header-anchor" href="#equal-width-vs-equal-frequency-vs-clustering"><span>Equal Width vs. Equal Frequency vs. Clustering</span></a></h2><table><thead><tr><th>Method</th><th>Equal Width Binning</th><th>Equal Frequency Binning</th><th>Clustering</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Each bin has the same width</td><td>Each bin contains the same number of data points</td><td>Groups data points based on similarity</td></tr><tr><td><strong>Type</strong></td><td>Data discretization</td><td>Data discretization</td><td>Unsupervised learning</td></tr><tr><td><strong>Advantages</strong></td><td>Easy to compute, simple</td><td>Suitable for skewed distributions</td><td>Can detect natural groupings in data</td></tr><tr><td><strong>Disadvantages</strong></td><td>Sparse or dense bins if data density varies</td><td>Uneven bin width, harder to interpret</td><td>May require tuning (e.g., number of clusters)</td></tr><tr><td><strong>Common Algorithms</strong></td><td>Fixed width intervals</td><td>Quantiles-based binning</td><td>K-Means, DBSCAN, Hierarchical Clustering</td></tr><tr><td><strong>Use Cases</strong></td><td>Histogram creation, feature engineering</td><td>Handling skewed data in ML models</td><td>Customer segmentation, anomaly detection</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><p>Equal Width BinningÔºöÁî®‰∫éÁÆÄÂçïÁ¶ªÊï£ÂåñÔºåÊØè‰∏™ bin ÂÆΩÂ∫¶Áõ∏ÂêåÔºå‰ΩÜÂèØËÉΩ‰ºöÂØºËá¥Êï∞ÊçÆÂØÜÂ∫¶‰∏çÂùáË°°„ÄÇ Equal Frequency BinningÔºöÊØè‰∏™ bin ÁöÑÊï∞ÊçÆÈáèÁõ∏Á≠âÔºåÈÄÇÂêàÂ§ÑÁêÜÂÅèÊÄÅÊï∞ÊçÆÔºå‰ΩÜ bin ÁöÑÂÆΩÂ∫¶‰∏ç‰∏ÄËá¥ÔºåÂèØËÉΩÈöæ‰ª•Ëß£Èáä„ÄÇ ClusteringÔºàËÅöÁ±ªÔºâÔºöÁî®‰∫éÊó†ÁõëÁù£Â≠¶‰π†ÔºåÊ†πÊçÆÊï∞ÊçÆÁÇπÁöÑÁõ∏‰ººÊÄßËá™Âä®ÂàÜÁªÑÔºåÈÄÇÂêàÂèëÁé∞ÈöêËóèÊ®°ÂºèÔºå‰ΩÜÈÄöÂ∏∏ÈúÄË¶ÅË∞ÉÊï¥ÂèÇÊï∞ÔºàÂ¶Ç k ÂÄºÔºâ„ÄÇ</p></div><h2 id="standardization-vs-normalization" tabindex="-1"><a class="header-anchor" href="#standardization-vs-normalization"><span>Standardization vs. Normalization</span></a></h2><table><thead><tr><th><strong>Method</strong></th><th><strong>Standardization (Z-score)</strong></th><th><strong>Min-Max Normalization</strong></th></tr></thead><tbody><tr><td><strong>Formula</strong></td><td>( X&#39; = \frac{X - \mu}{\sigma} )</td><td>( X&#39; = \frac{X - X_{\min}}{X_{\max} - X_{\min}} )</td></tr><tr><td><strong>Range</strong></td><td>Mean = 0, Std = 1</td><td>[0,1] or [-1,1]</td></tr><tr><td><strong>Best for</strong></td><td>Normally distributed data</td><td>Data with fixed bounds</td></tr><tr><td><strong>Sensitive to outliers?</strong></td><td>Less sensitive</td><td>More sensitive</td></tr><tr><td><strong>Formula</strong></td><td>(X - mean) / std</td><td>(X - min) / (max - min)</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><p>StandardizationÔºöÂ∞ÜÊï∞ÊçÆÊ†áÂáÜÂåñÂà∞ÂùáÂÄº‰∏∫ 0ÔºåÊ†áÂáÜÂ∑Æ‰∏∫ 1 ÁöÑÂàÜÂ∏ÉÔºåÈÄÇÁî®‰∫éÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÊï∞ÊçÆ„ÄÇ NormalizationÔºöÂ∞ÜÊï∞ÊçÆÁº©ÊîæÂà∞ [0,1] Êàñ [-1,1] ËåÉÂõ¥ÂÜÖÔºåÈÄÇÁî®‰∫éÊï∞ÊçÆËåÉÂõ¥Âõ∫ÂÆöÁöÑÊï∞ÊçÆ„ÄÇ</p></div><h2 id="week-2" tabindex="-1"><a class="header-anchor" href="#week-2"><span>Week 2</span></a></h2><h3 id="k-nearest-neighbors-knn" tabindex="-1"><a class="header-anchor" href="#k-nearest-neighbors-knn"><span>K-Nearest Neighbors (KNN)</span></a></h3><ul><li>supervied learning algorithm</li></ul><h4 id="knn-classification" tabindex="-1"><a class="header-anchor" href="#knn-classification"><span>KNN Classification</span></a></h4><p>‚Ä¢ Return the most common class label among neighbors ‚Ä¢ Example: cat vs dog images; text classification; ...</p><h4 id="knn-regression" tabindex="-1"><a class="header-anchor" href="#knn-regression"><span>KNN Regression</span></a></h4><p>‚Ä¢ Return the average value of among K nearest neighbors ‚Ä¢ Example: housing price prediction;</p><h4 id="to-measure-categorical-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-categorical-distance-we-can-use"><span>To measure categorical distance, we can use:</span></a></h4><ul><li>Hamming distance: number of positions where the two strings differ</li><li>Jaccard Similarity: intersection over union of two sets</li></ul><h4 id="to-measure-numerical-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-numerical-distance-we-can-use"><span>To measure numerical distance, we can use:</span></a></h4><ul><li>Manhattan distance: sum of absolute differences between corresponding components</li><li>Euclidean distance: square root of the sum of the squares of the differences between corresponding components</li><li>Cosine distance: 1 minus the cosine of the angle between two vectors</li></ul><h4 id="to-measure-oridinal-distance-we-can-use" tabindex="-1"><a class="header-anchor" href="#to-measure-oridinal-distance-we-can-use"><span>To measure oridinal distance, we can use:</span></a></h4><ul><li>Normalized Ranks: rank each value and normalize them to [0, 1]</li></ul><h4 id="majority-vote" tabindex="-1"><a class="header-anchor" href="#majority-vote"><span>Majority Vote</span></a></h4><h4 id="inverse-distance" tabindex="-1"><a class="header-anchor" href="#inverse-distance"><span>Inverse Distance</span></a></h4><ul><li>Give more weight to the nearer neighbors rather than quantity.</li><li>The bigger the weight, the more important the neighbor is. <img src="/personalweb/assets/img/Inverse-Distance.857bdd2b.png" alt="Inverse Distance" loading="lazy"></li></ul><h4 id="inverse-linear-distance" tabindex="-1"><a class="header-anchor" href="#inverse-linear-distance"><span>Inverse Linear Distance</span></a></h4><ul><li>Give more weight to the nearer neighbors, but with a decreasing slope.</li><li>The bigger the weight, the more important the neighbor is.</li></ul><h4 id="value-of-k" tabindex="-1"><a class="header-anchor" href="#value-of-k"><span>Value of K</span></a></h4><table><thead><tr><th><strong>K Value</strong></th><th><strong>Bias</strong></th><th><strong>Variance</strong></th><th><strong>Overfitting</strong></th><th><strong>Underfitting</strong></th><th><strong>Best For</strong></th></tr></thead><tbody><tr><td><strong>Small K</strong> (e.g., K=1, K=3)</td><td><strong>Low Bias</strong>: The model can closely follow the data.</td><td><strong>High Variance</strong>: Sensitive to noise and outliers.</td><td>Likely to overfit due to high sensitivity to small fluctuations in the training data.</td><td>Unlikely to underfit unless the data is too noisy or simple.</td><td>- Complex data with clear patterns<br> - When the dataset is relatively small.</td></tr><tr><td><strong>Large K</strong> (e.g., K=10, K=20)</td><td><strong>High Bias</strong>: The model becomes less sensitive to variations in the data.</td><td><strong>Low Variance</strong>: Smoothing out the noise by considering more neighbors.</td><td>Less likely to overfit as it smooths out fluctuations.</td><td>Might underfit if the data has complex relationships or non-linear patterns.</td><td>- Noisy data<br> - When a generalization is more important than capturing every detail.</td></tr><tr><td><strong>Medium K</strong> (e.g., K=5, K=7)</td><td>A balanced approach with moderate bias.</td><td>Balanced variance, aiming for generalization.</td><td>Minimizes both overfitting and underfitting.</td><td>Good compromise between bias and variance.</td><td>- Standard choice for most datasets, balancing generalization and accuracy.</td></tr></tbody></table><h4 id="why-knn" tabindex="-1"><a class="header-anchor" href="#why-knn"><span>Why KNN</span></a></h4><ul><li>Pros ‚Ä¢ Intuitive and simple ‚Ä¢ No assumptions ‚Ä¢ Supports classification and regression ‚Ä¢ No training: new data ‚Üíevolve and adapt immediately</li><li>Cons ‚Ä¢ How to decide on best distance functions? ‚Ä¢ How to combine multiple neighbors? ‚Ä¢ How to select K ? ‚Ä¢ Expensive with large (or growing) data sets</li></ul><h4 id="lazy-learning-vs-eager-learning" tabindex="-1"><a class="header-anchor" href="#lazy-learning-vs-eager-learning"><span>Lazy Learning vs. Eager Learning</span></a></h4><table><thead><tr><th>Criteria</th><th>Lazy Learning (e.g., KNN)</th><th>Eager Learning</th></tr></thead><tbody><tr><td><strong>Definition</strong></td><td>Delays learning until a query is made</td><td>Learns from the training data immediately</td></tr><tr><td><strong>Training Phase</strong></td><td>Fast (no model building)</td><td>Slow (model is built during training)</td></tr><tr><td><strong>Prediction Phase</strong></td><td>Slow (requires processing the entire dataset)</td><td>Fast (uses the pre-built model)</td></tr><tr><td><strong>Memory Requirement</strong></td><td>High (stores the entire training dataset)</td><td>Lower (only stores the model)</td></tr><tr><td><strong>Flexibility</strong></td><td>High (can adapt to new data easily)</td><td>Low (requires retraining for new data)</td></tr><tr><td><strong>Example</strong></td><td>K-Nearest Neighbors (KNN)</td><td>Decision Trees, Neural Networks</td></tr></tbody></table><h3 id="probility" tabindex="-1"><a class="header-anchor" href="#probility"><span>Probility</span></a></h3><ul><li>P(A=a): the probability that random variable A takes value a</li><li>0 &lt;= P(A=a) &lt;= 1</li><li>P(True) = 1</li><li>P(False) = 0</li></ul><h4 id="joint-probability" tabindex="-1"><a class="header-anchor" href="#joint-probability"><span>Joint Probability</span></a></h4><ul><li>P(A, B): joint probability of two events A and B</li><li>the probability of both A and B occurring = P(A ‚à© B)</li></ul><h4 id="conditional-probability" tabindex="-1"><a class="header-anchor" href="#conditional-probability"><span>Conditional Probability</span></a></h4><ul><li>P(A|B): the probability of A occurring given that B has occurred</li><li>P(A|B) = P(A ‚à© B) / P(B)</li></ul><h4 id="independent-probability" tabindex="-1"><a class="header-anchor" href="#independent-probability"><span>Independent Probability</span></a></h4><ul><li>Two events A and B are independent if P(A|B) = P(A)</li><li>P(A, B) = P(A) * P(B)</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h4 id="disjoint" tabindex="-1"><a class="header-anchor" href="#disjoint"><span>Disjoint</span></a></h4><ul><li>P(A‚à©B)=0</li></ul><h4 id="product-rule" tabindex="-1"><a class="header-anchor" href="#product-rule"><span>Product Rule</span></a></h4><ul><li>P(A, B) = P(A|B) * P(B) = P(B|A) * P(A)</li></ul><h4 id="chain-rule" tabindex="-1"><a class="header-anchor" href="#chain-rule"><span>Chain Rule</span></a></h4><ul><li>P(A,B,C)=P(A)‚ãÖP(B‚à£A)‚ãÖP(C‚à£A,B)</li></ul></div><h4 id="bayes-rule" tabindex="-1"><a class="header-anchor" href="#bayes-rule"><span>Bayes&#39; Rule</span></a></h4><ul><li>P(A|B) = ( P(B|A) * P(A) ) / P(B)</li><li>Bayes‚Äô Rule allows us to compute P(A|B) given knowledge of the ‚Äòinverse‚Äô probability P(B|A).</li></ul><h4 id="marginalization" tabindex="-1"><a class="header-anchor" href="#marginalization"><span>Marginalization</span></a></h4><figure><img src="/personalweb/assets/img/Marginalization.4e6857d9.png" alt="Marginalization" tabindex="0" loading="lazy"><figcaption>Marginalization</figcaption></figure><h4 id="probability-distributions" tabindex="-1"><a class="header-anchor" href="#probability-distributions"><span>Probability Distributions</span></a></h4><ul><li>Probability distributions can be discrete or continuous.</li><li>Discrete Random Variable: Takes on a countable number of distinct values (e.g., number of heads in coin flips).</li><li>Continuous Random Variable: Takes on an infinite number of possible values (e.g., height of students).</li></ul><table><thead><tr><th><strong>Distribution</strong></th><th><strong>Type</strong></th><th><strong>Range</strong></th><th><strong>Parameters</strong></th><th><strong>Formula</strong></th><th><strong>Example</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td><strong>Normal</strong></td><td>Continuous</td><td>‚àí‚àû to +‚àû</td><td>Mean Œº, Variance œÉ¬≤</td><td><code>P(x) = (1 / ‚àö(2œÄœÉ¬≤)) * exp(-((x - Œº)¬≤ / (2œÉ¬≤)))</code></td><td>Human height, exam scores</td><td>Linear regression, Gaussian models</td></tr><tr><td><strong>Bernoulli</strong></td><td>Discrete</td><td>0, 1</td><td>Probability p</td><td><code>P(X = k) = p^k (1 - p)^(1 - k)</code></td><td>Coin flip</td><td>Binary classification</td></tr><tr><td><strong>Binomial</strong></td><td>Discrete</td><td>0 to n</td><td>Number of trials n, Success probability p</td><td><code>P(k) = C(n, k) * p^k * (1 - p)^(n - k)</code></td><td>Number of heads in 10 coin flips</td><td>Binary classification, hypothesis testing</td></tr><tr><td><strong>Multinomial</strong></td><td>Discrete</td><td>0 to n for each category</td><td>Number of trials n, Probabilities p‚ÇÅ, ..., p‚Çñ</td><td><code>P(x‚ÇÅ, ..., x‚Çñ) = (n! / (x‚ÇÅ!x‚ÇÇ!...x‚Çñ!)) * ‚àè(p·µ¢^x·µ¢)</code></td><td>Rolling a dice multiple times</td><td>Text classification, NLP</td></tr><tr><td><strong>Categorical</strong></td><td>Discrete</td><td>1 to k</td><td>Probabilities p‚ÇÅ, ..., p‚Çñ</td><td><code>P(X = i) = p·µ¢</code></td><td>Choosing a color from a set of options</td><td>Classification, clustering</td></tr></tbody></table><h2 id="week-3" tabindex="-1"><a class="header-anchor" href="#week-3"><span>Week 3</span></a></h2><h3 id="zero-r" tabindex="-1"><a class="header-anchor" href="#zero-r"><span>Zero-R</span></a></h3><ul><li>A simple baseline model that predicts the most frequent class in the training data.</li></ul><h3 id="one-r" tabindex="-1"><a class="header-anchor" href="#one-r"><span>One-R</span></a></h3><ul><li>Also known as Decision stom</li><li>Uses only one feature (‚Äúbest‚Äù feature) to build a model</li></ul><h3 id="desicion-trees" tabindex="-1"><a class="header-anchor" href="#desicion-trees"><span>Desicion Trees</span></a></h3><figure><img src="/personalweb/assets/img/Decision-Tree-Example.6ee2db6e.png" alt="Decision Tree Example" tabindex="0" loading="lazy"><figcaption>Decision Tree Example</figcaption></figure><h3 id="id3-iterative-dichotomiser-3" tabindex="-1"><a class="header-anchor" href="#id3-iterative-dichotomiser-3"><span>ID3 (Iterative Dichotomiser 3)</span></a></h3><ul><li>A top-down approach that splits the data into smaller subsets based on the value of a chosen feature.</li></ul><h4 id="entropy-measure-of-uncertainty-the-expected-average-level-of-uncertainty-surprise" tabindex="-1"><a class="header-anchor" href="#entropy-measure-of-uncertainty-the-expected-average-level-of-uncertainty-surprise"><span>Entropy (measure of uncertainty. The expected (average) level of uncertainty (surprise))</span></a></h4><ul><li>For a Low probability event: if it happens, it‚Äôs big news! Big surprise! <strong>High information!</strong></li><li>For a High probability event: it was likely to happen anyway. Not very surprising. <strong>Low information!</strong></li><li>Higher H means more uncertain. <img src="/personalweb/assets/img/Entropy-Example.6d4874bc.png" alt="Entropy Example" loading="lazy"></li></ul><h4 id="conditional-entropy-measures-the-amount-of-uncertainty-in-x-given-y" tabindex="-1"><a class="header-anchor" href="#conditional-entropy-measures-the-amount-of-uncertainty-in-x-given-y"><span>Conditional Entropy measures the amount of uncertainty in X given Y.</span></a></h4><h4 id="information-gain-measure-of-the-reduction-in-entropy-after-splitting" tabindex="-1"><a class="header-anchor" href="#information-gain-measure-of-the-reduction-in-entropy-after-splitting"><span>Information Gain (measure of the reduction in entropy after splitting)</span></a></h4><ul><li>Information gain measures the reduction in entropy about the target variable achieved by partitioning the data based on a given feature.</li><li>Choose the largest as information gain.</li></ul><h4 id="shortcomings-of-ig" tabindex="-1"><a class="header-anchor" href="#shortcomings-of-ig"><span>Shortcomings of IG</span></a></h4><ul><li>Overfitting: Greedy algorithm may choose a feature that is too specific and does not generalize well to unseen data.</li><li>Gain ratio (GR) reduces the bias for information gain towards highlybranching attributes by normalising relative to the split information</li><li>Split info (SI) is the entropy of a given split (evenness of the distribution ofinstances to attribute values)</li></ul><h3 id="naive-bayes-theory" tabindex="-1"><a class="header-anchor" href="#naive-bayes-theory"><span>Naive Bayes Theory</span></a></h3><div class="hint-container info"><p class="hint-container-title">Info</p><p>arg max: argument of maximum value</p></div><ul><li>Supervied ML method</li></ul><h4 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>Example:</span></a></h4><p><img src="/personalweb/assets/img/Naive-Bayes-Example-1-1.2ea50406.png" alt="Naive Bayes Example1-1" loading="lazy"><img src="/personalweb/assets/img/Naive-Bayes-Example-1-2.b15986db.png" alt="Naive Bayes Example1-2" loading="lazy"></p><ul><li>If any term P(xm|y ) = 0 then the class probability P(y|x ) = 0</li><li>To solve this: use Laplace smoothing.</li></ul><ol><li>First Solution: We can assign a (small) positive probability ùúÄ to every unseen class-feature combination</li><li>Second Solution: We can add a ‚Äúpseudocount‚Äù Œ± to each feature count observed during training, often is 1.</li></ol><ul><li>Probabilities are changed drastically when there are few instances; with a large number of instances, the changes are small</li><li>Laplace smoothing (and smoothing in general) <strong>reduces variance</strong> of the NB classifier because it reduces sensitivity to individual (non-)observations in the training data</li></ul><h3 id="different-naive-bayes" tabindex="-1"><a class="header-anchor" href="#different-naive-bayes"><span>Different Naive Bayes</span></a></h3><p>Na√Øve Bayes classifiers have several key variants that differ based on how they model the distribution of features. Below is a comparison of the most common types:</p><table><thead><tr><th>Variant</th><th>Assumption on Feature Distribution</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>Gaussian Na√Øve Bayes (GNB)</strong></td><td>Assumes features follow a Gaussian (normal) distribution.</td><td>Suitable for continuous data, often used in text classification and real-world datasets with normally distributed features.</td></tr><tr><td><strong>Multinomial Na√Øve Bayes (MNB)</strong></td><td>Assumes feature counts follow a multinomial distribution.</td><td>Best for text classification (e.g., spam detection, document classification) where features are word counts or term frequencies.</td></tr><tr><td><strong>Bernoulli Na√Øve Bayes (BNB)</strong></td><td>Assumes binary feature presence (1 = present, 0 = absent).</td><td>Used in binary text classification (e.g., sentiment analysis, spam filtering), where features represent whether a word appears in a document.</td></tr><tr><td><strong>Complement Na√Øve Bayes (CNB)</strong></td><td>A modification of Multinomial Na√Øve Bayes, designed to handle class imbalances.</td><td>Works better for imbalanced datasets and improves accuracy by adjusting feature probabilities.</td></tr><tr><td><strong>Categorical Na√Øve Bayes</strong></td><td>Assumes features are discrete categorical variables.</td><td>Used for classification tasks with categorical inputs that are not necessarily text-based.</td></tr></tbody></table><p>Each variant modifies the way probabilities are calculated based on the data&#39;s nature, making Na√Øve Bayes a flexible and effective algorithm for different types of classification tasks.</p><h3 id="conclusion-of-naive-bayes" tabindex="-1"><a class="header-anchor" href="#conclusion-of-naive-bayes"><span>Conclusion of Naive Bayes</span></a></h3><ol><li>Why does it work given that it‚Äôs a blatantly wrong model of the data?</li></ol><ul><li>we don‚Äôt need the true distribution over P(y|x ), we just need to be able to identify the most likely outcome</li></ul><ol start="2"><li>Advantages of Naive Bayes</li></ol><ul><li>easy to build and estimate</li><li>easy to scale to many feature dimensions (e.g., words in the vocabulary) and data sizes</li><li>reasonably easy to explain why a specific class was predicted</li><li>good starting point for a classification project</li></ul><h2 id="week-4" tabindex="-1"><a class="header-anchor" href="#week-4"><span>Week 4</span></a></h2><h3 id="linear-regression" tabindex="-1"><a class="header-anchor" href="#linear-regression"><span>Linear Regression</span></a></h3><ul><li>A supervised learning algorithm that models the relationship between a scalar dependent variable (y) and one or more explanatory variables (X1, X2,..., Xn).</li><li>The model assumes that the relationship between the dependent and independent variables is linear.</li><li>The goal is to find the best line that fits the data.</li></ul><h4 id="loss-function" tabindex="-1"><a class="header-anchor" href="#loss-function"><span>Loss Function</span></a></h4><ul><li>The loss function measures the error between the predicted values and the actual values.</li><li>The loss function is used to optimize the model parameters (i.e., the weights and biases) to minimize the loss.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><ul><li>When using a regression model for prediction, it is important to only predict within the relevant range of data</li><li>We should not try to extrapolate beyond the range of observed X‚Äôs</li><li>Make sure independent variables are NOT highly correlated with each other, otherwise the model becomes unstable</li></ul></div><h3 id="optimization" tabindex="-1"><a class="header-anchor" href="#optimization"><span>Optimization</span></a></h3><ul><li>Find parameter values ùúΩ that maximize (or minimize) the value of a function f(ùúΩ)</li></ul><h3 id="iterative-optimization" tabindex="-1"><a class="header-anchor" href="#iterative-optimization"><span>Iterative Optimization</span></a></h3><h4 id="closed-form-solutions" tabindex="-1"><a class="header-anchor" href="#closed-form-solutions"><span>Closed-form solutions</span></a></h4><ul><li>Previously, we computed the closed form solution for the MLE of the binomial distribution</li><li>We follow our recipe, and arrive at a single solution</li></ul><h4 id="unfortunately-life-is-not-always-as-easy" tabindex="-1"><a class="header-anchor" href="#unfortunately-life-is-not-always-as-easy"><span>Unfortunately, life is not always as easy</span></a></h4><ul><li>Often, no closed-form solution exists</li><li>Instead, we have to iteratively improve our estimate of Œ∏ÀÜ until we arrive at a satisfactory solution</li><li>Gradient descent is one popular iterative optimization method</li></ul><h3 id="gradient-descent" tabindex="-1"><a class="header-anchor" href="#gradient-descent"><span>Gradient Descent</span></a></h3><ul><li>Descending a mountain (aka. our function) as fast as possible: atevery position take the next step that takes you most directly into the valley</li></ul><p><img src="/personalweb/assets/img/gradient-descent.587a0aa0.png" alt="Gradient Descent" loading="lazy"><img src="/personalweb/assets/img/gd-algo.e7a8b6b4.png" alt="Gradient Descent Algorithm" loading="lazy"></p><div class="hint-container info"><p class="hint-container-title">Info</p><ol><li>with an appropriate learning rate, GD will find the global minimum for differentiable convex functions</li><li>with an appropriate learning rate, GD will find a local minimum for differentiable non-convex functions</li></ol></div><h3 id="logistic-regression" tabindex="-1"><a class="header-anchor" href="#logistic-regression"><span>Logistic Regression</span></a></h3><table><thead><tr><th><strong>Comparison</strong></th><th><strong>Na√Øve Bayes</strong></th><th><strong>Logistic Regression</strong></th></tr></thead><tbody><tr><td><strong>Model Type</strong></td><td>Generative Model</td><td>Discriminative Model</td></tr><tr><td><strong>Probability Learned</strong></td><td>( P(x, y) ) (Joint Probability)</td><td>( P(y</td></tr><tr><td><strong>Assumptions</strong></td><td>Assumes feature independence</td><td>No specific feature independence assumption</td></tr><tr><td><strong>Computational Complexity</strong></td><td>Low, fast computation</td><td>Higher, requires gradient descent</td></tr><tr><td><strong>Use Cases</strong></td><td>Text classification (e.g., spam detection)</td><td>Tasks requiring feature relationship modeling</td></tr><tr><td><strong>Suitable for Large Datasets?</strong></td><td>Yes, simple computation</td><td>Yes, but computationally more intensive</td></tr></tbody></table><div class="hint-container info"><p class="hint-container-title">Info</p><h1 id="understanding-odds-and-log-odds-in-logistic-regression" tabindex="-1"><a class="header-anchor" href="#understanding-odds-and-log-odds-in-logistic-regression"><span>Understanding Odds and Log Odds in Logistic Regression</span></a></h1><h2 id="_1-what-are-odds" tabindex="-1"><a class="header-anchor" href="#_1-what-are-odds"><span>1. What are Odds?</span></a></h2><p>Odds represent the ratio of the probability of an event occurring to the probability of it not occurring. Mathematically, odds are defined as:</p><p>[ \text{odds} = \frac{P}{1 - P} ]</p><p>where:</p><ul><li>( P ) is the probability of the event occurring.</li><li>( 1 - P ) is the probability of the event not occurring.</li></ul><h3 id="types-of-odds" tabindex="-1"><a class="header-anchor" href="#types-of-odds"><span><strong>Types of Odds</strong></span></a></h3><ul><li><strong>Odds against an event</strong>: When ( 0 &lt; \text{odds} &lt; 1 ), meaning the event is less likely to happen than not.</li><li><strong>Odds in favor of an event</strong>: When ( \text{odds} &gt; 1 ), meaning the event is more likely to happen than not.</li></ul><p><strong>Example:</strong></p><ul><li>If an event has a <strong>60% chance</strong> of occurring (( P = 0.6 )), the odds are: [ \text{odds} = \frac{0.6}{1 - 0.6} = \frac{0.6}{0.4} = 1.5 ] This means the event is <strong>1.5 times more likely</strong> to occur than not.</li></ul><h2 id="_2-why-use-log-odds-logit-function" tabindex="-1"><a class="header-anchor" href="#_2-why-use-log-odds-logit-function"><span>2. Why Use Log Odds (Logit Function)?</span></a></h2><p>Since odds can range from <strong>0 to infinity</strong>, they are not ideal for direct modeling in a regression setting. Instead, we take the <strong>logarithm of odds</strong>, known as the <strong>logit function</strong>:</p><p>[ \text{log odds} = \log \left(\frac{P}{1 - P}\right) ]</p><h3 id="advantages-of-log-odds" tabindex="-1"><a class="header-anchor" href="#advantages-of-log-odds"><span><strong>Advantages of Log Odds:</strong></span></a></h3><ol><li><p><strong>Transforms probability into an unrestricted range</strong></p><ul><li>( P ) is always between ( 0 ) and ( 1 ), but log odds can take any value from <strong>(-\infty) to (+\infty)</strong>.</li><li>This makes it easier to model using linear regression techniques.</li></ul></li><li><p><strong>Handles Non-Linearity in Probability</strong></p><ul><li>The relationship between probability and log odds is <strong>non-linear</strong>, but when transformed to log odds, it becomes <strong>linear</strong>.</li></ul></li></ol><p><strong>Example Calculation:</strong></p><ul><li>If the event has a probability of ( P = 0.8 ): [ \text{odds} = \frac{0.8}{1 - 0.8} = \frac{0.8}{0.2} = 4 ] <ul><li>Taking the natural logarithm: [ \log(4) \approx 1.386 ]</li><li>Now, instead of dealing with probabilities, we can work with a linear scale.</li></ul></li></ul><h2 id="_3-connection-to-logistic-regression" tabindex="-1"><a class="header-anchor" href="#_3-connection-to-logistic-regression"><span>3. Connection to Logistic Regression</span></a></h2><p>In <strong>logistic regression</strong>, we model the probability of an event occurring using the equation:</p><p>[ \log \left(\frac{P}{1 - P}\right) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n ]</p><p>This means:</p><ul><li>Instead of predicting probabilities directly, logistic regression predicts <strong>log odds</strong>, which follow a linear relationship with input features.</li><li>We then use the <strong>sigmoid function</strong> to convert log odds back into probabilities.</li></ul><h2 id="_4-conclusion" tabindex="-1"><a class="header-anchor" href="#_4-conclusion"><span>4. Conclusion</span></a></h2><ul><li><strong>Odds</strong> measure how likely an event is compared to it not happening.</li><li><strong>Log odds (logit function)</strong> transform probabilities into a linear form, making them easier to model.</li><li>Logistic regression leverages log odds to predict probabilities effectively.</li></ul><p>Understanding this transformation is key to interpreting logistic regression results and making informed predictions.</p></div><ul><li>Logit function: The logarithm of the odds.</li></ul><h3 id="softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution" tabindex="-1"><a class="header-anchor" href="#softmax-function-the-function-that-converts-a-vector-of-real-numbers-to-a-probability-distribution"><span>Softmax function: The function that converts a vector of real numbers to a probability distribution.</span></a></h3><h3 id="logistic-regression-summary" tabindex="-1"><a class="header-anchor" href="#logistic-regression-summary"><span>Logistic Regression Summary</span></a></h3><ol><li>Pros</li></ol><ul><li>Probabilistic interpretation</li><li>No restrictive assumptions on features</li><li>Often outperforms Naive Bayes</li><li>Particularly suited to frequency-based features (so, popular in NLP)</li></ul><ol start="2"><li>Cons</li></ol><ul><li>Can only learn linear feature-data relationships</li><li>Some feature scaling issues</li><li>Often needs a lot of data to work well</li><li>Regularisation a nuisance, but important since overfitting can be a big problem</li></ul><h2 id="week-5" tabindex="-1"><a class="header-anchor" href="#week-5"><span>Week 5</span></a></h2><h3 id="support-vector-machines-svm" tabindex="-1"><a class="header-anchor" href="#support-vector-machines-svm"><span>Support Vector Machines (SVM)</span></a></h3><h4 id="classifier" tabindex="-1"><a class="header-anchor" href="#classifier"><span>Classifier</span></a></h4><ul><li>A linear classifier (through origin) with parameters divides the space into positive and negative halves</li></ul><h3 id="hard-margin" tabindex="-1"><a class="header-anchor" href="#hard-margin"><span>Hard Margin</span></a></h3><figure><img src="/personalweb/assets/img/margin.188baf72.png" alt="Hard Margin" tabindex="0" loading="lazy"><figcaption>Hard Margin</figcaption></figure><ul><li>The margin is the distance between the decision boundary and the closest data point.</li><li>The goal is to maximize the margin while keeping the data points as far away from the decision boundary as possible.</li><li>The decision boundary is the line that separates the positive and negative data points.</li><li>It is called hard margin because the data points must be correctly classified on both sides of the decision boundary.</li></ul><h4 id="soft-margin" tabindex="-1"><a class="header-anchor" href="#soft-margin"><span>Soft Margin</span></a></h4><ul><li>The margin is softened by introducing a penalty term that depends on the distance of the data points from the decision boundary.</li><li>The goal is to minimize the margin while keeping the data points as far away from the decision boundary as possible.</li><li>The decision boundary is the line that separates the positive and negative data points.</li><li>It is called soft margin because the data points can be misclassified on both sides of the decision boundary.</li></ul><div class="hint-container info"><p class="hint-container-title">Info</p><h3 id="kernel-trick" tabindex="-1"><a class="header-anchor" href="#kernel-trick"><span>Kernel Trick</span></a></h3><ul><li>The kernel trick is a way to transform non-linear data into a higher-dimensional space where it becomes linearly separable.</li><li>The kernel function is a similarity function that measures the similarity between two data points in the original space.</li><li>The kernel trick allows us to use non-linear models in a linear model.</li></ul></div><h3 id="applications-of-svm" tabindex="-1"><a class="header-anchor" href="#applications-of-svm"><span>Applications of SVM</span></a></h3><h4 id="multiclass-problems" tabindex="-1"><a class="header-anchor" href="#multiclass-problems"><span>Multiclass problems</span></a></h4><ol><li><strong>Introduce parameter vectors</strong>: If there are ( k ) categories, we introduce a parameter vector ( \theta_1, \theta_2, \ldots, \theta_k ) for each category.</li><li><strong>Jointly learn parameters</strong>: Jointly learn these parameters by ensuring that the discriminant function associated with the correct category has the highest value. The goal is to minimize the following expression: [ \frac{1}{2} \sum_{y=1}^{k} | \theta_y |^2 ] The constraint is that for all ( y&#39; \neq y_i ) and ( i = 1, \ldots, n ), it satisfies: [ (\theta_{y_i} \cdot x_i) \geq (\theta_{y&#39;} \cdot x_i) + 1 ]</li><li><strong>Predict new examples</strong>: Predict the label of a new example based on the following formula: [ \hat{y} = \arg\max_{y=1,\ldots,k} (\theta^*_y \cdot x) ]</li></ol><h4 id="rating-problems" tabindex="-1"><a class="header-anchor" href="#rating-problems"><span>Rating problems</span></a></h4><h4 id="ordinal-regression-problems" tabindex="-1"><a class="header-anchor" href="#ordinal-regression-problems"><span>Ordinal regression problems</span></a></h4><ul><li>Target variable: The target variable in ordinal regression is an ordinal variable, where the categories have a certain order, such as education level (elementary, middle school, high school, university, graduate) or movie ratings (1 star to 5 stars).</li><li>Model purpose: Unlike multiclass classification problems, ordinal regression models not only predict the correct category but also the order relationship between categories.</li></ul><h4 id="translate-each-rating-into-a-set-of-binary-labels" tabindex="-1"><a class="header-anchor" href="#translate-each-rating-into-a-set-of-binary-labels"><span>Translate each rating into a set of binary labels</span></a></h4><ul><li><p>We can convert these labels into binary labels. For example, we can divide the ratings into two categories: &quot;like&quot; and &quot;dislike&quot;. Here is a simple conversion example:</p></li><li><p>1 star and 2 stars: Dislike (represented by 0)</p></li><li><p>3 stars: May require additional processing as it can be considered neutral or context-dependent</p></li><li><p>4 stars and 5 stars: Like (represented by 1)</p></li><li><p>Alternatively, we can create a binary label for each rating level as follows:</p></li><li><p>1 star: [1, 0, 0, 0, 0]</p></li><li><p>2 stars: [0, 1, 0, 0, 0]</p></li><li><p>3 stars: [0, 0, 1, 0, 0]</p></li><li><p>4 stars: [0, 0, 0, 1, 0]</p></li><li><p>5 stars: [0, 0, 0, 0, 1]</p></li></ul><h3 id="ranking" tabindex="-1"><a class="header-anchor" href="#ranking"><span>Ranking</span></a></h3><ul><li>In machine learning, ranking tasks typically involve ordering a set of items such that certain items have higher priority over others. SVM can address ranking problems by modifying the standard binary classification SVM, a method commonly known as RankSVM. RankSVM learns a ranking function through pairwise comparisons, ensuring that positive examples rank higher than negative ones.</li></ul><h3 id="structured-prediction" tabindex="-1"><a class="header-anchor" href="#structured-prediction"><span>Structured prediction</span></a></h3><ul><li>Structured prediction is a branch of machine learning that involves predicting structured outputs, such as sequences, trees, or graphs. SVM can be extended to handle these problems through Structured SVM. Structured SVM learns a prediction function by maximizing the margin, which can output entire structures rather than simple class labels. This approach is particularly useful in fields like natural language processing, bioinformatics, and computer vision.</li></ul><p><strong>The core idea of Support Vector Machines (SVM) is to find an optimal hyperplane that best separates data points of different classes while maximizing the margin between the classes. This margin is the distance from the hyperplane to the nearest training data point. The goal of SVM is to find a hyperplane with the largest margin, which can improve the model&#39;s generalization ability and reduce the risk of overfitting.</strong></p><h2 id="week-6" tabindex="-1"><a class="header-anchor" href="#week-6"><span>Week 6</span></a></h2><h3 id="evaluation" tabindex="-1"><a class="header-anchor" href="#evaluation"><span>Evaluation</span></a></h3><p>if our model will perform effectively on unseen data</p><h4 id="holdout" tabindex="-1"><a class="header-anchor" href="#holdout"><span>Holdout</span></a></h4><ul><li>Split the data randomly into two parts: a training set and a test set.</li><li>Train the model on the training set and evaluate it on the test set.</li></ul><h4 id="holdout-weakness" tabindex="-1"><a class="header-anchor" href="#holdout-weakness"><span>Holdout Weakness</span></a></h4><ul><li>The size of the split affects the estimate of the model‚Äôs behaviour: Lots of test instances, and few training instances ‚Üí , the learner doesn‚Äôt have enough information to build an accurate model. Lots of training instances, and few test instances ‚Üí learner builds an accurate model, but test data might not be representative (so estimates of performance can be too high/too low)</li><li>Bias in Sampling: Random sampling of data can lead to different distribution in train and test datasets</li></ul><h4 id="stratification" tabindex="-1"><a class="header-anchor" href="#stratification"><span>Stratification</span></a></h4><ul><li>Stratification ensures that each fold or partition of the data maintains the same class distribution as the original dataset</li></ul><h4 id="stratification-weakness" tabindex="-1"><a class="header-anchor" href="#stratification-weakness"><span>Stratification Weakness</span></a></h4><ul><li>Complexity: Can be more complex to implement compared to simple random sampling.</li><li>Inefficient Resource Utilization: some data is only used for training and some only for testing</li></ul><h4 id="k-fold-cross-validation" tabindex="-1"><a class="header-anchor" href="#k-fold-cross-validation"><span>k-fold Cross-validation</span></a></h4><ul><li>Divide the data into k equal parts, and use k-1 parts for training and 1 part for testing.</li><li>Repeat k times, each time using a different part for testing and the remaining parts for training.</li><li>The average performance of the k models is the estimate of the performance of the model on the entire dataset.</li></ul><h4 id="k-fold-cross-validation-weakness" tabindex="-1"><a class="header-anchor" href="#k-fold-cross-validation-weakness"><span>k-fold Cross-validation Weakness</span></a></h4><ul><li>Fewer folds: more instances per partition, more variance in performance estimates</li><li>More folds: fewer instances per partition, less variance but slower</li></ul><h4 id="hyperparameter-tuning" tabindex="-1"><a class="header-anchor" href="#hyperparameter-tuning"><span>Hyperparameter Tuning</span></a></h4><ul><li>If the Decision Tree is too short (Depth is too small), The DT would be too simple (Something like 1-R)</li><li>If the Decision Tree is too long (Depth is too big), The DT would be too complex and cannot generalize well</li><li>To decide about the best depth for a tree (and other hyperparameters), we need a way to measure how well our tree can - edict labels for new, unseen data.</li><li>But‚Ä¶ If we use our Test data for hyperparameter tuning, then we don‚Äôt have any other (unseen) data to test the performance of the final model (after the hyperparameter tuning) ‚Üí It will cause data leakage</li><li>We need a third set ‚Üí Validation data, or use cross-validation to split the data into training, validation, and test sets.</li></ul><h3 id="evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#evaluation-metrics"><span>Evaluation Metrics</span></a></h3><ul><li>We can present all possible classification results a two-class problem in the following confusion matrix.</li><li>True Positive (TP): The model correctly predicts the positive class.</li><li>False Positive (FP): The model incorrectly predicts the positive class.</li><li>True Negative (TN): The model correctly predicts the negative class.</li><li>False Negative (FN): The model incorrectly predicts the negative class.</li></ul><h4 id="recall" tabindex="-1"><a class="header-anchor" href="#recall"><span>Recall</span></a></h4><ul><li>Recall: From the cases that are actually positive, what percentage has been correctly identified (predicted positive) by our classifier. Recall = TP / (TP + FN)</li></ul><h4 id="precision" tabindex="-1"><a class="header-anchor" href="#precision"><span>Precision</span></a></h4><ul><li>From the cases that are predicted positive, what percentage are actually positive. Precision = TP / (TP + FP)</li></ul><h4 id="f1-score" tabindex="-1"><a class="header-anchor" href="#f1-score"><span>F1-score</span></a></h4><ul><li>A popular metric that can help with finding a balance between Precision &amp; Recall is F1-Score. F1-Score is the harmonic mean of Precision and Recall. F1-Score = 2 * (Precision * Recall) / (Precision + Recall)</li></ul><h3 id="averaging-methods" tabindex="-1"><a class="header-anchor" href="#averaging-methods"><span>Averaging methods</span></a></h3><p>In multi-class problems, Macro Averaging, Micro Averaging, and Weighted Averaging are three methods for calculating the average precision or recall. These methods are particularly useful in evaluating the performance of classification models, especially in datasets with imbalanced classes. Here is a brief description of these three methods:</p><h3 id="_1-macro-averaging" tabindex="-1"><a class="header-anchor" href="#_1-macro-averaging"><span>1. Macro Averaging</span></a></h3><p>Macro Averaging first calculates the metrics (such as precision, recall, or F1 score) for each class, and then calculates the simple arithmetic average of these metrics. It treats each class as equally important, without considering the actual frequency or size of the classes. The calculation formula is as follows: $$ \text{Macro Average} = \frac{1}{N} \sum_{i=1}^{N} \text{metric}_i $$ Where, $N$ is the total number of classes, and $\text{metric}_i$ is the value of the metric for the $i$-th class. <strong>Advantages</strong>: Each class is treated equally, so this may be a better metric for imbalanced datasets. <strong>Disadvantages</strong>: If some classes are very few, they may disproportionately affect the overall performance evaluation.</p><h3 id="_2-micro-averaging" tabindex="-1"><a class="header-anchor" href="#_2-micro-averaging"><span>2. Micro Averaging</span></a></h3><p>Micro Averaging first calculates the total precision, total recall, and total F1 score for all classes, and then calculates the arithmetic average of these totals. It calculates the metrics by considering individual predictions in each class, so it takes into account the actual frequency of the classes. The calculation formula is as follows: $$ \text{Micro Average Precision} = \frac{\sum_{i=1}^{N} TP_i}{\sum_{i=1}^{N} TP_i + \sum_{i=1}^{N} FP_i} $$ $$ \text{Micro Average Recall} = \frac{\sum_{i=1}^{N} TP_i}{\sum_{i=1}^{N} TP_i + \sum_{i=1}^{N} FN_i} $$ Where, $TP_i$, $FP_i$, and $FN_i$ are the numbers of true positives, false positives, and false negatives, respectively, for the $i$-th class. <strong>Advantages</strong>: It takes into account the true distribution of each class, so it is usually a more reliable metric for imbalanced datasets. <strong>Disadvantages</strong>: It may overlook the imbalance between classes, as the contributions of all classes are averaged.</p><h3 id="_3-weighted-averaging" tabindex="-1"><a class="header-anchor" href="#_3-weighted-averaging"><span>3. Weighted Averaging</span></a></h3><p>Weighted Averaging is a variant of Macro Averaging, which takes into account the support of each class (i.e., the number of instances in each class). The value of the metric for each class is multiplied by the support of that class, and then the average of these weighted values is calculated. The calculation formula is as follows: $$ \text{Weighted Average} = \frac{\sum_{i=1}^{N} (\text{metric}_i \times \text{support}<em i="1">i)}{\sum</em>^{N} \text{support}_i} $$ Where, $\text{metric}_i$ is the value of the metric for the $i$-th class, and $\text{support}_i$ is the support of the $i$-th class. <strong>Advantages</strong>: It takes into account the relative size of each class, so it is a fairer metric for imbalanced datasets. <strong>Disadvantages</strong>: Like Macro Averaging, it may give disproportionate weight to minority classes, which may distort the overall performance evaluation. The choice of which averaging method to use depends on the specific application scenario and the characteristics of the dataset. In the case of class imbalance, Weighted Averaging is often considered the best choice, as it simultaneously considers the performance and relative importance of each class.</p><h3 id="regression-performance-metrics" tabindex="-1"><a class="header-anchor" href="#regression-performance-metrics"><span>Regression Performance Metrics</span></a></h3><h4 id="mse-rmse-and-mae-mean-squared-error-root-mean-squared-error-and-mean-absolute-error" tabindex="-1"><a class="header-anchor" href="#mse-rmse-and-mae-mean-squared-error-root-mean-squared-error-and-mean-absolute-error"><span>MSE, RMSE, and MAE (Mean Squared Error, Root Mean Squared Error, and Mean Absolute Error)</span></a></h4><ul><li>Sum of Squared Errors (SSE) is the sum of the squared differences between the predicted and actual values.</li><li>MSE emphasizes larger errors due to squaring and is sensitive to outliers.</li><li>RMSE is the square root of MSE, providing a more interpretable metric in the same units as the target variable.</li><li>MAE treats all errors equally, is less sensitive to outliers, and provides a straightforward average error measure.</li></ul><figure><img src="/personalweb/assets/img/generalization.81b87722.png" alt="Generalization Problem" tabindex="0" loading="lazy"><figcaption>Generalization Problem</figcaption></figure><ol><li>Evidence of overfitting: large gap between training and test performance</li><li>Evidence of underfitting: High error rate for both test and training set</li></ol><h3 id="learning-curves" tabindex="-1"><a class="header-anchor" href="#learning-curves"><span>Learning Curves</span></a></h3><ul><li>A learning curve is a plot of learning performance over experience or time</li><li>y-axis: Performance measured by accuracy, error rate or other metrics</li><li>x-axis: conditions, e.g., sizes of training sets, model complexity, number of iterations‚Ä¶</li><li>The learning curve can be used to identify overfitting, underfitting, or a suitable trade-off between the two.</li><li>Training learning curve: calculated from the training set that shows how well the model is learning.</li><li>Validation learning curve: calculated from a holdout set that shows how well the model is generalising.</li></ul><h4 id="data-trade-off" tabindex="-1"><a class="header-anchor" href="#data-trade-off"><span>data trade-off</span></a></h4><ul><li>More training instances? ‚Üí (usually) better model</li><li>More evaluation instances? ‚Üí more reliable estimate of effectivenes</li></ul><h3 id="bias-and-variance" tabindex="-1"><a class="header-anchor" href="#bias-and-variance"><span>Bias and Variance</span></a></h3><ul><li><p>Model bias: the tendency of our model to make systematically wrong predictions</p></li><li><p>Evaluation bias: the tendency of our evaluation strategy to over- or under-estimate the effectiveness of our model</p></li><li><p>Sampling bias: if our training or evaluation dataset isn‚Äôt representative of the population.</p></li><li><p>Model variance: Sensitivity of a machine learning model&#39;s predictions to small changes in the training data, leading to different outcomes when the model is trained on different subsets of the data.</p></li><li><p>Evaluation variance: Variability in the performance metrics of a model (such as accuracy, precision, or recall) when evaluated across different test datasets or under different evaluation conditions.</p></li></ul></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/Crc011220/personalweb/edit/main/src/posts/unimelb/COMP90049.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">4/15/2025, 6:12:32 AM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: ruocchen1220@gmail.com">Ruochen Chen</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/personalweb/posts/unimelb/COMP90048.html" aria-label="Declarative Programming (COMP90048)"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span>Declarative Programming (COMP90048)</div></a><a class="route-link auto-link next" href="/personalweb/posts/unimelb/SWEN90016.html" aria-label="Software Process and Management (SWEN90016)"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Software Process and Management (SWEN90016)<span class="font-icon icon fa-fw fa-sm fas fa-pen-to-square" style=""></span></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">Learning today, leading tomorrow</div><div class="vp-copyright">Copyright ¬© 2025 Richard Chen </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/personalweb/assets/js/runtime~app.ab402ce4.js" defer></script><script src="/personalweb/assets/js/6312.9fc55c3f.js" defer></script><script src="/personalweb/assets/js/app.6a4995c2.js" defer></script>
  </body>
</html>
