"use strict";(self.webpackChunkpersonalweb=self.webpackChunkpersonalweb||[]).push([[5416],{35579:(e,t,n)=>{n.r(t),n.d(t,{comp:()=>l,data:()=>s});var r=n(20641);const a={},l=(0,n(66262).A)(a,[["render",function(e,t){return(0,r.uX)(),(0,r.CE)("div",null,t[0]||(t[0]=[(0,r.Fv)('<h1 id="模型演进-从-cnn-到-transformer-及-bert-与-gpt" tabindex="-1"><a class="header-anchor" href="#模型演进-从-cnn-到-transformer-及-bert-与-gpt"><span>模型演进：从 CNN 到 Transformer 及 BERT 与 GPT</span></a></h1><h2 id="演进逻辑与学习过程" tabindex="-1"><a class="header-anchor" href="#演进逻辑与学习过程"><span>演进逻辑与学习过程</span></a></h2><p>底层逻辑架构是技术核心，工具和 API 会变，但架构不变，需理解以不变应万变。</p><p>AI 模型演进类似人类学习过程：</p><p><strong>CNN（看特征）→ RNN（记顺序）→ Transformer（注意力机制）→ BERT/GPT（理解与生成）</strong></p><h2 id="一、cnn-卷积神经网络" tabindex="-1"><a class="header-anchor" href="#一、cnn-卷积神经网络"><span>一、CNN（卷积神经网络）</span></a></h2><h3 id="核心逻辑" tabindex="-1"><a class="header-anchor" href="#核心逻辑"><span>核心逻辑</span></a></h3><p>像「特征猎人」，通过<strong>卷积核</strong>（小窗口）扫描图像提取局部特征（如眼睛、嘴巴），再经<strong>池化</strong>（压缩，保留最大特征值）处理。</p><h3 id="图像处理优势" tabindex="-1"><a class="header-anchor" href="#图像处理优势"><span>图像处理优势</span></a></h3><p>2012 年 AlexNet 将图像识别错误率降至 15.3%，准确率超人类肉眼。</p><h3 id="文本处理缺陷" tabindex="-1"><a class="header-anchor" href="#文本处理缺陷"><span>文本处理缺陷</span></a></h3><p>无法理解顺序，视语言为词语随机组合。如「我吃苹果」与「苹果吃我」对 CNN 无区别，分不清主宾语，导致语言模型像「记性不好的鹦鹉」。</p><h2 id="二、rnn-循环神经网络" tabindex="-1"><a class="header-anchor" href="#二、rnn-循环神经网络"><span>二、RNN（循环神经网络）</span></a></h2><h3 id="核心改进" tabindex="-1"><a class="header-anchor" href="#核心改进"><span>核心改进</span></a></h3><p>解决顺序问题，通过「隐藏状态」传递信息（类似人类边读书边记笔记），能区分「我吃苹果」和「苹果吃我」。</p><h3 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景"><span>应用场景</span></a></h3><p>早期机器翻译、情感分析等。</p><h3 id="两大缺陷" tabindex="-1"><a class="header-anchor" href="#两大缺陷"><span>两大缺陷</span></a></h3><ol><li><strong>无法并行</strong>：串行计算，训练速度慢，如排队过安检，需等前一个词计算完</li><li><strong>长距离依赖</strong>：长文本中开头信息传递到结尾会淡化，如「Alice 拿钥匙... 打开门」，模型可能忘记「Alice」</li></ol><h2 id="三、transformer" tabindex="-1"><a class="header-anchor" href="#三、transformer"><span>三、Transformer</span></a></h2><h3 id="提出背景" tabindex="-1"><a class="header-anchor" href="#提出背景"><span>提出背景</span></a></h3><p>2017 年谷歌论文《Attention Is All You Need》，抛弃循环和卷积，核心为<strong>注意力机制</strong>。</p><h3 id="注意力机制" tabindex="-1"><a class="header-anchor" href="#注意力机制"><span>注意力机制</span></a></h3><p>模仿人类抓重点，计算词间关联程度。如「我爱中国」中「我」与「爱」、「爱」与「中国」强关联，「你」弱关联。</p><h3 id="并行计算" tabindex="-1"><a class="header-anchor" href="#并行计算"><span>并行计算</span></a></h3><p>解决 RNN 速度问题，可同时计算所有词间关系（多窗口办公 vs 单窗口排队）。</p><h3 id="位置编码" tabindex="-1"><a class="header-anchor" href="#位置编码"><span>位置编码</span></a></h3><p>给每个词分配「座位号」（位置信息），结合词含义输入模型，确保顺序不混乱。</p><h2 id="四、bert-与-gpt" tabindex="-1"><a class="header-anchor" href="#四、bert-与-gpt"><span>四、BERT 与 GPT</span></a></h2><h3 id="两大流派" tabindex="-1"><a class="header-anchor" href="#两大流派"><span>两大流派</span></a></h3><table><thead><tr><th>模型</th><th>结构</th><th>注意力</th><th>擅长</th></tr></thead><tbody><tr><td><strong>BERT</strong></td><td>Transformer 编码器</td><td>双向（可同时看上下文）</td><td>理解（如知网查重通过语义识别，非字面比对）</td></tr><tr><td><strong>GPT</strong></td><td>Transformer 解码器</td><td>单向（只能看前文）</td><td>生成（模拟人类写字逻辑，逐词生成）</td></tr></tbody></table><h3 id="gpt-训练方法" tabindex="-1"><a class="header-anchor" href="#gpt-训练方法"><span>GPT 训练方法</span></a></h3><ul><li><strong>预训练</strong>：喂入海量互联网数据，学通用语法常识</li><li><strong>微调</strong>：喂专业数据（如法律文档、代码），成为领域专家</li></ul><h3 id="gpt-进化" tabindex="-1"><a class="header-anchor" href="#gpt-进化"><span>GPT 进化</span></a></h3><p>参数量增长带来「涌现」能力：</p><ul><li>GPT-3：1750 亿参数，具备逻辑推理</li><li>GPT-4：参数量超 2000 亿，能读图</li></ul><h2 id="结语" tabindex="-1"><a class="header-anchor" href="#结语"><span>结语</span></a></h2><p>AI 演进是技术接力：<strong>CNN 学特征 → RNN 学顺序 → Transformer 学理解表达</strong>，背后是工程师 10 多年的结构精雕细琢，体现科技浪漫。</p>',38)]))}]]),s=JSON.parse('{"path":"/zh/posts/daily/transformer.html","title":"模型演进：从 CNN 到 Transformer 及 BERT 与 GPT","lang":"zh-CN","frontmatter":{"icon":"pen-to-square","date":"2026-02-15T00:00:00.000Z","category":["Learning Records"],"tag":["Notes"],"description":"模型演进：从 CNN 到 Transformer 及 BERT 与 GPT 演进逻辑与学习过程 底层逻辑架构是技术核心，工具和 API 会变，但架构不变，需理解以不变应万变。 AI 模型演进类似人类学习过程： CNN（看特征）→ RNN（记顺序）→ Transformer（注意力机制）→ BERT/GPT（理解与生成） 一、CNN（卷积神经网络） 核心...","head":[["meta",{"property":"og:url","content":"https://crc011220.github.io/zh/posts/daily/transformer.html"}],["meta",{"property":"og:site_name","content":"Ruochen Chen"}],["meta",{"property":"og:title","content":"模型演进：从 CNN 到 Transformer 及 BERT 与 GPT"}],["meta",{"property":"og:description","content":"模型演进：从 CNN 到 Transformer 及 BERT 与 GPT 演进逻辑与学习过程 底层逻辑架构是技术核心，工具和 API 会变，但架构不变，需理解以不变应万变。 AI 模型演进类似人类学习过程： CNN（看特征）→ RNN（记顺序）→ Transformer（注意力机制）→ BERT/GPT（理解与生成） 一、CNN（卷积神经网络） 核心..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-02-23T10:57:54.000Z"}],["meta",{"property":"article:tag","content":"Notes"}],["meta",{"property":"article:published_time","content":"2026-02-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2026-02-23T10:57:54.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"模型演进：从 CNN 到 Transformer 及 BERT 与 GPT\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2026-02-15T00:00:00.000Z\\",\\"dateModified\\":\\"2026-02-23T10:57:54.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Ruochen Chen\\"}]}"]]},"headers":[{"level":2,"title":"演进逻辑与学习过程","slug":"演进逻辑与学习过程","link":"#演进逻辑与学习过程","children":[]},{"level":2,"title":"一、CNN（卷积神经网络）","slug":"一、cnn-卷积神经网络","link":"#一、cnn-卷积神经网络","children":[{"level":3,"title":"核心逻辑","slug":"核心逻辑","link":"#核心逻辑","children":[]},{"level":3,"title":"图像处理优势","slug":"图像处理优势","link":"#图像处理优势","children":[]},{"level":3,"title":"文本处理缺陷","slug":"文本处理缺陷","link":"#文本处理缺陷","children":[]}]},{"level":2,"title":"二、RNN（循环神经网络）","slug":"二、rnn-循环神经网络","link":"#二、rnn-循环神经网络","children":[{"level":3,"title":"核心改进","slug":"核心改进","link":"#核心改进","children":[]},{"level":3,"title":"应用场景","slug":"应用场景","link":"#应用场景","children":[]},{"level":3,"title":"两大缺陷","slug":"两大缺陷","link":"#两大缺陷","children":[]}]},{"level":2,"title":"三、Transformer","slug":"三、transformer","link":"#三、transformer","children":[{"level":3,"title":"提出背景","slug":"提出背景","link":"#提出背景","children":[]},{"level":3,"title":"注意力机制","slug":"注意力机制","link":"#注意力机制","children":[]},{"level":3,"title":"并行计算","slug":"并行计算","link":"#并行计算","children":[]},{"level":3,"title":"位置编码","slug":"位置编码","link":"#位置编码","children":[]}]},{"level":2,"title":"四、BERT 与 GPT","slug":"四、bert-与-gpt","link":"#四、bert-与-gpt","children":[{"level":3,"title":"两大流派","slug":"两大流派","link":"#两大流派","children":[]},{"level":3,"title":"GPT 训练方法","slug":"gpt-训练方法","link":"#gpt-训练方法","children":[]},{"level":3,"title":"GPT 进化","slug":"gpt-进化","link":"#gpt-进化","children":[]}]},{"level":2,"title":"结语","slug":"结语","link":"#结语","children":[]}],"git":{"createdTime":1771295886000,"updatedTime":1771844274000,"contributors":[{"name":"Ruochen Chen","email":"ruocchen1220@gmail.com","commits":2}]},"readingTime":{"minutes":2.38,"words":715},"filePathRelative":"zh/posts/daily/transformer.md","localizedDate":"2026年2月15日","excerpt":"\\n<h2>演进逻辑与学习过程</h2>\\n<p>底层逻辑架构是技术核心，工具和 API 会变，但架构不变，需理解以不变应万变。</p>\\n<p>AI 模型演进类似人类学习过程：</p>\\n<p><strong>CNN（看特征）→ RNN（记顺序）→ Transformer（注意力机制）→ BERT/GPT（理解与生成）</strong></p>\\n<h2>一、CNN（卷积神经网络）</h2>\\n<h3>核心逻辑</h3>\\n<p>像「特征猎人」，通过<strong>卷积核</strong>（小窗口）扫描图像提取局部特征（如眼睛、嘴巴），再经<strong>池化</strong>（压缩，保留最大特征值）处理。</p>","autoDesc":true}')},66262:(e,t)=>{t.A=(e,t)=>{const n=e.__vccOpts||e;for(const[e,r]of t)n[e]=r;return n}}}]);