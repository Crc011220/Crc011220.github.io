"use strict";(self.webpackChunkpersonalweb=self.webpackChunkpersonalweb||[]).push([[574],{6262:(t,s)=>{s.A=(t,s)=>{const a=t.__vccOpts||t;for(const[t,e]of s)a[t]=e;return a}},9424:(t,s,a)=>{a.r(s),a.d(s,{comp:()=>i,data:()=>r});var e=a(641);const n={},i=(0,a(6262).A)(n,[["render",function(t,s){return(0,e.uX)(),(0,e.CE)("div",null,s[0]||(s[0]=[(0,e.Fv)('<h1 id="spark-intro" tabindex="-1"><a class="header-anchor" href="#spark-intro"><span>Spark Intro</span></a></h1><h2 id="differences-between-spark-and-hadoop-mapreduce" tabindex="-1"><a class="header-anchor" href="#differences-between-spark-and-hadoop-mapreduce"><span>Differences between Spark and Hadoop MapReduce</span></a></h2><table><thead><tr><th>Feature</th><th>Apache Spark</th><th>Apache MapReduce</th></tr></thead><tbody><tr><td><strong>Processing Model</strong></td><td>In-memory processing, DAG-based</td><td>Disk-based, Map and Reduce operations</td></tr><tr><td><strong>Performance</strong></td><td>Faster due to in-memory computation</td><td>Slower, as data is read/written to disk after each operation</td></tr><tr><td><strong>Ease of Use</strong></td><td>High-level APIs (Java, Scala, Python, R)</td><td>Low-level APIs (Java)</td></tr><tr><td><strong>Fault Tolerance</strong></td><td>Uses lineage for fault tolerance (re-computation)</td><td>Replication of data blocks for fault tolerance</td></tr><tr><td><strong>Data Processing</strong></td><td>Supports batch and real-time processing (streaming)</td><td>Primarily batch processing</td></tr><tr><td><strong>Memory Consumption</strong></td><td>High, due to in-memory processing</td><td>Lower, because it relies on disk storage</td></tr><tr><td><strong>Data Shuffling</strong></td><td>More efficient due to DAG and in-memory storage</td><td>Less efficient, as it involves disk I/O</td></tr><tr><td><strong>APIs and Libraries</strong></td><td>Rich libraries (MLlib, GraphX, SparkSQL, etc.)</td><td>Limited libraries, mostly for basic processing</td></tr><tr><td><strong>Latency</strong></td><td>Low latency due to in-memory processing</td><td>High latency due to disk I/O</td></tr><tr><td><strong>Fault Tolerance Mechanism</strong></td><td>Lineage-based re-computation</td><td>Data replication at HDFS level</td></tr><tr><td><strong>Resource Management</strong></td><td>Built-in support for YARN, Mesos, and Kubernetes</td><td>Requires Hadoop YARN or similar systems</td></tr><tr><td><strong>Scalability</strong></td><td>Easily scalable, supports large-scale clusters</td><td>Scalable but with higher overhead due to disk-based operations</td></tr><tr><td><strong>Suitability</strong></td><td>Suitable for both batch and real-time workloads</td><td>Best for batch processing workloads</td></tr><tr><td><strong>Popularity</strong></td><td>Gaining widespread adoption for big data processing</td><td>Mature but less popular for newer workloads</td></tr></tbody></table><h2 id="running-spark-locally" tabindex="-1"><a class="header-anchor" href="#running-spark-locally"><span>Running Spark locally</span></a></h2><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">bin/spark-submit</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--class </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">org.apache.spark.examples.SparkPi</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--master </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">local[The</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> number</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> of</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> cores</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> you</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> want</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> to</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> use,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> can</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> use</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> *</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> to</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> use</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> all</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> available</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> cores]</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--deploy-mode </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">client</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">./examples/jars/spark-examples_2.12-3.3.1.jar </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="running-spark-on-yarn" tabindex="-1"><a class="header-anchor" href="#running-spark-on-yarn"><span>Running Spark on YARN</span></a></h2><h4 id="client-mode" tabindex="-1"><a class="header-anchor" href="#client-mode"><span>Client Mode</span></a></h4><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">bin/spark-submit</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--class </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">org.apache.spark.examples.SparkPi</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--master </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">yarn</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--deploy-mode </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">client</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">./examples/jars/spark-examples_2.12-3.3.1.jar </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="cluster-mode" tabindex="-1"><a class="header-anchor" href="#cluster-mode"><span>Cluster Mode</span></a></h4><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" data-title="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">bin/spark-submit</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--class </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">org.apache.spark.examples.SparkPi</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--master </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">yarn</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">--deploy-mode </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">cluster</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">./examples/jars/spark-examples_2.12-3.3.1.jar </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\\</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Spark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。 yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。 yarn-cluster：Driver程序运行在由ResourceManager启动的APPMaster，适用于生产环境。</p><h2 id="running-spark-standalone" tabindex="-1"><a class="header-anchor" href="#running-spark-standalone"><span>Running Spark Standalone</span></a></h2><p>Standalone模式是Spark自带的资源调度引擎，构建一个由Master + Worker构成的Spark集群，Spark运行在集群中。</p><table><thead><tr><th><strong>Mode</strong></th><th><strong>Number of Spark Machines</strong></th><th><strong>Processes to Start</strong></th><th><strong>Owner</strong></th></tr></thead><tbody><tr><td><strong>Local</strong></td><td>1</td><td>None (Runs Locally)</td><td>Spark</td></tr><tr><td><strong>Standalone</strong></td><td>≥3</td><td>Master and Worker</td><td>Spark</td></tr><tr><td><strong>YARN</strong></td><td>1</td><td>YARN and HDFS</td><td>Hadoop</td></tr></tbody></table>',14)]))}]]),r=JSON.parse('{"path":"/posts/spark/Spark-Intro.html","title":"Spark Intro","lang":"en-US","frontmatter":{"icon":"pen-to-square","date":"2025-03-18T00:00:00.000Z","category":["Learning Records"],"tag":["Spark"],"description":"Spark Intro Differences between Spark and Hadoop MapReduce Running Spark locally Running Spark on YARN Client Mode Cluster Mode Spark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程...","head":[["meta",{"property":"og:url","content":"https://crc011220.github.io/posts/spark/Spark-Intro.html"}],["meta",{"property":"og:site_name","content":"Richard Chen"}],["meta",{"property":"og:title","content":"Spark Intro"}],["meta",{"property":"og:description","content":"Spark Intro Differences between Spark and Hadoop MapReduce Running Spark locally Running Spark on YARN Client Mode Cluster Mode Spark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-18T12:08:56.000Z"}],["meta",{"property":"article:tag","content":"Spark"}],["meta",{"property":"article:published_time","content":"2025-03-18T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-18T12:08:56.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Spark Intro\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-03-18T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-18T12:08:56.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Richard Chen\\"}]}"]]},"headers":[{"level":2,"title":"Differences between Spark and Hadoop MapReduce","slug":"differences-between-spark-and-hadoop-mapreduce","link":"#differences-between-spark-and-hadoop-mapreduce","children":[]},{"level":2,"title":"Running Spark locally","slug":"running-spark-locally","link":"#running-spark-locally","children":[]},{"level":2,"title":"Running Spark on YARN","slug":"running-spark-on-yarn","link":"#running-spark-on-yarn","children":[]},{"level":2,"title":"Running Spark Standalone","slug":"running-spark-standalone","link":"#running-spark-standalone","children":[]}],"git":{"createdTime":1742299736000,"updatedTime":1742299736000,"contributors":[{"name":"Ruochen Chen","email":"ruocchen1220@gmail.com","commits":1}]},"readingTime":{"minutes":1.47,"words":440},"filePathRelative":"posts/spark/Spark-Intro.md","localizedDate":"March 18, 2025","excerpt":"\\n<h2>Differences between Spark and Hadoop MapReduce</h2>\\n<table>\\n<thead>\\n<tr>\\n<th>Feature</th>\\n<th>Apache Spark</th>\\n<th>Apache MapReduce</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><strong>Processing Model</strong></td>\\n<td>In-memory processing, DAG-based</td>\\n<td>Disk-based, Map and Reduce operations</td>\\n</tr>\\n<tr>\\n<td><strong>Performance</strong></td>\\n<td>Faster due to in-memory computation</td>\\n<td>Slower, as data is read/written to disk after each operation</td>\\n</tr>\\n<tr>\\n<td><strong>Ease of Use</strong></td>\\n<td>High-level APIs (Java, Scala, Python, R)</td>\\n<td>Low-level APIs (Java)</td>\\n</tr>\\n<tr>\\n<td><strong>Fault Tolerance</strong></td>\\n<td>Uses lineage for fault tolerance (re-computation)</td>\\n<td>Replication of data blocks for fault tolerance</td>\\n</tr>\\n<tr>\\n<td><strong>Data Processing</strong></td>\\n<td>Supports batch and real-time processing (streaming)</td>\\n<td>Primarily batch processing</td>\\n</tr>\\n<tr>\\n<td><strong>Memory Consumption</strong></td>\\n<td>High, due to in-memory processing</td>\\n<td>Lower, because it relies on disk storage</td>\\n</tr>\\n<tr>\\n<td><strong>Data Shuffling</strong></td>\\n<td>More efficient due to DAG and in-memory storage</td>\\n<td>Less efficient, as it involves disk I/O</td>\\n</tr>\\n<tr>\\n<td><strong>APIs and Libraries</strong></td>\\n<td>Rich libraries (MLlib, GraphX, SparkSQL, etc.)</td>\\n<td>Limited libraries, mostly for basic processing</td>\\n</tr>\\n<tr>\\n<td><strong>Latency</strong></td>\\n<td>Low latency due to in-memory processing</td>\\n<td>High latency due to disk I/O</td>\\n</tr>\\n<tr>\\n<td><strong>Fault Tolerance Mechanism</strong></td>\\n<td>Lineage-based re-computation</td>\\n<td>Data replication at HDFS level</td>\\n</tr>\\n<tr>\\n<td><strong>Resource Management</strong></td>\\n<td>Built-in support for YARN, Mesos, and Kubernetes</td>\\n<td>Requires Hadoop YARN or similar systems</td>\\n</tr>\\n<tr>\\n<td><strong>Scalability</strong></td>\\n<td>Easily scalable, supports large-scale clusters</td>\\n<td>Scalable but with higher overhead due to disk-based operations</td>\\n</tr>\\n<tr>\\n<td><strong>Suitability</strong></td>\\n<td>Suitable for both batch and real-time workloads</td>\\n<td>Best for batch processing workloads</td>\\n</tr>\\n<tr>\\n<td><strong>Popularity</strong></td>\\n<td>Gaining widespread adoption for big data processing</td>\\n<td>Mature but less popular for newer workloads</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}')}}]);