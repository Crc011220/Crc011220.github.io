---
icon: pen-to-square
date: 2026-02-01
category:
  - Learning Records
tag:
  - Notes
---

# Fine Tunning

微调与 LoRA 的底层逻辑 
什么是微调 
可以将预训练模型比作刚毕业的全能本科生，具备广泛知识但缺乏行业特定业务能力。微调就是通过行业数据训练，使其从通才转变为懂业务专才的过程，能让模型掌握公司业务或行业黑话。
LoRA 的原理 
LoRA（Low-Rank Adaptation）的核心逻辑是冻结原模型参数，不修改模型 “大脑”，而是在模型层间插入可训练的低秩矩阵（Adapter 适配器参数），如同给教科书贴便利贴，新增知识写在便利贴上，模型推理时同时参考原模型和新增参数。
LoRA 的三大优势 
省显存：仅训练新增的 Adapter 参数，显存占用仅为全量微调的 1/3 甚至更少，使免费云端显卡也能运行。
速度快：训练时间大幅缩短，别人需一周，LoRA 可能仅需几十分钟，且在垂直领域效果不输全量微调。
效果好：以最小代价实现专属领域效果优化，实现 “不动大手术，只做微整形”。
环境准备 
使用 ModelScope 免费云端算力 
登录 ModelScope 官网，进入 “我的 Notebook”，选择 GPU 环境（24G 显存配置足够运行 DeepSeek-R1 的 LoRA 微调），镜像选择官方默认 Pytorch 镜像，其预装必要驱动，开箱即用。启动后点击 “查看 Notebook” 进入网页代码编辑器界面，注意超过一小时无操作会自动关闭，需定期操作或设置自动刷新。
核心依赖库安装 
离开 ModelScope 官方镜像时需安装核心库：transformers（大模型基础设施）、peft（管理 LoRA 适配器参数）、bitsandbytes（量化工具，节省显存）。安装命令可使用国内镜像源，提升下载速度。
代码实战步骤 
工作区整理 
在文件浏览器右键新建文件夹，用于存放训练模型文件、日志和数据集，避免文件混乱便于复盘。进入该文件夹后，新建 Jupyter Notebook 文件（相较于.py 脚本，可分块运行，容错率高，适合边学边练）。
加载模型并测试 
在 ModelScope 模型库找到版本，通过 SDK 下载的 Python 代码将模型下载到本地。模型默认存放在隐藏的缓存目录（.cache/modelscope/models/deepseek-ai/），需通过终端命令（ls -a、cd 等）找到绝对路径并替换代码中的变量。加载模型时，若使用 GPU 环境需保留，无独显设备则删除该部分。
制作数据集 
准备问答数据（如 50 条唱歌技巧问答），转换为 JSON 格式（一行一个 JSON 对象）。将问题和答案拼接，通过代码直接定义数据集（数据量少时更直观）。
拆分数据集 
将数据集按比例拆分为训练集和测试集（如 50 条数据拆分为 45 条训练集和 5 条测试集），避免模型死记硬背测试数据，确保真正理解知识。
Tokenizer 处理 
定义函数完成两项任务：拼接问题和答案为一句话；进行 tokenize 分词（设置，对数据进行截断或补齐，统一数据长度以便 GPU 处理）。
量化设置 
通过代码对模型进行量化（如八倍量化），压缩模型体积一半以上，显著降低显存占用，虽牺牲少量精度但不影响整体效果。
LoRA 设置 
关键参数包括：（秩，决定 Adapter 参数容量，值越大学习能力越强但显存占用越高，轻量级微调设为 8 或 16 性价比高）；（缩放因子，通常设为 r 的两倍，控制微调对原模型的影响程度）；指定任务类型为语言模型对话生成。设置后仅训练约 0.06% 的参数，其余参数冻结。
训练参数配置与执行 
核心训练参数：（训练成果保存路径）；（训练轮次，数据量少需多轮训练，数据量大则 1-3 轮即可）；（每次送入显卡的数据条数，显存溢出时可减小为 2 或 1）；（开启半精度训练，显存占用砍半，训练速度翻倍）。配置完成后执行训练，训练成果保存在指定文件夹，关键文件为（训练的 Adapter 参数）和（配置说明）。
微调技能的价值与应用 
微调技能的核心价值在于举一反三，可根据不同数据将模型打造成特定领域专家：替换为公司产品手册数据可成为金牌客服；替换为法律条文可成为法律顾问；替换为 Python 代码库可成为编程助手。2026 年，会调用 API 的人众多，而能根据业务需求完成模型微调的人才是行业稀缺资源，掌握从数据清洗到模型微调的全流程是 AI 工程师的基本功。