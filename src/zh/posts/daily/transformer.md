---
icon: pen-to-square
date: 2026-02-15
category:
  - Learning Records
tag:
  - Notes
---

# 模型演进：从 CNN 到 Transformer 及 BERT 与 GPT

## 演进逻辑与学习过程

底层逻辑架构是技术核心，工具和 API 会变，但架构不变，需理解以不变应万变。

AI 模型演进类似人类学习过程：

**CNN（看特征）→ RNN（记顺序）→ Transformer（注意力机制）→ BERT/GPT（理解与生成）**

## 一、CNN（卷积神经网络）

### 核心逻辑

像「特征猎人」，通过**卷积核**（小窗口）扫描图像提取局部特征（如眼睛、嘴巴），再经**池化**（压缩，保留最大特征值）处理。

### 图像处理优势

2012 年 AlexNet 将图像识别错误率降至 15.3%，准确率超人类肉眼。

### 文本处理缺陷

无法理解顺序，视语言为词语随机组合。如「我吃苹果」与「苹果吃我」对 CNN 无区别，分不清主宾语，导致语言模型像「记性不好的鹦鹉」。

## 二、RNN（循环神经网络）

### 核心改进

解决顺序问题，通过「隐藏状态」传递信息（类似人类边读书边记笔记），能区分「我吃苹果」和「苹果吃我」。

### 应用场景

早期机器翻译、情感分析等。

### 两大缺陷

1. **无法并行**：串行计算，训练速度慢，如排队过安检，需等前一个词计算完
2. **长距离依赖**：长文本中开头信息传递到结尾会淡化，如「Alice 拿钥匙... 打开门」，模型可能忘记「Alice」

## 三、Transformer

### 提出背景

2017 年谷歌论文《Attention Is All You Need》，抛弃循环和卷积，核心为**注意力机制**。

### 注意力机制

模仿人类抓重点，计算词间关联程度。如「我爱中国」中「我」与「爱」、「爱」与「中国」强关联，「你」弱关联。

### 并行计算

解决 RNN 速度问题，可同时计算所有词间关系（多窗口办公 vs 单窗口排队）。

### 位置编码

给每个词分配「座位号」（位置信息），结合词含义输入模型，确保顺序不混乱。

## 四、BERT 与 GPT

### 两大流派

| 模型 | 结构 | 注意力 | 擅长 |
|------|------|--------|------|
| **BERT** | Transformer 编码器 | 双向（可同时看上下文） | 理解（如知网查重通过语义识别，非字面比对） |
| **GPT** | Transformer 解码器 | 单向（只能看前文） | 生成（模拟人类写字逻辑，逐词生成） |

### GPT 训练方法

- **预训练**：喂入海量互联网数据，学通用语法常识
- **微调**：喂专业数据（如法律文档、代码），成为领域专家

### GPT 进化

参数量增长带来「涌现」能力：

- GPT-3：1750 亿参数，具备逻辑推理
- GPT-4：参数量超 2000 亿，能读图

## 结语

AI 演进是技术接力：**CNN 学特征 → RNN 学顺序 → Transformer 学理解表达**，背后是工程师 10 多年的结构精雕细琢，体现科技浪漫。
