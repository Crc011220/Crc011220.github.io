---
icon: pen-to-square
date: 2025-03-02
category:
  - Learning Records
tag:
  - Unimelb
---

# Introduction to Machine Learning (COMP90049)

## Week 1

- Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitly programmed.

### Three ingredients for machine learning
1. Data
â€¢ Discrete vs continuous vs ...
â€¢ Big data vs small data
â€¢ Labeled data vs unlabeled data
â€¢ Public vs sensitive data

2. Models
â€¢ function mapping from inputs to outputs
â€¢ probabilistic machine learning models
â€¢ geometric machine learning models
â€¢ parameters of the function are unknown

3. Learning
â€¢ Improving (on a task) after data is taken into account
â€¢ Finding the best model parameters (for a given task)
â€¢ Supervised vs. unsupervised learning

:::info
Supervised Learning:ç›‘ç£å­¦ä¹ ä½¿ç”¨å¸¦æœ‰æ ‡ç­¾çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹å­¦ä¹ è¾“å…¥ï¼ˆfeaturesï¼‰åˆ°è¾“å‡ºï¼ˆlabelsï¼‰ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚
Unsupervised Learning:æ— ç›‘ç£å­¦ä¹ ä½¿ç”¨æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œæ¨¡å‹é€šè¿‡åˆ†ææ•°æ®çš„æ¨¡å¼å’Œç»“æ„æ¥è¿›è¡Œå­¦ä¹ ã€‚
:::

### Linear Algebra Review
#### Matrices
- Matrices addition/subtraction: Add(Subtract) correspond ingentries in A and B 
- Matrix multiplication: Multiply corresponding entries in A and B and sum the products
![Matrix Multiplication](matrix-multiplication.png)
- Matrix transpose: Transpose of a matrix is obtained by interchanging its rows and columns. Matrix is **symmetric** if it is equal to its transpose.
- Matrix inverse: The inverse of a matrix A is denoted by A^-1 and is obtained by multiplying A by its inverse. 
- A matrix cannot be inverted if: More rows than columns, More columns than rows,Redundant rows/columns (linear independence)
#### Vectors
- A vector is a matrix with several rows and **one** column
- Vector addition/subtraction: Add(Subtract) corresponding entries in A and B
- Vector inner product: Multiply corresponding entries in A and B and sum the products
- Vector Euclidean norm: The square root of the sum of the squares of the entries in the vector.
![Verctor Euclidean Norm](vector-norm.png)
- Vector inner product: The dot product of two vectors A and B is the sum of the products of their corresponding entries.
- The cosine of the angle between two vectors can be found by using norms and the inner product

### Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)
- In ML terminology examples are called Instances
- Each instance can have some Features or Attributes
- Concepts are things that we aim to learn. Generally, in the form of labels or classes

#### Unsupervised do not have access to an inventory of classes and instead discover groups of â€˜similarâ€™ examples in a given dataset.
- Clustering is unsupervised â€” the learner operates without a set of labelled training data
- **Success is often measured subjectively; evaluation is problematic**

#### Supervised methods have prior knowledge of classes and set out to discover and categorise new instances according to those classes
- Classification learning is supervised
â€¢ In Classification, we can exhaustively list/enumerate all possible labels for a given instance; a correct prediction
entails mapping an instance to the label which is truly correct
- Regression learning is supervised 
â€¢ In Regression,"infinitely" many labels are possible, we cannot conceivably enumerate them; a â€œcorrectâ€ prediction is when the numeric value is acceptably close to the true value

### Featured Data Types
1. Discrete: Nominal (Categorical)
- Values are distinct symbols, values themselves serve only as labels or names
- No relation is implied among nominal values (no ordering or distance measure)
- Only equality tests can be performed
- e.g. Student Number
2. Ordinal
- An explicit order is imposed on the values
- Addition and subtraction does not make sense
- e.g. Educational Level
3. Continuous: Numeric
- Numeric quantities are real-valued attributes
- All mathematical operations are allowed 

## Equal Width vs. Equal Frequency vs. Clustering

| Method                  | Equal Width Binning          | Equal Frequency Binning       | Clustering                   |
|-------------------------|----------------------------|------------------------------|------------------------------|
| **Definition**          | Each bin has the same width | Each bin contains the same number of data points | Groups data points based on similarity |
| **Type**               | Data discretization        | Data discretization         | Unsupervised learning       |
| **Advantages**         | Easy to compute, simple     | Suitable for skewed distributions | Can detect natural groupings in data |
| **Disadvantages**       | Sparse or dense bins if data density varies | Uneven bin width, harder to interpret | May require tuning (e.g., number of clusters) |
| **Common Algorithms**   | Fixed width intervals      | Quantiles-based binning     | K-Means, DBSCAN, Hierarchical Clustering |
| **Use Cases**          | Histogram creation, feature engineering | Handling skewed data in ML models | Customer segmentation, anomaly detection |

:::info
Equal Width Binningï¼šç”¨äºç®€å•ç¦»æ•£åŒ–ï¼Œæ¯ä¸ª bin å®½åº¦ç›¸åŒï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´æ•°æ®å¯†åº¦ä¸å‡è¡¡ã€‚
Equal Frequency Binningï¼šæ¯ä¸ª bin çš„æ•°æ®é‡ç›¸ç­‰ï¼Œé€‚åˆå¤„ç†åæ€æ•°æ®ï¼Œä½† bin çš„å®½åº¦ä¸ä¸€è‡´ï¼Œå¯èƒ½éš¾ä»¥è§£é‡Šã€‚
Clusteringï¼ˆèšç±»ï¼‰ï¼šç”¨äºæ— ç›‘ç£å­¦ä¹ ï¼Œæ ¹æ®æ•°æ®ç‚¹çš„ç›¸ä¼¼æ€§è‡ªåŠ¨åˆ†ç»„ï¼Œé€‚åˆå‘ç°éšè—æ¨¡å¼ï¼Œä½†é€šå¸¸éœ€è¦è°ƒæ•´å‚æ•°ï¼ˆå¦‚ k å€¼ï¼‰ã€‚
:::

## Standardization vs. Normalization

| **Method**              | **Standardization (Z-score)**            | **Min-Max Normalization**         |
|-------------------------|-----------------------------------------|----------------------------------|
| **Formula**            | \( X' = \frac{X - \mu}{\sigma} \)       | \( X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}} \) |
| **Range**              | Mean = 0, Std = 1                        | [0,1] or [-1,1]                 |
| **Best for**           | Normally distributed data                | Data with fixed bounds          |
| **Sensitive to outliers?** | Less sensitive                          | More sensitive                   |
| **Formula** | (X - mean) / std | (X - min) / (max - min) |
:::info
Standardizationï¼šå°†æ•°æ®æ ‡å‡†åŒ–åˆ°å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1 çš„åˆ†å¸ƒï¼Œé€‚ç”¨äºæ­£æ€åˆ†å¸ƒçš„æ•°æ®ã€‚
Normalizationï¼šå°†æ•°æ®ç¼©æ”¾åˆ° [0,1] æˆ– [-1,1] èŒƒå›´å†…ï¼Œé€‚ç”¨äºæ•°æ®èŒƒå›´å›ºå®šçš„æ•°æ®ã€‚
:::

## Week 2
### K-Nearest Neighbors (KNN)
- supervied learning algorithm
#### KNN Classification
â€¢ Return the most common class label among neighbors
â€¢ Example: cat vs dog images; text classification; ...
#### KNN Regression
â€¢ Return the average value of among K nearest neighbors
â€¢ Example: housing price prediction;

#### To measure categorical distance, we can use:
- Hamming distance: number of positions where the two strings differ
- Jaccard Similarity: intersection over union of two sets

#### To measure numerical distance, we can use:
- Manhattan distance: sum of absolute differences between corresponding components
- Euclidean distance: square root of the sum of the squares of the differences between corresponding components
- Cosine distance: 1 minus the cosine of the angle between two vectors

#### To measure oridinal distance, we can use:
- Normalized Ranks: rank each value and normalize them to [0, 1]

#### Majority Vote

#### Inverse Distance
- Give more weight to the nearer neighbors rather than quantity.
- The bigger the weight, the more important the neighbor is.
![Inverse Distance](Inverse-Distance.png)

#### Inverse Linear Distance
- Give more weight to the nearer neighbors, but with a decreasing slope.
- The bigger the weight, the more important the neighbor is.

#### Value of K
| **K Value** | **Bias**               | **Variance**            | **Overfitting**  | **Underfitting**  | **Best For**                                 |
|-------------|------------------------|-------------------------|------------------|-------------------|---------------------------------------------|
| **Small K** (e.g., K=1, K=3) | **Low Bias**: The model can closely follow the data. | **High Variance**: Sensitive to noise and outliers. | Likely to overfit due to high sensitivity to small fluctuations in the training data. | Unlikely to underfit unless the data is too noisy or simple. | - Complex data with clear patterns<br> - When the dataset is relatively small. |
| **Large K** (e.g., K=10, K=20) | **High Bias**: The model becomes less sensitive to variations in the data. | **Low Variance**: Smoothing out the noise by considering more neighbors. | Less likely to overfit as it smooths out fluctuations. | Might underfit if the data has complex relationships or non-linear patterns. | - Noisy data<br> - When a generalization is more important than capturing every detail. |
| **Medium K** (e.g., K=5, K=7) | A balanced approach with moderate bias. | Balanced variance, aiming for generalization. | Minimizes both overfitting and underfitting. | Good compromise between bias and variance. | - Standard choice for most datasets, balancing generalization and accuracy. |

#### Why KNN
- Pros
â€¢ Intuitive and simple
â€¢ No assumptions
â€¢ Supports classification and regression
â€¢ No training: new data â†’evolve and adapt immediately
- Cons
â€¢ How to decide on best distance functions?
â€¢ How to combine multiple neighbors?
â€¢ How to select K ?
â€¢ Expensive with large (or growing) data sets

#### Lazy Learning vs. Eager Learning

| Criteria               | Lazy Learning (e.g., KNN)                     | Eager Learning                             |
|------------------------|-----------------------------------------------|--------------------------------------------|
| **Definition**          | Delays learning until a query is made          | Learns from the training data immediately  |
| **Training Phase**      | Fast (no model building)                      | Slow (model is built during training)       |
| **Prediction Phase**    | Slow (requires processing the entire dataset)  | Fast (uses the pre-built model)            |
| **Memory Requirement**  | High (stores the entire training dataset)       | Lower (only stores the model)              |
| **Flexibility**          | High (can adapt to new data easily)            | Low (requires retraining for new data)     |
| **Example**             | K-Nearest Neighbors (KNN)                      | Decision Trees, Neural Networks            |


### Probility
- P(A=a): the probability that random variable A takes value a
- 0 <= P(A=a) <= 1
- P(True) = 1
- P(False) = 0

#### Joint Probability
- P(A, B): joint probability of two events A and B
- the probability of both A and B occurring = P(A âˆ© B)

#### Conditional Probability
- P(A|B): the probability of A occurring given that B has occurred
- P(A|B) = P(A âˆ© B) / P(B)

#### Independent Probability
- Two events A and B are independent if P(A|B) = P(A)
- P(A, B) = P(A) * P(B)

:::info
#### Disjoint
- P(Aâˆ©B)=0

#### Product Rule
- P(A, B) = P(A|B) * P(B) = P(B|A) * P(A)

#### Chain Rule
- P(A,B,C)=P(A)â‹…P(Bâˆ£A)â‹…P(Câˆ£A,B)
:::

#### Bayes' Rule
- P(A|B) = ( P(B|A) * P(A) ) / P(B)
- Bayesâ€™ Rule allows us to compute P(A|B) given knowledge of the â€˜inverseâ€™ probability P(B|A).


#### Marginalization
![Marginalization](Marginalization.png)

#### Probability Distributions
- Probability distributions can be discrete or continuous. 
- Discrete Random Variable: Takes on a countable number of distinct values (e.g., number of heads in coin flips). 
- Continuous Random Variable: Takes on an infinite number of possible values (e.g., height of students).

| **Distribution** | **Type** | **Range** | **Parameters** | **Formula** | **Example** | **Use Cases** |
|-----------------|----------|-----------|---------------|-------------|-------------|---------------|
| **Normal** | Continuous | âˆ’âˆ to +âˆ | Mean Î¼, Variance ÏƒÂ² | <code>P(x) = (1 / âˆš(2Ï€ÏƒÂ²)) * exp(-((x - Î¼)Â² / (2ÏƒÂ²)))</code> | Human height, exam scores | Linear regression, Gaussian models |
| **Bernoulli** | Discrete | 0, 1 | Probability p | <code>P(X = k) = p^k (1 - p)^(1 - k)</code> | Coin flip | Binary classification |
| **Binomial** | Discrete | 0 to n | Number of trials n, Success probability p | <code>P(k) = C(n, k) * p^k * (1 - p)^(n - k)</code> | Number of heads in 10 coin flips | Binary classification, hypothesis testing |
| **Multinomial** | Discrete | 0 to n for each category | Number of trials n, Probabilities pâ‚, ..., pâ‚– | <code>P(xâ‚, ..., xâ‚–) = (n! / (xâ‚!xâ‚‚!...xâ‚–!)) * âˆ(páµ¢^xáµ¢)</code> | Rolling a dice multiple times | Text classification, NLP |
| **Categorical** | Discrete | 1 to k | Probabilities pâ‚, ..., pâ‚– | <code>P(X = i) = páµ¢</code> | Choosing a color from a set of options | Classification, clustering |

## Week 3

### Zero-R
- A simple baseline model that predicts the most frequent class in the training data.

### One-R
- Also known as Decision stom
- Uses only one feature (â€œbestâ€ feature) to build a model

### Desicion Trees
![Decision Tree Example](Decision-Tree-Example.png)

#### ID3 (Iterative Dichotomiser 3)
- A top-down approach that splits the data into smaller subsets based on the value of a chosen feature.

#### Entropy (measure of uncertainty. The expected (average) level of uncertainty (surprise))
- For a Low probability event: if it happens, itâ€™s big news! Big surprise! **High information!**
- For a High probability event: it was likely to happen anyway. Not very surprising. **Low information!**
- Higher H means more uncertain.
![Entropy Example](Entropy-Example.png)
#### Conditional Entropy measures the amount of uncertainty in X given Y.

#### Information Gain (measure of the reduction in entropy after splitting)
- Information gain measures the reduction in entropy about the target variable achieved by partitioning the data based on a given feature.
- Choose the largest as information gain.

#### Shortcomings of IG
- Overfitting: Greedy algorithm may choose a feature that is too specific and does not generalize well to unseen data.
- Gain ratio (GR) reduces the bias for information gain towards highlybranching attributes by normalising relative to the split information
- Split info (SI) is the entropy of a given split (evenness of the distribution ofinstances to attribute values)

### Naive Bayes Theory
:::info
arg max: argument of maximum value
:::

- Supervied ML method

#### Example:
![Naive Bayes Example1-1](Naive-Bayes-Example-1-1.png)
![Naive Bayes Example1-2](Naive-Bayes-Example-1-2.png)

- If any term P(xm|y ) = 0 then the class probability P(y|x ) = 0
- To solve this: use Laplace smoothing.

1. First Solution: We can assign a (small) positive probability ğœ€ to every unseen class-feature combination
2. Second Solution: We can add a â€œpseudocountâ€ Î± to each feature count observed during training, often is 1.

- Probabilities are changed drastically when there are few instances; with a large number of instances, the changes are small
-  Laplace smoothing (and smoothing in general) **reduces variance** of the NB classifier because it reduces sensitivity to individual (non-)observations in the training data

### Different Naive Bayes

NaÃ¯ve Bayes classifiers have several key variants that differ based on how they model the distribution of features. Below is a comparison of the most common types:

| Variant        | Assumption on Feature Distribution | Use Case |
|--------------|----------------------------------|---------|
| **Gaussian NaÃ¯ve Bayes (GNB)** | Assumes features follow a Gaussian (normal) distribution. | Suitable for continuous data, often used in text classification and real-world datasets with normally distributed features. |
| **Multinomial NaÃ¯ve Bayes (MNB)** | Assumes feature counts follow a multinomial distribution. | Best for text classification (e.g., spam detection, document classification) where features are word counts or term frequencies. |
| **Bernoulli NaÃ¯ve Bayes (BNB)** | Assumes binary feature presence (1 = present, 0 = absent). | Used in binary text classification (e.g., sentiment analysis, spam filtering), where features represent whether a word appears in a document. |
| **Complement NaÃ¯ve Bayes (CNB)** | A modification of Multinomial NaÃ¯ve Bayes, designed to handle class imbalances. | Works better for imbalanced datasets and improves accuracy by adjusting feature probabilities. |
| **Categorical NaÃ¯ve Bayes** | Assumes features are discrete categorical variables. | Used for classification tasks with categorical inputs that are not necessarily text-based. |

Each variant modifies the way probabilities are calculated based on the data's nature, making NaÃ¯ve Bayes a flexible and effective algorithm for different types of classification tasks.


### Conclusion of Naive Bayes
1. Why does it work given that itâ€™s a blatantly wrong model of the data?
- we donâ€™t need the true distribution over P(y|x ), we just need to be able to identify the most likely outcome

2. Advantages of Naive Bayes
- easy to build and estimate
- easy to scale to many feature dimensions (e.g., words in the vocabulary) and data sizes
- reasonably easy to explain why a specific class was predicted
- good starting point for a classification project