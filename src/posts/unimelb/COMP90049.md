---
icon: pen-to-square
date: 2025-03-02
category:
  - Learning Records
tag:
  - Unimelb
---

# Introduction to Machine Learning (COMP90049)

## Week 1

- Machine learning is a method of teaching software to learn from data and make decisions on their own, without being explicitly programmed.

### Three ingredients for machine learning
1. Data
• Discrete vs continuous vs ...
• Big data vs small data
• Labeled data vs unlabeled data
• Public vs sensitive data

2. Models
• function mapping from inputs to outputs
• probabilistic machine learning models
• geometric machine learning models
• parameters of the function are unknown

3. Learning
• Improving (on a task) after data is taken into account
• Finding the best model parameters (for a given task)
• Supervised vs. unsupervised learning

:::info
Supervised Learning:监督学习使用带有标签的数据进行训练，模型学习输入（features）到输出（labels）之间的映射关系。
Unsupervised Learning:无监督学习使用没有标签的数据，模型通过分析数据的模式和结构来进行学习。
:::

### Linear Algebra Review
#### Matrices
- Matrices addition/subtraction: Add(Subtract) correspond ingentries in A and B 
- Matrix multiplication: Multiply corresponding entries in A and B and sum the products
![Matrix Multiplication](matrix-multiplication.png)
- Matrix transpose: Transpose of a matrix is obtained by interchanging its rows and columns. Matrix is **symmetric** if it is equal to its transpose.
- Matrix inverse: The inverse of a matrix A is denoted by A^-1 and is obtained by multiplying A by its inverse. 
- A matrix cannot be inverted if: More rows than columns, More columns than rows,Redundant rows/columns (linear independence)
#### Vectors
- A vector is a matrix with several rows and **one** column
- Vector addition/subtraction: Add(Subtract) corresponding entries in A and B
- Vector inner product: Multiply corresponding entries in A and B and sum the products
- Vector Euclidean norm: The square root of the sum of the squares of the entries in the vector.
![Verctor Euclidean Norm](vector-norm.png)
- Vector inner product: The dot product of two vectors A and B is the sum of the products of their corresponding entries.
- The cosine of the angle between two vectors can be found by using norms and the inner product

### Instances, Attributes, and Learning Paradigms (Supervised vs. Unsupervised Learning)
- In ML terminology examples are called Instances
- Each instance can have some Features or Attributes
- Concepts are things that we aim to learn. Generally, in the form of labels or classes

#### Unsupervised do not have access to an inventory of classes and instead discover groups of ‘similar’ examples in a given dataset.
- Clustering is unsupervised — the learner operates without a set of labelled training data
- **Success is often measured subjectively; evaluation is problematic**

#### Supervised methods have prior knowledge of classes and set out to discover and categorise new instances according to those classes
- Classification learning is supervised
• In Classification, we can exhaustively list/enumerate all possible labels for a given instance; a correct prediction
entails mapping an instance to the label which is truly correct
- Regression learning is supervised 
• In Regression,"infinitely" many labels are possible, we cannot conceivably enumerate them; a “correct” prediction is when the numeric value is acceptably close to the true value

### Featured Data Types
1. Discrete: Nominal (Categorical)
- Values are distinct symbols, values themselves serve only as labels or names
- No relation is implied among nominal values (no ordering or distance measure)
- Only equality tests can be performed
- e.g. Student Number
2. Ordinal
- An explicit order is imposed on the values
- Addition and subtraction does not make sense
- e.g. Educational Level
3. Continuous: Numeric
- Numeric quantities are real-valued attributes
- All mathematical operations are allowed 

## Equal Width vs. Equal Frequency vs. Clustering

| Method                  | Equal Width Binning          | Equal Frequency Binning       | Clustering                   |
|-------------------------|----------------------------|------------------------------|------------------------------|
| **Definition**          | Each bin has the same width | Each bin contains the same number of data points | Groups data points based on similarity |
| **Type**               | Data discretization        | Data discretization         | Unsupervised learning       |
| **Advantages**         | Easy to compute, simple     | Suitable for skewed distributions | Can detect natural groupings in data |
| **Disadvantages**       | Sparse or dense bins if data density varies | Uneven bin width, harder to interpret | May require tuning (e.g., number of clusters) |
| **Common Algorithms**   | Fixed width intervals      | Quantiles-based binning     | K-Means, DBSCAN, Hierarchical Clustering |
| **Use Cases**          | Histogram creation, feature engineering | Handling skewed data in ML models | Customer segmentation, anomaly detection |

:::info
Equal Width Binning：用于简单离散化，每个 bin 宽度相同，但可能会导致数据密度不均衡。
Equal Frequency Binning：每个 bin 的数据量相等，适合处理偏态数据，但 bin 的宽度不一致，可能难以解释。
Clustering（聚类）：用于无监督学习，根据数据点的相似性自动分组，适合发现隐藏模式，但通常需要调整参数（如 k 值）。
:::

## Standardization vs. Normalization

| **Method**              | **Standardization (Z-score)**            | **Min-Max Normalization**         |
|-------------------------|-----------------------------------------|----------------------------------|
| **Formula**            | \( X' = \frac{X - \mu}{\sigma} \)       | \( X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}} \) |
| **Range**              | Mean = 0, Std = 1                        | [0,1] or [-1,1]                 |
| **Best for**           | Normally distributed data                | Data with fixed bounds          |
| **Sensitive to outliers?** | Less sensitive                          | More sensitive                   |
| **Formula** | (X - mean) / std | (X - min) / (max - min) |
:::info
Standardization：将数据标准化到均值为 0，标准差为 1 的分布，适用于正态分布的数据。
Normalization：将数据缩放到 [0,1] 或 [-1,1] 范围内，适用于数据范围固定的数据。
:::

## Week 2
### K-Nearest Neighbors (KNN)

#### KNN Classification
• Return the most common class label among neighbors
• Example: cat vs dog images; text classification; ...
#### KNN Regression
• Return the average value of among K nearest neighbors
• Example: housing price prediction;

#### To measure categorical distance, we can use:
- Hamming distance: number of positions where the two strings differ
- Jaccard Similarity: intersection over union of two sets

#### To measure numerical distance, we can use:
- Manhattan distance: sum of absolute differences between corresponding components
- Euclidean distance: square root of the sum of the squares of the differences between corresponding components
- Cosine distance: 1 minus the cosine of the angle between two vectors

#### To measure oridinal distance, we can use:
- Normalized Ranks: rank each value and normalize them to [0, 1]

#### Value of K
| **K Value** | **Bias**               | **Variance**            | **Overfitting**  | **Underfitting**  | **Best For**                                 |
|-------------|------------------------|-------------------------|------------------|-------------------|---------------------------------------------|
| **Small K** (e.g., K=1, K=3) | **Low Bias**: The model can closely follow the data. | **High Variance**: Sensitive to noise and outliers. | Likely to overfit due to high sensitivity to small fluctuations in the training data. | Unlikely to underfit unless the data is too noisy or simple. | - Complex data with clear patterns<br> - When the dataset is relatively small. |
| **Large K** (e.g., K=10, K=20) | **High Bias**: The model becomes less sensitive to variations in the data. | **Low Variance**: Smoothing out the noise by considering more neighbors. | Less likely to overfit as it smooths out fluctuations. | Might underfit if the data has complex relationships or non-linear patterns. | - Noisy data<br> - When a generalization is more important than capturing every detail. |
| **Medium K** (e.g., K=5, K=7) | A balanced approach with moderate bias. | Balanced variance, aiming for generalization. | Minimizes both overfitting and underfitting. | Good compromise between bias and variance. | - Standard choice for most datasets, balancing generalization and accuracy. |

#### Why KNN
- Pros
• Intuitive and simple
• No assumptions
• Supports classification and regression
• No training: new data →evolve and adapt immediately
- Cons
• How to decide on best distance functions?
• How to combine multiple neighbors?
• How to select K ?
• Expensive with large (or growing) data sets

#### Lazy Learning vs. Eager Learning
- Lazy Learning: KNN is a lazy learning algorithm, which means it does not learn from the training data until it is queried.
- Eager Learning: KNN is an eager learning algorithm, which means it learns from the training data immediately.

### Probility
- P(A=a): the probability that random variable A takes value a
- 0 <= P(A=a) <= 1
- P(True) = 1
- P(False) = 0

#### Joint Probability
- P(A, B): joint probability of two events A and B
- the probability of both A and B occurring = P(A ∩ B)

#### Disjoint
- P(A∩B)=0

#### Conditional Probability
- P(A|B): the probability of A occurring given that B has occurred
- P(A|B) = P(A ∩ B) / P(B)

#### Independent Probability
- Two events A and B are independent if P(A|B) = P(A)
- P(A, B) = P(A) * P(B)

#### Product Rule
- P(A, B) = P(A|B) * P(B) = P(B|A) * P(A)

#### Bayes' Rule
- P(A|B) = ( P(B|A) * P(A) ) / P(B)
- Bayes’ Rule allows us to compute P(A|B) given knowledge of the ‘inverse’ probability P(B|A).

#### Chain Rule
- P(A,B,C)=P(A)⋅P(B∣A)⋅P(C∣A,B)

#### Marginalization
![Marginalization](Marginalization.png)

#### Probability Distributions
- Probability distributions can be discrete or continuous. Discrete Random Variable: Takes on a countable number of distinct values (e.g., number of heads in coin flips). Continuous Random Variable: Takes on an infinite number of possible values (e.g., height of students).

| **Distribution** | **Type** | **Range** | **Parameters** | **Formula** | **Example** | **Use Cases** |
|-----------------|----------|-----------|---------------|-------------|-------------|---------------|
| **Normal** | Continuous | \( -\infty \) to \( +\infty \) | Mean \( \mu \), Variance \( \sigma^2 \) | \( P(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2 \sigma^2} \right) \) | Human height, exam scores | Linear regression, Gaussian models |
| **Bernoulli** | Discrete | 0, 1 | Probability \( p \) | \( P(X = k) = p^k (1 - p)^{1 - k} \) | Coin flip | Binary classification |
| **Binomial** | Discrete | 0 to n | Number of trials \( n \), Success probability \( p \) | \( P(k) = \binom{n}{k} p^k (1 - p)^{n - k} \) | Number of heads in 10 coin flips | Binary classification, hypothesis testing |
| **Multinomial** | Discrete | 0 to n for each category | Number of trials \( n \), Probabilities \( p_1, \ldots, p_k \) | \( P(x_1, \ldots, x_k) = \frac{n!}{x_1! x_2! \cdots x_k!} \prod_{i=1}^{k} p_i^{x_i} \) | Rolling a dice multiple times | Text classification, NLP |
| **Categorical** | Discrete | 1 to k | Probabilities \( p_1, \ldots, p_k \) | \( P(X = i) = p_i \) | Choosing a color from a set of options | Classification, clustering |
