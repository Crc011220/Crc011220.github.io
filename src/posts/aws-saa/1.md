---
icon: pen-to-square
date: 2024-11-14
category:
  - Learning Records
tag:
  - AWS 
---

# AWS Solution Architect Associate

## Question1
An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.  
Which solution will provide private network connectivity to Amazon S3?

- A. Create a gateway VPC endpoint to the S3 bucket.
- B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.
- C. Create an instance profile on Amazon EC2 to allow S3 access.
- D. Create an Amazon API Gateway API with a private link to access the S3 endpoint.

### Answer:
**A. Create a gateway VPC endpoint to the S3 bucket.**

### Explanation:
To allow an EC2 instance in a VPC to access an S3 bucket without needing internet connectivity, you can use a **VPC endpoint**. Specifically, a **gateway VPC endpoint** is designed to provide private network connectivity between your VPC and Amazon S3 (and DynamoDB) without using the internet.

- **B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.**
    - This option does not directly address the need for private connectivity between the EC2 instance and S3. It involves routing logs to CloudWatch, which may not be necessary if direct access to S3 is required.

- **C. Create an instance profile on Amazon EC2 to allow S3 access.**
    - An instance profile allows EC2 instances to assume an IAM role for permissions to access S3, but this does not provide private network connectivity. You still need an endpoint like a VPC endpoint for private access to S3.

- **D. Create an Amazon API Gateway API with a private link to access the S3 endpoint.**
    - API Gateway and PrivateLink are typically used for private access to services within VPCs, but creating an API Gateway for accessing S3 is unnecessary and complex compared to simply using a VPC endpoint.

Thus, creating a **gateway VPC endpoint** is the simplest and most efficient solution for providing private network connectivity to S3.


## Question2
A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection.  
The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity.  
Which solution meets these requirements?

- A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.
- B. Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.
- C. Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.
- D. Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.

### Answer:
**A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.**

### Explanation:
- **Option A** is the best solution because **S3 Transfer Acceleration** speeds up the upload process for large amounts of data over long distances. By using **multipart uploads**, you can upload large files in parallel, improving throughput and efficiency. This solution minimizes operational complexity as it directly uploads to S3 without additional services or manual processes.

- **Option B** involves using **S3 Cross-Region Replication**, which introduces unnecessary complexity by replicating data across regions. While it can be useful for disaster recovery, it's not as efficient for a simple data aggregation task.

- **Option C** involves using **AWS Snowball Edge**, which is a physical data transfer solution. This is more complex and would be slower than using direct uploads for daily data aggregation, especially since the sites have high-speed Internet connections.

- **Option D** is overly complex, as it involves setting up EC2 instances, EBS volumes, and taking snapshots. This is far more complicated than directly uploading the data to S3 using transfer acceleration.

Thus, **Option A** is the most suitable solution for aggregating data with minimal complexity and high speed.


## Question3
A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.  
What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?

- A. Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.
- B. Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.
- C. Use Amazon Athena directly with Amazon S3 to run the queries as needed.
- D. Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.

### Answer:
**C. Use Amazon Athena directly with Amazon S3 to run the queries as needed.**

### Explanation:
- **Option C** is the best choice as **Amazon Athena** allows you to run SQL queries directly on data stored in **Amazon S3**. Since the logs are already stored in S3 in JSON format, Athena can query this data without needing to move it or change the existing architecture. Athena is fully managed and requires minimal operational overhead. It is ideal for simple, on-demand queries and provides a straightforward solution for this use case.

- **Option A** (Amazon Redshift) would require loading the logs into Redshift, which introduces additional complexity and overhead to set up and manage the Redshift cluster. It is more suited for complex analytics and data warehousing, not for simple log analysis.

- **Option B** (Amazon CloudWatch Logs) would require moving the logs to CloudWatch, which introduces additional complexity for log management and is more suited for logs that need to be actively monitored, rather than for on-demand query-based analysis.

- **Option D** (AWS Glue + Apache Spark on Amazon EMR) involves setting up AWS Glue for cataloging and running queries on an EMR cluster, which introduces significant operational overhead for simple log queries. This solution would be much more complex than necessary for on-demand queries on S3 data.

Thus, **Option C** is the most efficient and cost-effective solution for running simple, on-demand queries on JSON log data stored in Amazon S3.


## Question4
A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.  
Which solution meets these requirements with the LEAST amount of operational overhead?

- A. Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.
- B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.
- C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.
- D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.

### Answer:
**A. Add the aws:PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.**

### Explanation:
- **Option A** is the most efficient solution because the **`aws:PrincipalOrgID`** condition key in the S3 bucket policy allows you to restrict access based on the AWS Organization ID. This ensures that only users from accounts within the specified organization can access the S3 bucket, without needing to manually manage tags, monitor account events, or create organizational units. This is the least operationally complex solution.

- **Option B** introduces unnecessary complexity by requiring the creation of organizational units (OUs) for each department and adding additional conditions. While this could work, it is more complicated than using the organization ID directly.

- **Option C** involves using **AWS CloudTrail** to monitor account-related events. While this could be used to track account changes, it does not directly address the requirement of restricting access to the S3 bucket, and would require additional operational overhead to monitor and update the bucket policy based on those events.

- **Option D** requires tagging each user and adding a condition in the S3 bucket policy to check those tags. While this can work, it introduces additional manual overhead in managing tags for each user and does not directly leverage the organizational structure for access control.

Thus, **Option A** provides the simplest and most operationally efficient way to restrict access to the S3 bucket based on the AWS Organization ID.


## Question5
A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.  
What should a solutions architect propose to ensure users see all of their documents at once?

- A. Copy the data so both EBS volumes contain all the documents
- B. Configure the Application Load Balancer to direct a user to the server with the documents
- C. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS
- D. Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server

### Answer:
**C. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS.**

### Explanation:
- **Option C** is the most suitable solution because **Amazon EFS** (Elastic File System) provides a scalable, shared file system that can be mounted simultaneously by multiple EC2 instances. By using Amazon EFS, both EC2 instances in different Availability Zones will have access to the same set of documents, ensuring that users can see all of their documents no matter which EC2 instance handles their request. This solution also removes the need for complex data synchronization between EBS volumes.

- **Option A** suggests copying the data to both EBS volumes, which would introduce complexity and might not be scalable, as each EC2 instance would need to maintain its own copy of the data. Additionally, this could lead to data inconsistency issues.

- **Option B** suggests configuring the Application Load Balancer to direct users to the server with the required documents, but this doesn’t address the core issue of sharing data between the EC2 instances. It could lead to users being directed to different instances where their documents are not available.

- **Option D** suggests configuring the Application Load Balancer to send requests to both servers, which could cause confusion and performance issues. It does not ensure that both servers have access to all the documents, and users may still see incomplete data.

Thus, **Option C** is the most efficient and scalable approach to ensure users can access all of their documents simultaneously.


## Question6
A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.  
Which solution will meet these requirements?

- A. Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.
- B. Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.
- C. Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.
- D. Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.

### Answer:
**B. Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.**

### Explanation:
- **Option B** is the most suitable solution because **AWS Snowball Edge** is designed for large-scale data transfer with minimal bandwidth usage. The Snowball Edge device can handle up to 100 TB of data, and since the company's total storage is 70 TB, it fits within the device's capacity. The solution minimizes the network bandwidth required by using physical transport, which is ideal for migrating large datasets quickly and efficiently.

- **Option A** requires using the AWS CLI to copy the files directly to S3, which could consume a significant amount of network bandwidth, especially considering the large size and number of files. This would not meet the requirement of minimizing bandwidth usage.

- **Option C** and **Option D** both involve setting up an **S3 File Gateway**, which allows transferring data to S3 as if it were an NFS share. While this could work, it still requires using the network to transfer all the files, and it might not be as bandwidth-efficient as using a physical device like Snowball Edge. Option D also introduces the complexity of setting up an AWS Direct Connect connection, which adds unnecessary operational overhead for this use case.

Thus, **Option B** offers the least bandwidth consumption while meeting the speed and scale requirements for the migration.


## Question7:
A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.  
The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.  
Which solution will meet these requirements?

- A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.
- B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.
- C. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.
- D. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.

### Answer:
**B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.**

### Explanation:
- **Option B** is the best solution because **Amazon S3 File Gateway** provides a way to extend on-premises storage to the cloud while allowing access to files over the SMB protocol. The **S3 Lifecycle policy** can be used to transition data to **S3 Glacier Deep Archive** after 7 days, which is the ideal solution for infrequently accessed data. This meets the requirement of increasing available storage while still allowing low-latency access to recent files.

- **Option A** involves using **AWS DataSync** to copy data older than 7 days to AWS, but it does not provide a seamless solution for accessing the data on an ongoing basis, nor does it handle lifecycle management effectively.

- **Option C** involves **Amazon FSx for Windows File Server**, which is a fully managed Windows file system, but it doesn't address the need for lifecycle management (i.e., transitioning data to lower-cost storage after 7 days). FSx is more suited for continuous file access, but it can be more expensive compared to transitioning data to S3 Glacier.

- **Option D** requires installing a utility on each user’s computer to access Amazon S3 directly, which would complicate access and management, and is not ideal for seamless file server access over SMB.

Thus, **Option B** offers a comprehensive solution by extending storage with an S3 File Gateway, providing lifecycle management, and ensuring low-latency access to recently used files.


